{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, ActivationsStore, HookedSAETransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sae_cooccurrence.utils.set_paths import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "git_root = get_git_root()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92b50f198014bc18c00da2a49e30e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edaae0011114ac88ed79ac0f30d9654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/training/activations_store.py:245: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name\n",
    "    sae_id=\"layer_12/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=4,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens can be either black or white. Complete the following sentences, always use numbers (one, two, three, etc.) never digits (1, 2, 3, etc.):\n",
      "\n",
      "Training Questions:\n",
      "Q: I have three tokens, and one of them are white. How many of my tokens are black?\n",
      "A: two of them are black\n",
      "\n",
      "Q: I have six tokens, and none of them are white. How many of my tokens are black?\n",
      "A: all of them are black\n",
      "\n",
      "Q: I have two tokens, and all of them are black. How many of my tokens are white?\n",
      "A: none of them are white\n",
      "\n",
      "Q: I have nine tokens, and nine of them are white. How many of my tokens are black?\n",
      "A: zero of them are black\n",
      "\n",
      "Q: I have one tokens, and zero of them are black. How many of my tokens are white?\n",
      "A: one of them are white\n",
      "\n",
      "\n",
      "Test Question:\n",
      "Q: I have three tokens, and three of them are white. How many of my tokens are black?\n",
      "\n",
      "Test Answer:\n",
      "A: zero of them are black\n"
     ]
    }
   ],
   "source": [
    "class TokenQuestionGenerator:\n",
    "    def __init__(self):\n",
    "        self.colors = [\"black\", \"white\"]\n",
    "        self.special_cases = {\"some\": \"some\", \"all\": \"none\", \"none\": \"all\"}\n",
    "\n",
    "    def _number_to_words(self, n: int) -> str:\n",
    "        \"\"\"Convert a number to words.\"\"\"\n",
    "        return num2words(n)\n",
    "\n",
    "    def _questions_are_equivalent(self, q1: str, q2: str) -> bool:\n",
    "        \"\"\"Compare two questions to check if they are functionally equivalent.\"\"\"\n",
    "        # Extract the numerical values and colors from both questions\n",
    "        q1_parts = q1.split()\n",
    "        q2_parts = q2.split()\n",
    "\n",
    "        # If lengths are different, questions can't be equivalent\n",
    "        if len(q1_parts) != len(q2_parts):\n",
    "            return False\n",
    "\n",
    "        return q1 == q2  # Direct comparison for now\n",
    "\n",
    "    def generate_numeric_question(self) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question with numeric values.\"\"\"\n",
    "        n_total = random.randint(1, 10)\n",
    "        n_color_tokens = random.randint(0, n_total)\n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"{self._number_to_words(n_color_tokens)} of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "\n",
    "        answer = f\"A: {self._number_to_words(n_total - n_color_tokens)} of them are {test_color}\"\n",
    "\n",
    "        return question, answer\n",
    "\n",
    "    def generate_special_case_question(self) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question with special quantifiers (some, all, none).\"\"\"\n",
    "        special_type = random.choice(list(self.special_cases.keys()))\n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "        n_total = random.randint(2, 10)  # Using at least 2 tokens for special cases\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"{special_type} of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "\n",
    "        answer = f\"A: {self.special_cases[special_type]} of them are {test_color}\"\n",
    "\n",
    "        return question, answer\n",
    "\n",
    "    def generate_training_set(\n",
    "        self, num_numeric: int = 5, num_special: int = 3\n",
    "    ) -> list[tuple[str, str]]:\n",
    "        \"\"\"Generate a set of training questions.\"\"\"\n",
    "        training_set = []\n",
    "\n",
    "        # Generate numeric questions\n",
    "        for _ in range(num_numeric):\n",
    "            training_set.append(self.generate_numeric_question())\n",
    "\n",
    "        # Generate special case questions\n",
    "        for _ in range(num_special):\n",
    "            training_set.append(self.generate_special_case_question())\n",
    "\n",
    "        random.shuffle(training_set)\n",
    "        return training_set\n",
    "\n",
    "    def generate_test_question(\n",
    "        self, force_numeric: bool | None = None, force_special: bool | None = None\n",
    "    ) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate a test question.\n",
    "        Parameters:\n",
    "            force_numeric: If True, generates numeric question\n",
    "            force_special: If True, generates special case question\n",
    "        If both are None, randomly chooses between numeric and special case.\n",
    "        \"\"\"\n",
    "        if force_numeric and force_special:\n",
    "            raise ValueError(\"Cannot force both numeric and special case\")\n",
    "\n",
    "        if force_numeric:\n",
    "            return self.generate_numeric_question()\n",
    "        elif force_special:\n",
    "            return self.generate_special_case_question()\n",
    "        else:\n",
    "            return random.choice(\n",
    "                [self.generate_numeric_question, self.generate_special_case_question]\n",
    "            )()\n",
    "\n",
    "\n",
    "def generate_training_and_test(\n",
    "    num_training_numeric: int = 5,\n",
    "    num_training_special: int = 3,\n",
    "    force_test_type: str | None = None,  # Can be 'numeric', 'special', or None\n",
    "    max_attempts: int = 100,  # Add maximum attempts to prevent infinite loops\n",
    ") -> dict:\n",
    "    \"\"\"Generate a complete training set and test question.\"\"\"\n",
    "    generator = TokenQuestionGenerator()\n",
    "\n",
    "    # Generate test question first\n",
    "    force_numeric = True if force_test_type == \"numeric\" else None\n",
    "    force_special = True if force_test_type == \"special\" else None\n",
    "    test_question, test_answer = generator.generate_test_question(\n",
    "        force_numeric=force_numeric, force_special=force_special\n",
    "    )\n",
    "\n",
    "    # Generate training set, ensuring no duplicates with test question\n",
    "    training_set = []\n",
    "    attempts = 0\n",
    "\n",
    "    while (\n",
    "        len(training_set) < (num_training_numeric + num_training_special)\n",
    "        and attempts < max_attempts\n",
    "    ):\n",
    "        current_set = generator.generate_training_set(\n",
    "            num_numeric=num_training_numeric, num_special=num_training_special\n",
    "        )\n",
    "\n",
    "        # Filter out any questions that match the test question\n",
    "        training_set = [\n",
    "            (q, a)\n",
    "            for q, a in current_set\n",
    "            if not generator._questions_are_equivalent(q, test_question)\n",
    "        ]\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    if attempts >= max_attempts:\n",
    "        raise RuntimeError(\n",
    "            \"Failed to generate unique training set after maximum attempts\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"introduction\": \"Tokens can be either black or white. Complete the following sentences, always use numbers (one, two, three, etc.) never digits (1, 2, 3, etc.):\",\n",
    "        \"training_questions\": training_set,\n",
    "        \"test_question\": test_question,\n",
    "        \"test_answer\": test_answer,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    result = generate_training_and_test(\n",
    "        num_training_numeric=3,\n",
    "        num_training_special=2,\n",
    "        force_test_type=\"numeric\",  # Can be 'numeric', 'special', or None\n",
    "    )\n",
    "\n",
    "    print(result[\"introduction\"])\n",
    "    print(\"\\nTraining Questions:\")\n",
    "    for q, a in result[\"training_questions\"]:\n",
    "        print(f\"{q}\\n{a}\\n\")\n",
    "    print(\"\\nTest Question:\")\n",
    "    print(result[\"test_question\"])\n",
    "    print(\"\\nTest Answer:\")\n",
    "    print(result[\"test_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index. This is useful for\n",
    "    calibrating the right amount of the feature to add.\n",
    "    \"\"\"\n",
    "    max_activation = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae.cfg.hook_name],\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0, 1)\n",
    "        batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "        max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "\n",
    "def steering(activations, steering_strength=1.0, steering_vector=None, max_act=1.0):\n",
    "    # Note if the feature fires anyway, we'd be adding to that here.\n",
    "    return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=False if device == \"mps\" else True,\n",
    "            prepend_bos=sae.cfg.prepend_bos,\n",
    "        )\n",
    "\n",
    "    return model.tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28277e4ccec34216b3e2de8c74b04f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e515ecd15b4352ba0ab570c0c31e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Test without steering\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     normal_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_steering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResults without steering:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumeric accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnormal_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 63\u001b[0m, in \u001b[0;36mevaluate_model_with_steering\u001b[0;34m(model, sae, feature_to_steer, steering_strength, max_act, n_numeric, n_special, max_new_tokens)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Convert prompt to tokens first\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_tokens(prompt, prepend_bos\u001b[38;5;241m=\u001b[39msae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mprepend_bos)\n\u001b[0;32m---> 63\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# temperature=0.7,\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# top_p=0.9,\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_at_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     generated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     73\u001b[0m test_result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m: generated,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m generated,\n\u001b[1;32m     78\u001b[0m }\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:2155\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2148\u001b[0m             tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m   2149\u001b[0m             return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2152\u001b[0m             past_kv_cache\u001b[38;5;241m=\u001b[39mpast_kv_cache,\n\u001b[1;32m   2153\u001b[0m         )\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;66;03m# We input the entire sequence, as a [batch, pos] tensor, since we aren't using\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m     \u001b[38;5;66;03m# the cache.\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2166\u001b[0m         tokens,\n\u001b[1;32m   2167\u001b[0m         return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2168\u001b[0m         prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos,\n\u001b[1;32m   2169\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   2170\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:573\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    570\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 573\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:174\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(attn_out)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model_with_steering(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    feature_to_steer: int | None = None,\n",
    "    steering_strength: float = 1.0,\n",
    "    max_act: float = 60.0,\n",
    "    n_numeric: int = 10,\n",
    "    n_special: int = 10,\n",
    "    max_new_tokens: int = 5,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test model accuracy on token counting tasks with optional feature steering.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: The sparse autoencoder\n",
    "        feature_to_steer: Feature index to steer, or None for no steering\n",
    "        steering_strength: Strength of steering (default 1.0)\n",
    "        max_act: Maximum activation for the steered feature\n",
    "        n_numeric: Number of numeric test questions\n",
    "        n_special: Number of special test questions\n",
    "        max_new_tokens: Maximum tokens to generate for each answer\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics and test results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"numeric_correct\": 0,\n",
    "        \"special_correct\": 0,\n",
    "        \"numeric_tests\": [],\n",
    "        \"special_tests\": [],\n",
    "    }\n",
    "\n",
    "    for test_type in [\"numeric\", \"special\"]:\n",
    "        n_tests = n_numeric if test_type == \"numeric\" else n_special\n",
    "        for _ in range(n_tests):\n",
    "            test_data = generate_training_and_test(\n",
    "                num_training_numeric=40,\n",
    "                num_training_special=20,\n",
    "                force_test_type=test_type,\n",
    "            )\n",
    "\n",
    "            prompt = (\n",
    "                test_data[\"introduction\"]\n",
    "                + \"\\n\\n\"\n",
    "                + \"\\n\\n\".join(f\"{q}\\n{a}\" for q, a in test_data[\"training_questions\"])\n",
    "                + f\"\\n\\n{test_data['test_question']}\\nA: \"\n",
    "            )\n",
    "\n",
    "            if feature_to_steer is not None:\n",
    "                generated = generate_with_steering(\n",
    "                    model,\n",
    "                    sae,\n",
    "                    prompt,\n",
    "                    feature_to_steer,\n",
    "                    max_act,\n",
    "                    steering_strength=steering_strength,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                )\n",
    "            else:\n",
    "                # Convert prompt to tokens first\n",
    "                input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    # temperature=0.7,\n",
    "                    # top_p=0.9,\n",
    "                    stop_at_eos=False if device == \"mps\" else True,\n",
    "                    prepend_bos=sae.cfg.prepend_bos,\n",
    "                )\n",
    "                generated = model.tokenizer.decode(output[0])\n",
    "\n",
    "            test_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"expected\": test_data[\"test_answer\"],\n",
    "                \"generated\": generated,\n",
    "                \"correct\": test_data[\"test_answer\"] in generated,\n",
    "            }\n",
    "\n",
    "            if test_type == \"numeric\":\n",
    "                results[\"numeric_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"numeric_correct\"] += 1\n",
    "            else:\n",
    "                results[\"special_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"special_correct\"] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    results[\"numeric_accuracy\"] = results[\"numeric_correct\"] / n_numeric\n",
    "    results[\"special_accuracy\"] = results[\"special_correct\"] / n_special\n",
    "    results[\"total_accuracy\"] = (\n",
    "        results[\"numeric_correct\"] + results[\"special_correct\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Test without steering\n",
    "    normal_results = evaluate_model_with_steering(model, sae)\n",
    "    print(\"\\nResults without steering:\")\n",
    "    print(f\"Numeric accuracy: {normal_results['numeric_accuracy']:.2%}\")\n",
    "    print(f\"Special accuracy: {normal_results['special_accuracy']:.2%}\")\n",
    "    print(f\"Total accuracy: {normal_results['total_accuracy']:.2%}\")\n",
    "\n",
    "    print(\"\\nNumeric test results:\")\n",
    "    for i, test in enumerate(normal_results[\"numeric_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    print(\"\\nSpecial test results:\")\n",
    "    for i, test in enumerate(normal_results[\"special_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    # Test with feature steering\n",
    "    feature_to_steer = 12257  # Replace with your feature of interest\n",
    "    steering_strength = 0.0\n",
    "    steered_results = evaluate_model_with_steering(\n",
    "        model,\n",
    "        sae,\n",
    "        feature_to_steer=feature_to_steer,\n",
    "        steering_strength=steering_strength,\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nResults with feature {feature_to_steer} steered (strength {steering_strength}):\"\n",
    "    )\n",
    "    print(f\"Numeric accuracy: {steered_results['numeric_accuracy']:.2%}\")\n",
    "    print(f\"Special accuracy: {steered_results['special_accuracy']:.2%}\")\n",
    "    print(f\"Total accuracy: {steered_results['total_accuracy']:.2%}\")\n",
    "\n",
    "    print(\"\\nNumeric test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"numeric_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    print(\"\\nSpecial test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"special_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    # Test with feature steering\n",
    "    feature_to_steer = 15441  # Replace with your feature of interest\n",
    "    steering_strength = 0.0\n",
    "    steered_results = evaluate_model_with_steering(\n",
    "        model,\n",
    "        sae,\n",
    "        feature_to_steer=feature_to_steer,\n",
    "        steering_strength=steering_strength,\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nResults with feature {feature_to_steer} steered (strength {steering_strength}):\"\n",
    "    )\n",
    "    print(f\"Numeric accuracy: {steered_results['numeric_accuracy']:.2%}\")\n",
    "    print(f\"Special accuracy: {steered_results['special_accuracy']:.2%}\")\n",
    "    print(f\"Total accuracy: {steered_results['total_accuracy']:.2%}\")\n",
    "\n",
    "    print(\"\\nNumeric test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"numeric_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    print(\"\\nSpecial test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"special_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_intervention(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    feature_ids: int | list[int] | None = None,\n",
    "    intervention_type: str = \"none\",  # \"none\", \"steering\", or \"ablation\"\n",
    "    steering_strength: float = 1.0,\n",
    "    max_act: float = 60.0,\n",
    "    n_numeric: int = 10,\n",
    "    n_special: int = 10,\n",
    "    max_new_tokens: int = 5,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test model accuracy on token counting tasks with optional feature steering or ablation.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: The sparse autoencoder\n",
    "        feature_ids: Feature index(es) to intervene on, or None for no intervention\n",
    "        intervention_type: Type of intervention (\"none\", \"steering\", or \"ablation\")\n",
    "        steering_strength: Strength of steering (default 1.0)\n",
    "        max_act: Maximum activation for the steered feature\n",
    "        n_numeric: Number of numeric test questions\n",
    "        n_special: Number of special test questions\n",
    "        max_new_tokens: Maximum tokens to generate for each answer\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics and test results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"numeric_correct\": 0,\n",
    "        \"special_correct\": 0,\n",
    "        \"numeric_tests\": [],\n",
    "        \"special_tests\": [],\n",
    "    }\n",
    "\n",
    "    # Convert single feature_id to list for consistency\n",
    "    if isinstance(feature_ids, int):\n",
    "        feature_ids = [feature_ids]\n",
    "\n",
    "    def ablate_feature_hook(feature_activations, feature_ids):\n",
    "        feature_activations[:, :, feature_ids] = 0\n",
    "        return feature_activations\n",
    "\n",
    "    for test_type in [\"numeric\", \"special\"]:\n",
    "        n_tests = n_numeric if test_type == \"numeric\" else n_special\n",
    "        for _ in range(n_tests):\n",
    "            test_data = generate_training_and_test(\n",
    "                num_training_numeric=40,\n",
    "                num_training_special=20,\n",
    "                force_test_type=test_type,\n",
    "            )\n",
    "\n",
    "            prompt = (\n",
    "                test_data[\"introduction\"]\n",
    "                + \"\\n\\n\"\n",
    "                + \"\\n\\n\".join(f\"{q}\\n{a}\" for q, a in test_data[\"training_questions\"])\n",
    "                + f\"\\n\\n{test_data['test_question']}\\nA: \"\n",
    "            )\n",
    "\n",
    "            # Convert prompt to tokens first\n",
    "            input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "            if intervention_type == \"none\" or feature_ids is None:\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    stop_at_eos=False if device == \"mps\" else True,\n",
    "                    prepend_bos=sae.cfg.prepend_bos,\n",
    "                )\n",
    "\n",
    "            elif intervention_type == \"steering\":\n",
    "                steering_vector = sae.W_dec[feature_ids[0]].to(model.cfg.device)\n",
    "                steering_hook = partial(\n",
    "                    steering,\n",
    "                    steering_vector=steering_vector,\n",
    "                    steering_strength=steering_strength,\n",
    "                    max_act=max_act,\n",
    "                )\n",
    "\n",
    "                with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "                    output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        stop_at_eos=False if device == \"mps\" else True,\n",
    "                        prepend_bos=sae.cfg.prepend_bos,\n",
    "                    )\n",
    "\n",
    "            elif intervention_type == \"ablation\":\n",
    "                ablation_hook = partial(ablate_feature_hook, feature_ids=feature_ids)\n",
    "                model.add_sae(sae)\n",
    "                hook_point = sae.cfg.hook_name + \".hook_sae_acts_post\"\n",
    "\n",
    "                with model.hooks(fwd_hooks=[(hook_point, ablation_hook)]):\n",
    "                    output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        stop_at_eos=False if device == \"mps\" else True,\n",
    "                        prepend_bos=sae.cfg.prepend_bos,\n",
    "                    )\n",
    "\n",
    "                model.reset_hooks()\n",
    "                model.reset_saes()\n",
    "\n",
    "            generated = model.tokenizer.decode(output[0])\n",
    "            generated_answer = generated.split(\"A: \")[-1].strip()\n",
    "            expected_answer = test_data[\"test_answer\"].replace(\"A: \", \"\").strip()\n",
    "\n",
    "            test_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"expected\": expected_answer,\n",
    "                \"generated\": generated_answer,\n",
    "                \"correct\": expected_answer in generated_answer,\n",
    "            }\n",
    "\n",
    "            if test_type == \"numeric\":\n",
    "                results[\"numeric_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"numeric_correct\"] += 1\n",
    "            else:\n",
    "                results[\"special_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"special_correct\"] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    results[\"numeric_accuracy\"] = results[\"numeric_correct\"] / n_numeric\n",
    "    results[\"special_accuracy\"] = results[\"special_correct\"] / n_special\n",
    "    results[\"total_accuracy\"] = (\n",
    "        results[\"numeric_correct\"] + results[\"special_correct\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb11ae3a3a1447d93ee0aaf2e4fc211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042c62af37134728ab68680cc409e2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_results = evaluate_model_with_intervention(\n",
    "    model, sae, intervention_type=\"none\", n_numeric=1, n_special=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Tokens can be either black or white. Complete the following sentences, always use numbers (one, two, three, etc.) never digits (1, 2, 3, etc.):\\n\\nQ: I have two tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have ten tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have eight tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have three tokens, and zero of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have five tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have four tokens, and zero of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\nQ: I have two tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have four tokens, and one of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have ten tokens, and six of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\nQ: I have seven tokens, and two of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have ten tokens, and five of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have three tokens, and zero of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nQ: I have eight tokens, and five of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have two tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have five tokens, and two of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nQ: I have six tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have seven tokens, and four of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nQ: I have two tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have eight tokens, and two of them are white. How many of my tokens are black?\\nA: six of them are black\\n\\nQ: I have ten tokens, and one of them are white. How many of my tokens are black?\\nA: nine of them are black\\n\\nQ: I have ten tokens, and three of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\nQ: I have two tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have two tokens, and zero of them are white. How many of my tokens are black?\\nA: two of them are black\\n\\nQ: I have four tokens, and one of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have seven tokens, and six of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have one tokens, and zero of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have ten tokens, and zero of them are black. How many of my tokens are white?\\nA: ten of them are white\\n\\nQ: I have ten tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have six tokens, and four of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have three tokens, and two of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have four tokens, and zero of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\nQ: I have five tokens, and zero of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have eight tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have eight tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have six tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have one tokens, and one of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nQ: I have nine tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have five tokens, and one of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nQ: I have seven tokens, and six of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have one tokens, and zero of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have seven tokens, and seven of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nQ: I have five tokens, and three of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have seven tokens, and two of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have eight tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have two tokens, and zero of them are white. How many of my tokens are black?\\nA: two of them are black\\n\\nQ: I have seven tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have seven tokens, and four of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have three tokens, and three of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nQ: I have two tokens, and two of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nQ: I have two tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have five tokens, and zero of them are white. How many of my tokens are black?\\nA: five of them are black\\n\\nQ: I have ten tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have six tokens, and five of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have seven tokens, and two of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have six tokens, and zero of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nQ: I have ten tokens, and nine of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have five tokens, and four of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have nine tokens, and all of them are black. How many of my tokens are white?\\nA: ', 'expected': 'none of them are white', 'generated': '0 of them are white', 'correct': False}]\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"special_tests\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01067dd9baac40888ab074b00fa14d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9942726761b4be49d2ef206d4379498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b75837e936a41c8b7c51ec70bc25c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921f43effce842ee9957deda414eee3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d4e4842da44f918df07af917c82d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4695716488d54f019a943be68f88b4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec01af9b2f9c4382990db2366ca4a76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c685c37502344793b5e0934270b424bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd62819f9e1484d9d2b557fda3a2ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cd702f1c284ea5879b1e4974b8b75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26493ac00db4bb7894f43a38a884f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4254fe98262c4d369ca4bb9b2ce029e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe432bd1e1534d2b9cd3d20e66a06ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc655758d98849f096523b3be4f407f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80728e83d92d4552827c88c7ac6893cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071f9ca7fe18492e9a2d611d44d9ec96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67719589c0bd4cf38d470cb32de9874f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edb806da1ba4d2fa93383bb77e5bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d8016d9424dd4b59e1f0e8bddb5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424e2e79dc97458688493f9203582b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251b8b00f26e4bab9215e03f283e4bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc60b0ecda4f6fad65c4b7e1b9cb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2096be60b1704267864cc1aa67b2eb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7d82d145a648cd8ef93db7405d49f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8a32dab3aa40fc8293d4069798bb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d0c7585aa343e4bc17ffe526644229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ff370d27124538b9156017b34ed422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b79427b9f040f1912d9dc2fd626d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfba45a42adb4707909d4f67d6e0dd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e219499e11a44df89574f9291bf4b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf97e892f4764a8fa5b9e2a7a9163b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f65b6e692c4cc4b72c3d19fc760be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670168eb5d41423698979dad7715b9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5daf3fbaf74dbaa133b88ec7601acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a03d75534494481bb55f151166feae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b038dfd7b644f17a85d67bfed40e4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89738b2a01aa4401bbb3037d6d56f87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198371e4acf949a98985c6afd4349519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a071b55932e546d4bfeef617ddafb0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94bf2f1440c468e90e7cc74940c0096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f151259c96434ed38bd0cdf65868c68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c49ea63f3846d0a105d5cefe76c2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11722afdb0df4d66bc4aa76de9e1768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104283ab30f9440ca445a475a28e7d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a14e350e9d24524b8bf6fb90140a401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a9688805ee40209fe4dc9f7bcb8f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a676397f3a424064a96475faf4eb9303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16b1d99526240ff8a71dbd80f87ad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9c7a2b415b46e98bc26c3adb340583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf4a373f8cc47b3ac409264fd24839f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4e2d131a28488c8b0e72621851afba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab327db448324c9f9a795b0757276e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf2a5b9b03a4e229414dae305f705b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630214cdddfc472786ab4afce2256678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a27ba05ade84ff2a77a23b5b5ed6ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09e2308ce7b4dcaa9be5154d2300967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a304c3ccd94c05a932f0020da46308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e762eb65310346a0abb8ba6ab7ebe802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4b67beef8a4b5b8314780e5875882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbad12aca04492aa255c49473b30445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d48503b9a4008a6767f3942e47f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eec54aa5d274d4cb4e92a1d13e01f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143e669d93634e68a24dd06483e141fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3b9f6f30c64067b3d6f8f5584c3a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea57a0e27db43278af4387f93b0ecbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3c951d639b4289a5de6790686c5b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0550072b714c4ef19388243389904529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc650a0a39d44efbc0808b10751f189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f44fbf93dc7411ab9f4383922198c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901c21f39ad54c12afea2ac98d7181e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2a9f750a224a6aa6a294a9eeb74f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bcf7b157734eeea975d3ae14ec24c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e128ffaea6d64d8d8d28a76fd82e7927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c6036a64844ee0979d6612d7e1e615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5d1dfb9e284928b479be5fb27ecdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c79f8f5de96497ab681cbd2134ef2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba79df90d7de4bd68330e8aec60abbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddbd055cc8c481cad268073739c2c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b9735ee20e451d9a340de7f6c37ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471ff0df7d14a099438855b95a8fac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9038c58c79444ecf9581636f619d2a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455ed178c69c4384a91b3007637b5b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77669f7d21140058287fb356b584902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8e4658909e46ad967e5756e59d93b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f8165572794a6b8dce0d73e3967172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb06d2b59b04242b74fa80573fd5a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8aa74f046aa4cffb46b6ed0f3032172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d638b2959054a248b88cdd88f17911a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129e29935c5e406c9963584fd2e685b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a68f6bf72944219db7dbc1f65bb96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42bddd8c29e46b0a43c3bb192db9aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73307a0ec8724fcc9321f5634357be31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491b608c4eca4b019d9d329e20f4f1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e483d6094469cb8567bb9447c3981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4b37c9143b49bba3c3eccb8eb25fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd9e89c4be843058f7d062c0a69d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b945971f235a4c5ea883867683853ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d839f6e6dd4cd49a45d528a0e124ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d63490ed1f446ff8e2f4f0a02bf3676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d73d11fc7d4b8cb23f1aafa398ff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cbf1d5f53d4d4b9c32aa1664b17126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b827a10d8c2240aa83df511d48f70079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26180084f08a4246b50d3bb0142dbf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78906788e0b2485e96e6af14dfd0a24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010e53ae1ab74ea5a5b2c836b16a1a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425968dfad0b4ec68397570734effaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6539aab803174914a202822902b9f459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d455097c4b84994a9772d8d52046005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbda7af8c269416fb05fd08f8e758368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d699a90d7ce4d5f92c140eab0f8fbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0c6bb0511043bfab1944ca037726a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8977034d45734aadb9a95c2a6ee893f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73f2e894ad945fb8d50eb64f3552bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6febf989edc47d59a6eb8354db5b7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0dfaf4103b47e1ac63ce27f0f79762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338c4be892b84bce8c4ae32aca02b1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ecb7a17985420898af6f591d247361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068f0414867545f28affc3897681734d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cefe1507c744ba88f9e182a89f4572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e17502b74a45dbb847a26ba7aca0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test without intervention\n",
    "normal_results = evaluate_model_with_intervention(model, sae, intervention_type=\"none\")\n",
    "\n",
    "# # Test with steering\n",
    "# steered_results = evaluate_model_with_intervention(\n",
    "#     model,\n",
    "#     sae,\n",
    "#     feature_ids=12257,\n",
    "#     intervention_type=\"steering\",\n",
    "#     steering_strength=1.0,\n",
    "# )\n",
    "\n",
    "# Test with ablation\n",
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_all = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_spokes = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_hub_spoke_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the effect of ablation of the hub alone both spokes alone hub and both spokes together and both spokes together I hope hub and spokes will be more effective than any of these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.9\n",
      "0.85\n",
      "0.9\n",
      "0.85\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"total_accuracy\"])\n",
    "print(ablated_results_hub[\"total_accuracy\"])\n",
    "print(ablated_results_some[\"total_accuracy\"])\n",
    "print(ablated_results_all[\"total_accuracy\"])\n",
    "print(ablation_results_spokes[\"total_accuracy\"])\n",
    "print(ablation_results_hub_spoke_some[\"total_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.9\n",
      "0.7\n",
      "0.8\n",
      "0.7\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"numeric_accuracy\"])\n",
    "print(ablated_results_hub[\"numeric_accuracy\"])\n",
    "print(ablated_results_some[\"numeric_accuracy\"])\n",
    "print(ablated_results_all[\"numeric_accuracy\"])\n",
    "print(ablation_results_spokes[\"numeric_accuracy\"])\n",
    "print(ablation_results_hub_spoke_some[\"numeric_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"special_accuracy\"])\n",
    "print(ablated_results_hub[\"special_accuracy\"])\n",
    "print(ablated_results_some[\"special_accuracy\"])\n",
    "print(ablated_results_all[\"special_accuracy\"])\n",
    "print(ablation_results_spokes[\"special_accuracy\"])\n",
    "print(ablation_results_hub_spoke_some[\"special_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without intervention\n",
    "normal_results = evaluate_model_with_intervention(model, sae, intervention_type=\"none\")\n",
    "\n",
    "# # Test with steering\n",
    "# steered_results = evaluate_model_with_intervention(\n",
    "#     model,\n",
    "#     sae,\n",
    "#     feature_ids=12257,\n",
    "#     intervention_type=\"steering\",\n",
    "#     steering_strength=1.0,\n",
    "# )\n",
    "\n",
    "# Test with ablation\n",
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_all = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_spokes = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_hub_spoke_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-cooccurence-DZTJ6ajw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
