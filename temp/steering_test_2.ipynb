{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, ActivationsStore, HookedSAETransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sae_cooccurrence.utils.set_paths import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "git_root = get_git_root()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca21d50c19a468d82fe8864c562c40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdc7da07d624e33b29fe10faa64cb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/training/activations_store.py:245: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name\n",
    "    sae_id=\"layer_12/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=4,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens can be either black or white. Complete the following sentences, always use numbers (one, two, three, etc.) never digits (1, 2, 3, etc.):\n",
      "\n",
      "Training Questions:\n",
      "Example of a correct question and answer:\n",
      "Q: I have seven tokens, and all of them are white. How many of my tokens are black?\n",
      "A: none of them are black\n",
      "\n",
      "Example of a correct question and answer:\n",
      "Q: I have nine tokens, and some of them are white. How many of my tokens are black?\n",
      "A: some of them are black\n",
      "\n",
      "Example of a correct question and answer:\n",
      "Q: I have seven tokens, and one of them are white. How many of my tokens are black?\n",
      "A: six of them are black\n",
      "\n",
      "Example of an incorrect question and answer:\n",
      "Q: I have seven tokens, and one of them are white. How many of my tokens are black?\n",
      "A: 6 of them are black\n",
      "\n",
      "Example of a correct question and answer:\n",
      "Q: I have five tokens, and two of them are white. How many of my tokens are black?\n",
      "A: three of them are black\n",
      "\n",
      "Example of an incorrect question and answer:\n",
      "Q: I have five tokens, and two of them are white. How many of my tokens are black?\n",
      "A: 3 of them are black\n",
      "\n",
      "Example of a correct question and answer:\n",
      "Q: I have two tokens, and zero of them are black. How many of my tokens are white?\n",
      "A: two of them are white\n",
      "\n",
      "Example of an incorrect question and answer:\n",
      "Q: I have two tokens, and zero of them are black. How many of my tokens are white?\n",
      "A: 2 of them are white\n",
      "\n",
      "\n",
      "Test Question:\n",
      "Example of a correct question and answer:\n",
      "Q: I have five tokens, and five of them are white. How many of my tokens are black?\n",
      "\n",
      "Test Answer:\n",
      "A: zero of them are black\n"
     ]
    }
   ],
   "source": [
    "class TokenQuestionGenerator:\n",
    "    def __init__(self):\n",
    "        self.colors = [\"black\", \"white\"]\n",
    "        self.special_cases = {\"some\": \"some\", \"all\": \"none\", \"none\": \"all\"}\n",
    "\n",
    "    def _number_to_words(self, n: int) -> str:\n",
    "        \"\"\"Convert a number to words.\"\"\"\n",
    "        return num2words(n)\n",
    "\n",
    "    def _questions_are_equivalent(self, q1: str, q2: str) -> bool:\n",
    "        \"\"\"Compare two questions to check if they are functionally equivalent.\"\"\"\n",
    "        # Extract the numerical values and colors from both questions\n",
    "        q1_parts = q1.split()\n",
    "        q2_parts = q2.split()\n",
    "\n",
    "        # If lengths are different, questions can't be equivalent\n",
    "        if len(q1_parts) != len(q2_parts):\n",
    "            return False\n",
    "\n",
    "        return q1 == q2  # Direct comparison for now\n",
    "\n",
    "    def generate_numeric_question(self) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question with numeric values.\"\"\"\n",
    "        n_total = random.randint(1, 10)\n",
    "        n_color_tokens = random.randint(0, n_total)\n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"{self._number_to_words(n_color_tokens)} of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "\n",
    "        answer = f\"A: {self._number_to_words(n_total - n_color_tokens)} of them are {test_color}\"\n",
    "\n",
    "        return question, answer\n",
    "\n",
    "    def generate_special_case_question(self) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question with special quantifiers (some, all, none).\"\"\"\n",
    "        special_type = random.choice(list(self.special_cases.keys()))\n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "        n_total = random.randint(2, 10)  # Using at least 2 tokens for special cases\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"{special_type} of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "\n",
    "        answer = f\"A: {self.special_cases[special_type]} of them are {test_color}\"\n",
    "\n",
    "        return question, answer\n",
    "\n",
    "    def generate_training_set(\n",
    "        self, num_numeric: int = 5, num_special: int = 3\n",
    "    ) -> list[tuple[str, str]]:\n",
    "        \"\"\"Generate a set of training questions.\"\"\"\n",
    "        training_set = []\n",
    "\n",
    "        # Generate numeric questions\n",
    "        for _ in range(num_numeric):\n",
    "            training_set.append(self.generate_numeric_question())\n",
    "\n",
    "        # Generate special case questions\n",
    "        for _ in range(num_special):\n",
    "            training_set.append(self.generate_special_case_question())\n",
    "\n",
    "        random.shuffle(training_set)\n",
    "        return training_set\n",
    "\n",
    "    def generate_test_question(\n",
    "        self, force_numeric: bool | None = None, force_special: bool | None = None\n",
    "    ) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate a test question.\n",
    "        Parameters:\n",
    "            force_numeric: If True, generates numeric question\n",
    "            force_special: If True, generates special case question\n",
    "        If both are None, randomly chooses between numeric and special case.\n",
    "        \"\"\"\n",
    "        if force_numeric and force_special:\n",
    "            raise ValueError(\"Cannot force both numeric and special case\")\n",
    "\n",
    "        if force_numeric:\n",
    "            return self.generate_numeric_question()\n",
    "        elif force_special:\n",
    "            return self.generate_special_case_question()\n",
    "        else:\n",
    "            return random.choice(\n",
    "                [self.generate_numeric_question, self.generate_special_case_question]\n",
    "            )()\n",
    "\n",
    "\n",
    "def generate_training_and_test(\n",
    "    num_training_numeric: int = 5,\n",
    "    num_training_special: int = 3,\n",
    "    force_test_type: str | None = None,\n",
    "    max_attempts: int = 100,\n",
    ") -> dict:\n",
    "    \"\"\"Generate a complete training set and test question with labeled examples.\"\"\"\n",
    "    generator = TokenQuestionGenerator()\n",
    "\n",
    "    # Generate test question first\n",
    "    force_numeric = True if force_test_type == \"numeric\" else None\n",
    "    force_special = True if force_test_type == \"special\" else None\n",
    "    test_question, test_answer = generator.generate_test_question(\n",
    "        force_numeric=force_numeric, force_special=force_special\n",
    "    )\n",
    "\n",
    "    # Generate training set, ensuring no duplicates with test question\n",
    "    training_set = []\n",
    "    attempts = 0\n",
    "\n",
    "    while (\n",
    "        len(training_set) < (num_training_numeric + num_training_special)\n",
    "        and attempts < max_attempts\n",
    "    ):\n",
    "        current_set = generator.generate_training_set(\n",
    "            num_numeric=num_training_numeric, num_special=num_training_special\n",
    "        )\n",
    "\n",
    "        # Filter out any questions that match the test question\n",
    "        filtered_set = [\n",
    "            (q, a)\n",
    "            for q, a in current_set\n",
    "            if not generator._questions_are_equivalent(q, test_question)\n",
    "        ]\n",
    "\n",
    "        # For each question-answer pair, create labeled versions\n",
    "        labeled_set = []\n",
    "        for q, a in filtered_set:\n",
    "            # Check if this is a numeric question (contains no special case words)\n",
    "            is_numeric = not any(word in q.lower() for word in [\"some\", \"all\", \"none\"])\n",
    "\n",
    "            # Add correct version with word numbers\n",
    "            labeled_set.append(\n",
    "                (\"Example of a correct question and answer:\", q, a)\n",
    "            )  # Changed order here\n",
    "\n",
    "            # Only create incorrect digit version for numeric questions\n",
    "            if is_numeric:\n",
    "                incorrect_a = a.replace(\"A: \", \"A: \")\n",
    "                for word_num in [\n",
    "                    \"zero\",\n",
    "                    \"one\",\n",
    "                    \"two\",\n",
    "                    \"three\",\n",
    "                    \"four\",\n",
    "                    \"five\",\n",
    "                    \"six\",\n",
    "                    \"seven\",\n",
    "                    \"eight\",\n",
    "                    \"nine\",\n",
    "                    \"ten\",\n",
    "                ]:\n",
    "                    digit = str(\n",
    "                        [\n",
    "                            \"zero\",\n",
    "                            \"one\",\n",
    "                            \"two\",\n",
    "                            \"three\",\n",
    "                            \"four\",\n",
    "                            \"five\",\n",
    "                            \"six\",\n",
    "                            \"seven\",\n",
    "                            \"eight\",\n",
    "                            \"nine\",\n",
    "                            \"ten\",\n",
    "                        ].index(word_num)\n",
    "                    )\n",
    "                    incorrect_a = incorrect_a.replace(word_num, digit)\n",
    "                labeled_set.append(\n",
    "                    (\"Example of an incorrect question and answer:\", q, incorrect_a)\n",
    "                )  # Changed order here\n",
    "\n",
    "        training_set = labeled_set\n",
    "        attempts += 1\n",
    "\n",
    "    if attempts >= max_attempts:\n",
    "        raise RuntimeError(\n",
    "            \"Failed to generate unique training set after maximum attempts\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"introduction\": \"Tokens can be either black or white. Complete the following sentences, always use numbers (one, two, three, etc.) never digits (1, 2, 3, etc.):\",\n",
    "        \"training_questions\": training_set,\n",
    "        \"test_question\": f\"Example of a correct question and answer:\\n{test_question}\",\n",
    "        \"test_answer\": test_answer,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    result = generate_training_and_test(\n",
    "        num_training_numeric=3,\n",
    "        num_training_special=2,\n",
    "        force_test_type=\"numeric\",  # Can be 'numeric', 'special', or None\n",
    "    )\n",
    "\n",
    "    print(result[\"introduction\"])\n",
    "    print(\"\\nTraining Questions:\")\n",
    "    for q, a, label in result[\"training_questions\"]:\n",
    "        print(f\"{q}\\n{a}\\n{label}\\n\")\n",
    "    print(\"\\nTest Question:\")\n",
    "    print(result[\"test_question\"])\n",
    "    print(\"\\nTest Answer:\")\n",
    "    print(result[\"test_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index. This is useful for\n",
    "    calibrating the right amount of the feature to add.\n",
    "    \"\"\"\n",
    "    max_activation = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae.cfg.hook_name],\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0, 1)\n",
    "        batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "        max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "\n",
    "def steering(activations, steering_strength=1.0, steering_vector=None, max_act=1.0):\n",
    "    # Note if the feature fires anyway, we'd be adding to that here.\n",
    "    return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=False if device == \"mps\" else True,\n",
    "            prepend_bos=sae.cfg.prepend_bos,\n",
    "        )\n",
    "\n",
    "    return model.tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015aaa07b7f64f4b988968fb1404353d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Test without steering\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     normal_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_steering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResults without steering:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumeric accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnormal_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mevaluate_model_with_steering\u001b[0;34m(model, sae, feature_to_steer, steering_strength, max_act, n_numeric, n_special, max_new_tokens)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Convert prompt to tokens first\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_tokens(prompt, prepend_bos\u001b[38;5;241m=\u001b[39msae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mprepend_bos)\n\u001b[0;32m---> 66\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# temperature=0.7,\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# top_p=0.9,\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_at_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     generated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     76\u001b[0m test_result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m: generated,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m generated,\n\u001b[1;32m     81\u001b[0m }\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:2155\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2148\u001b[0m             tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m   2149\u001b[0m             return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2152\u001b[0m             past_kv_cache\u001b[38;5;241m=\u001b[39mpast_kv_cache,\n\u001b[1;32m   2153\u001b[0m         )\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;66;03m# We input the entire sequence, as a [batch, pos] tensor, since we aren't using\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m     \u001b[38;5;66;03m# the cache.\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2166\u001b[0m         tokens,\n\u001b[1;32m   2167\u001b[0m         return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2168\u001b[0m         prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos,\n\u001b[1;32m   2169\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   2170\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:573\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    570\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 573\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:174\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(attn_out)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/rms_norm.py:44\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     41\u001b[0m scale: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos 1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_scale(\n\u001b[1;32m     42\u001b[0m     (x\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_normalized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# [batch, pos, length]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1732\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1729\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model_with_steering(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    feature_to_steer: int | None = None,\n",
    "    steering_strength: float = 1.0,\n",
    "    max_act: float = 60.0,\n",
    "    n_numeric: int = 10,\n",
    "    n_special: int = 10,\n",
    "    max_new_tokens: int = 5,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test model accuracy on token counting tasks with optional feature steering.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: The sparse autoencoder\n",
    "        feature_to_steer: Feature index to steer, or None for no steering\n",
    "        steering_strength: Strength of steering (default 1.0)\n",
    "        max_act: Maximum activation for the steered feature\n",
    "        n_numeric: Number of numeric test questions\n",
    "        n_special: Number of special test questions\n",
    "        max_new_tokens: Maximum tokens to generate for each answer\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics and test results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"numeric_correct\": 0,\n",
    "        \"special_correct\": 0,\n",
    "        \"numeric_tests\": [],\n",
    "        \"special_tests\": [],\n",
    "    }\n",
    "\n",
    "    for test_type in [\"numeric\", \"special\"]:\n",
    "        n_tests = n_numeric if test_type == \"numeric\" else n_special\n",
    "        for _ in range(n_tests):\n",
    "            test_data = generate_training_and_test(\n",
    "                num_training_numeric=40,\n",
    "                num_training_special=20,\n",
    "                force_test_type=test_type,\n",
    "            )\n",
    "\n",
    "            prompt = (\n",
    "                test_data[\"introduction\"]\n",
    "                + \"\\n\\n\"\n",
    "                + \"\\n\\n\".join(\n",
    "                    f\"{q}\\n{a}\\n{label}\"\n",
    "                    for q, a, label in test_data[\"training_questions\"]\n",
    "                )\n",
    "                + f\"\\n\\n{test_data['test_question']}\\nA: \"\n",
    "            )\n",
    "\n",
    "            if feature_to_steer is not None:\n",
    "                generated = generate_with_steering(\n",
    "                    model,\n",
    "                    sae,\n",
    "                    prompt,\n",
    "                    feature_to_steer,\n",
    "                    max_act,\n",
    "                    steering_strength=steering_strength,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                )\n",
    "            else:\n",
    "                # Convert prompt to tokens first\n",
    "                input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    # temperature=0.7,\n",
    "                    # top_p=0.9,\n",
    "                    stop_at_eos=False if device == \"mps\" else True,\n",
    "                    prepend_bos=sae.cfg.prepend_bos,\n",
    "                )\n",
    "                generated = model.tokenizer.decode(output[0])\n",
    "\n",
    "            test_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"expected\": test_data[\"test_answer\"],\n",
    "                \"generated\": generated,\n",
    "                \"correct\": test_data[\"test_answer\"] in generated,\n",
    "            }\n",
    "\n",
    "            if test_type == \"numeric\":\n",
    "                results[\"numeric_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"numeric_correct\"] += 1\n",
    "            else:\n",
    "                results[\"special_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"special_correct\"] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    results[\"numeric_accuracy\"] = results[\"numeric_correct\"] / n_numeric\n",
    "    results[\"special_accuracy\"] = results[\"special_correct\"] / n_special\n",
    "    results[\"total_accuracy\"] = (\n",
    "        results[\"numeric_correct\"] + results[\"special_correct\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Test without steering\n",
    "    normal_results = evaluate_model_with_steering(model, sae)\n",
    "    print(\"\\nResults without steering:\")\n",
    "    print(f\"Numeric accuracy: {normal_results['numeric_accuracy']:.2%}\")\n",
    "    print(f\"Special accuracy: {normal_results['special_accuracy']:.2%}\")\n",
    "    print(f\"Total accuracy: {normal_results['total_accuracy']:.2%}\")\n",
    "\n",
    "    print(\"\\nNumeric test results:\")\n",
    "    for i, test in enumerate(normal_results[\"numeric_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    print(\"\\nSpecial test results:\")\n",
    "    for i, test in enumerate(normal_results[\"special_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    # Test with feature steering\n",
    "    feature_to_steer = 12257  # Replace with your feature of interest\n",
    "    steering_strength = 0.0\n",
    "    steered_results = evaluate_model_with_steering(\n",
    "        model,\n",
    "        sae,\n",
    "        feature_to_steer=feature_to_steer,\n",
    "        steering_strength=steering_strength,\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nResults with feature {feature_to_steer} steered (strength {steering_strength}):\"\n",
    "    )\n",
    "    print(f\"Numeric accuracy: {steered_results['numeric_accuracy']:.2%}\")\n",
    "    print(f\"Special accuracy: {steered_results['special_accuracy']:.2%}\")\n",
    "    print(f\"Total accuracy: {steered_results['total_accuracy']:.2%}\")\n",
    "\n",
    "    print(\"\\nNumeric test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"numeric_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    print(\"\\nSpecial test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"special_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    # Test with feature steering\n",
    "    feature_to_steer = 15441  # Replace with your feature of interest\n",
    "    steering_strength = 0.0\n",
    "    steered_results = evaluate_model_with_steering(\n",
    "        model,\n",
    "        sae,\n",
    "        feature_to_steer=feature_to_steer,\n",
    "        steering_strength=steering_strength,\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nResults with feature {feature_to_steer} steered (strength {steering_strength}):\"\n",
    "    )\n",
    "    print(f\"Numeric accuracy: {steered_results['numeric_accuracy']:.2%}\")\n",
    "    print(f\"Special accuracy: {steered_results['special_accuracy']:.2%}\")\n",
    "    print(f\"Total accuracy: {steered_results['total_accuracy']:.2%}\")\n",
    "\n",
    "    print(\"\\nNumeric test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"numeric_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "    print(\"\\nSpecial test results (with steering):\")\n",
    "    for i, test in enumerate(steered_results[\"special_tests\"], 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {test['expected']}\")\n",
    "        print(f\"Generated: {test['generated']}\")\n",
    "        print(f\"Correct: {test['correct']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_intervention(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    feature_ids: int | list[int] | None = None,\n",
    "    intervention_type: str = \"none\",  # \"none\", \"steering\", or \"ablation\"\n",
    "    steering_strength: float = 1.0,\n",
    "    max_act: float = 60.0,\n",
    "    n_numeric: int = 10,\n",
    "    n_special: int = 10,\n",
    "    max_new_tokens: int = 6,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test model accuracy on token counting tasks with optional feature steering or ablation.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: The sparse autoencoder\n",
    "        feature_ids: Feature index(es) to intervene on, or None for no intervention\n",
    "        intervention_type: Type of intervention (\"none\", \"steering\", or \"ablation\")\n",
    "        steering_strength: Strength of steering (default 1.0)\n",
    "        max_act: Maximum activation for the steered feature\n",
    "        n_numeric: Number of numeric test questions\n",
    "        n_special: Number of special test questions\n",
    "        max_new_tokens: Maximum tokens to generate for each answer\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics and test results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"numeric_correct_strict\": 0,\n",
    "        \"numeric_correct_lenient\": 0,\n",
    "        \"special_correct_strict\": 0,\n",
    "        \"special_correct_lenient\": 0,\n",
    "        \"numeric_tests\": [],\n",
    "        \"special_tests\": [],\n",
    "    }\n",
    "\n",
    "    def convert_words_to_digits(text: str) -> str:\n",
    "        \"\"\"Convert number words to digits in the text.\"\"\"\n",
    "        word_to_digit = {\n",
    "            \"zero\": \"0\",\n",
    "            \"none\": \"0\",\n",
    "            \"one\": \"1\",\n",
    "            \"two\": \"2\",\n",
    "            \"three\": \"3\",\n",
    "            \"four\": \"4\",\n",
    "            \"five\": \"5\",\n",
    "            \"six\": \"6\",\n",
    "            \"seven\": \"7\",\n",
    "            \"eight\": \"8\",\n",
    "            \"nine\": \"9\",\n",
    "            \"ten\": \"10\",\n",
    "        }\n",
    "        for word, digit in word_to_digit.items():\n",
    "            text = text.replace(word, digit)\n",
    "        return text\n",
    "\n",
    "    # Convert single feature_id to list for consistency\n",
    "    if isinstance(feature_ids, int):\n",
    "        feature_ids = [feature_ids]\n",
    "\n",
    "    def ablate_feature_hook(feature_activations, hook=None, feature_ids=None):  # noqa: ARG001\n",
    "        feature_activations[:, :, feature_ids] = 0\n",
    "        return feature_activations\n",
    "\n",
    "    for test_type in [\"numeric\", \"special\"]:\n",
    "        n_tests = n_numeric if test_type == \"numeric\" else n_special\n",
    "        for _ in range(n_tests):\n",
    "            test_data = generate_training_and_test(\n",
    "                num_training_numeric=40,\n",
    "                num_training_special=20,\n",
    "                force_test_type=test_type,\n",
    "            )\n",
    "\n",
    "            prompt = (\n",
    "                test_data[\"introduction\"]\n",
    "                + \"\\n\\n\"\n",
    "                + \"\\n\\n\".join(\n",
    "                    f\"{q}\\n{a}\\n{label}\"\n",
    "                    for q, a, label in test_data[\"training_questions\"]\n",
    "                )\n",
    "                + f\"\\n\\n{test_data['test_question']}\\nA: \"\n",
    "            )\n",
    "\n",
    "            # Convert prompt to tokens first\n",
    "            input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "            if intervention_type == \"none\" or feature_ids is None:\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    stop_at_eos=False if device == \"mps\" else True,\n",
    "                    prepend_bos=sae.cfg.prepend_bos,\n",
    "                )\n",
    "\n",
    "            elif intervention_type == \"steering\":\n",
    "                steering_vector = sae.W_dec[feature_ids[0]].to(model.cfg.device)\n",
    "                steering_hook = partial(\n",
    "                    steering,\n",
    "                    steering_vector=steering_vector,\n",
    "                    steering_strength=steering_strength,\n",
    "                    max_act=max_act,\n",
    "                )\n",
    "\n",
    "                with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "                    output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        stop_at_eos=False if device == \"mps\" else True,\n",
    "                        prepend_bos=sae.cfg.prepend_bos,\n",
    "                    )\n",
    "\n",
    "            elif intervention_type == \"ablation\":\n",
    "                ablation_hook = partial(ablate_feature_hook, feature_ids=feature_ids)\n",
    "                model.add_sae(sae)\n",
    "                hook_point = sae.cfg.hook_name + \".hook_sae_acts_post\"\n",
    "\n",
    "                with model.hooks(fwd_hooks=[(hook_point, ablation_hook)]):\n",
    "                    output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        stop_at_eos=False if device == \"mps\" else True,\n",
    "                        prepend_bos=sae.cfg.prepend_bos,\n",
    "                    )\n",
    "\n",
    "                model.reset_hooks()\n",
    "                model.reset_saes()\n",
    "\n",
    "            generated = model.tokenizer.decode(output[0])  # type: ignore\n",
    "            generated_answer = generated.split(\"A: \")[-1].strip()\n",
    "            expected_answer = test_data[\"test_answer\"].replace(\"A: \", \"\").strip()\n",
    "\n",
    "            # Create lenient versions of answers\n",
    "            generated_lenient = convert_words_to_digits(generated_answer.lower())\n",
    "            expected_lenient = convert_words_to_digits(expected_answer.lower())\n",
    "\n",
    "            test_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"expected\": expected_answer,\n",
    "                \"generated\": generated_answer,\n",
    "                \"correct_strict\": expected_answer in generated_answer,\n",
    "                \"correct_lenient\": expected_lenient in generated_lenient,\n",
    "            }\n",
    "\n",
    "            if test_type == \"numeric\":\n",
    "                results[\"numeric_tests\"].append(test_result)\n",
    "                if test_result[\"correct_strict\"]:\n",
    "                    results[\"numeric_correct_strict\"] += 1\n",
    "                if test_result[\"correct_lenient\"]:\n",
    "                    results[\"numeric_correct_lenient\"] += 1\n",
    "            else:\n",
    "                results[\"special_tests\"].append(test_result)\n",
    "                if test_result[\"correct_strict\"]:\n",
    "                    results[\"special_correct_strict\"] += 1\n",
    "                if test_result[\"correct_lenient\"]:\n",
    "                    results[\"special_correct_lenient\"] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    results[\"numeric_accuracy_strict\"] = results[\"numeric_correct_strict\"] / n_numeric\n",
    "    results[\"numeric_accuracy_lenient\"] = results[\"numeric_correct_lenient\"] / n_numeric\n",
    "    results[\"special_accuracy_strict\"] = results[\"special_correct_strict\"] / n_special\n",
    "    results[\"special_accuracy_lenient\"] = results[\"special_correct_lenient\"] / n_special\n",
    "    results[\"total_accuracy_strict\"] = (\n",
    "        results[\"numeric_correct_strict\"] + results[\"special_correct_strict\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "    results[\"total_accuracy_lenient\"] = (\n",
    "        results[\"numeric_correct_lenient\"] + results[\"special_correct_lenient\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a7917c15c24354bfb3b838fcbd6532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e36734c09646be819eff4d05ff58d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_results = evaluate_model_with_intervention(\n",
    "    model, sae, intervention_type=\"none\", n_numeric=1, n_special=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Tokens can be either black or white. Complete the following sentences, always use numbers (one, two, three, etc.) never digits (1, 2, 3, etc.):\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and four of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and four of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and one of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have five tokens, and one of them are white. How many of my tokens are black?\\nA: 4 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and four of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and four of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nExample of a correct question and answer:\\nQ: I have one tokens, and zero of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have one tokens, and zero of them are white. How many of my tokens are black?\\nA: 1 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have ten tokens, and six of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have ten tokens, and six of them are white. How many of my tokens are black?\\nA: 4 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have one tokens, and one of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have one tokens, and one of them are white. How many of my tokens are black?\\nA: 0 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and zero of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and zero of them are white. How many of my tokens are black?\\nA: 4 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and seven of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have seven tokens, and seven of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nExample of a correct question and answer:\\nQ: I have one tokens, and zero of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have one tokens, and zero of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have ten tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nExample of a correct question and answer:\\nQ: I have two tokens, and two of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have two tokens, and two of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and zero of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have three tokens, and zero of them are black. How many of my tokens are white?\\nA: 3 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and zero of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have three tokens, and zero of them are black. How many of my tokens are white?\\nA: 3 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have two tokens, and one of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have two tokens, and one of them are white. How many of my tokens are black?\\nA: 1 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and zero of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have six tokens, and zero of them are black. How many of my tokens are white?\\nA: 6 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and four of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have five tokens, and four of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and zero of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have six tokens, and zero of them are black. How many of my tokens are white?\\nA: 6 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nExample of a correct question and answer:\\nQ: I have eight tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and four of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and four of them are white. How many of my tokens are black?\\nA: 0 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have two tokens, and two of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have two tokens, and two of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and five of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have six tokens, and five of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and two of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have five tokens, and two of them are white. How many of my tokens are black?\\nA: 3 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nExample of a correct question and answer:\\nQ: I have eight tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and zero of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have seven tokens, and zero of them are black. How many of my tokens are white?\\nA: 7 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have one tokens, and one of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have one tokens, and one of them are white. How many of my tokens are black?\\nA: 0 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have one tokens, and zero of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have one tokens, and zero of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and six of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have six tokens, and six of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and three of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have three tokens, and three of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nExample of a correct question and answer:\\nQ: I have nine tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nExample of a correct question and answer:\\nQ: I have eight tokens, and one of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have eight tokens, and one of them are black. How many of my tokens are white?\\nA: 7 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have three tokens, and zero of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have three tokens, and zero of them are white. How many of my tokens are black?\\nA: 3 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have two tokens, and zero of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have two tokens, and zero of them are black. How many of my tokens are white?\\nA: 2 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have one tokens, and one of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have one tokens, and one of them are black. How many of my tokens are white?\\nA: 0 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and zero of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and zero of them are black. How many of my tokens are white?\\nA: 4 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and one of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have seven tokens, and one of them are black. How many of my tokens are white?\\nA: 6 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have ten tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and five of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have seven tokens, and five of them are black. How many of my tokens are white?\\nA: 2 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have nine tokens, and eight of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have nine tokens, and eight of them are white. How many of my tokens are black?\\nA: 1 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have eight tokens, and seven of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have eight tokens, and seven of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nExample of a correct question and answer:\\nQ: I have ten tokens, and two of them are white. How many of my tokens are black?\\nA: eight of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have ten tokens, and two of them are white. How many of my tokens are black?\\nA: 8 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and one of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have five tokens, and one of them are black. How many of my tokens are white?\\nA: 4 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have six tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nExample of a correct question and answer:\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: 1 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have five tokens, and three of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nExample of an incorrect question and answer:\\nQ: I have five tokens, and three of them are black. How many of my tokens are white?\\nA: 2 of them are white\\n\\nExample of a correct question and answer:\\nQ: I have seven tokens, and three of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nExample of an incorrect question and answer:\\nQ: I have seven tokens, and three of them are white. How many of my tokens are black?\\nA: 4 of them are black\\n\\nExample of a correct question and answer:\\nQ: I have two tokens, and all of them are black. How many of my tokens are white?\\nA: ', 'expected': 'none of them are white', 'generated': '0 of them are white', 'correct_strict': False, 'correct_lenient': True}]\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"special_tests\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceada074d52546bc8344ed755df76de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75463859eff24aee834195b85070e6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    "    n_numeric=1,\n",
    "    n_special=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c70590380324134982e1583077da653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb5c385d50b4baeafb269163d3d3e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398f0e962d4f4c4da5cf39c8c3f2c7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a18277b8dc4aa18598e6ec29648be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b152b95e8f54f1b9f5334ea1a6abc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314e44523f5a4d3ca2f1da924f8b72c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34506c016c048bd93e138e1a143bc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2cc540b48b44e192eedd5944931774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922945b58534b14ae44992018f22be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aa8e2987f24b29920f9d79bd8b6dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b662626c74e943d7b99576fdceaf0e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a841fed9c33e4730941e859adfebadd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d234b6c70344db86279554992b284d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e72e643dc14bc597221d1a488469cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807a64bd4ac44492b9f20f969f3d54b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054cf4b78e4a4cfb9afb980378512569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d948595388840c1a6184749dfbc1b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc58c993f894251a812f9eb4100ff56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a348101f9684db6a924aeaa09de1167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0255c3eff1142f18bcba2a04af062bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572a942cb15f4042bd751a5f4d245260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate_model_with_intervention.<locals>.ablate_feature_hook() got an unexpected keyword argument 'hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m normal_results \u001b[38;5;241m=\u001b[39m evaluate_model_with_intervention(model, sae, intervention_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# # Test with steering\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# steered_results = evaluate_model_with_intervention(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Test with ablation\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m ablated_results_hub \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_intervention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12257\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Can ablate multiple features\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mablation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m ablated_results_some \u001b[38;5;241m=\u001b[39m evaluate_model_with_intervention(\n\u001b[1;32m     22\u001b[0m     model,\n\u001b[1;32m     23\u001b[0m     sae,\n\u001b[1;32m     24\u001b[0m     feature_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15441\u001b[39m],  \u001b[38;5;66;03m# Can ablate multiple features\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     intervention_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mablation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m ablated_results_all \u001b[38;5;241m=\u001b[39m evaluate_model_with_intervention(\n\u001b[1;32m     29\u001b[0m     model,\n\u001b[1;32m     30\u001b[0m     sae,\n\u001b[1;32m     31\u001b[0m     feature_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m12649\u001b[39m],  \u001b[38;5;66;03m# Can ablate multiple features\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     intervention_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mablation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m )\n",
      "Cell \u001b[0;32mIn[22], line 121\u001b[0m, in \u001b[0;36mevaluate_model_with_intervention\u001b[0;34m(model, sae, feature_ids, intervention_type, steering_strength, max_act, n_numeric, n_special, max_new_tokens)\u001b[0m\n\u001b[1;32m    118\u001b[0m hook_point \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hook_sae_acts_post\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_point, ablation_hook)]):\n\u001b[0;32m--> 121\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_at_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[1;32m    129\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_saes()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:2155\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2148\u001b[0m             tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m   2149\u001b[0m             return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2152\u001b[0m             past_kv_cache\u001b[38;5;241m=\u001b[39mpast_kv_cache,\n\u001b[1;32m   2153\u001b[0m         )\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;66;03m# We input the entire sequence, as a [batch, pos] tensor, since we aren't using\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m     \u001b[38;5;66;03m# the cache.\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2166\u001b[0m         tokens,\n\u001b[1;32m   2167\u001b[0m         return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2168\u001b[0m         prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos,\n\u001b[1;32m   2169\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   2170\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:573\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    570\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 573\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:183\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n\u001b[1;32m    182\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mlp(normalized_resid_mid)\n\u001b[0;32m--> 183\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_resid_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_out\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     normalized_resid_pre_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(\n\u001b[1;32m    188\u001b[0m         resid_pre \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_pre\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    189\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/sae.py:389\u001b[0m, in \u001b[0;36mSAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    387\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 389\u001b[0m     feature_acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     sae_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(feature_acts)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# TEMP\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/sae.py:533\u001b[0m, in \u001b[0;36mSAE.encode_jumprelu\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# \"... d_in, d_in d_sae -> ... d_sae\",\u001b[39;00m\n\u001b[1;32m    531\u001b[0m hidden_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_sae_acts_pre(sae_in \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_enc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_enc)\n\u001b[0;32m--> 533\u001b[0m feature_acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_sae_acts_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_pre\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_pre\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_acts\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1803\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1801\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1803\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1806\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/hook_points.py:109\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     module_output \u001b[38;5;241m=\u001b[39m module_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_model_with_intervention.<locals>.ablate_feature_hook() got an unexpected keyword argument 'hook'"
     ]
    }
   ],
   "source": [
    "# Test without intervention\n",
    "normal_results = evaluate_model_with_intervention(model, sae, intervention_type=\"none\")\n",
    "\n",
    "# # Test with steering\n",
    "# steered_results = evaluate_model_with_intervention(\n",
    "#     model,\n",
    "#     sae,\n",
    "#     feature_ids=12257,\n",
    "#     intervention_type=\"steering\",\n",
    "#     steering_strength=1.0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65522729e32f47589ce88f7ac62a5a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72a6521a45f41f786ccdd35193fda9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3043cf01ce47c28ad82a347c12cc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2673df75704d30abfef6531beccce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bb1c24ed5244c7a2bb2aa04d7f3bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee897d4551346d092dea10001a3bda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67136e5798bb436ab5219084f1e06d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e85d7ffd884e6a81c30778d4010999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f457313bbc643328760d0306845de50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3adeb52f9d34ff5ba34a38eb4f8f360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9039c59f1e4646b7c76a3af1ffe19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739c3b3bbcff497a981e8eabfb439089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb620e1a2d614c3fa93d417969107a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94966f3a3bf74efe8c41cee8b7bd9aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124a6ce63692413c9b4d9b8e1768ea2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10133d670f084e068245a29e382a0abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2e5bb8a6c1439fbb8912ab7192cd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test with ablation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ablated_results_hub \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_intervention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12257\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Can ablate multiple features\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mablation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ablated_results_some \u001b[38;5;241m=\u001b[39m evaluate_model_with_intervention(\n\u001b[1;32m     10\u001b[0m     model,\n\u001b[1;32m     11\u001b[0m     sae,\n\u001b[1;32m     12\u001b[0m     feature_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15441\u001b[39m],  \u001b[38;5;66;03m# Can ablate multiple features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     intervention_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mablation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m ablated_results_all \u001b[38;5;241m=\u001b[39m evaluate_model_with_intervention(\n\u001b[1;32m     17\u001b[0m     model,\n\u001b[1;32m     18\u001b[0m     sae,\n\u001b[1;32m     19\u001b[0m     feature_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m12649\u001b[39m],  \u001b[38;5;66;03m# Can ablate multiple features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     intervention_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mablation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n",
      "Cell \u001b[0;32mIn[31], line 121\u001b[0m, in \u001b[0;36mevaluate_model_with_intervention\u001b[0;34m(model, sae, feature_ids, intervention_type, steering_strength, max_act, n_numeric, n_special, max_new_tokens)\u001b[0m\n\u001b[1;32m    118\u001b[0m hook_point \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hook_sae_acts_post\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_point, ablation_hook)]):\n\u001b[0;32m--> 121\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_at_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[1;32m    129\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_saes()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:2147\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_past_kv_cache:\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;66;03m# We just take the final tokens, as a [batch, 1] tensor\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2147\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2151\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2155\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2156\u001b[0m             tokens,\n\u001b[1;32m   2157\u001b[0m             return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2160\u001b[0m             past_kv_cache\u001b[38;5;241m=\u001b[39mpast_kv_cache,\n\u001b[1;32m   2161\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:573\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    570\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 573\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:251\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    248\u001b[0m     attn_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattention_dir \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# If causal attention, we mask it to only attend backwards. If bidirectional, we don't mask.\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_pos_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m additive_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     attn_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m additive_attention_mask\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:460\u001b[0m, in \u001b[0;36mAbstractAttention.apply_causal_mask\u001b[0;34m(self, attn_scores, past_kv_pos_offset, attention_mask)\u001b[0m\n\u001b[1;32m    458\u001b[0m     einsum_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch head pos offset_pos, batch offset_pos -> batch head pos offset_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     final_mask \u001b[38;5;241m=\u001b[39m final_mask\u001b[38;5;241m.\u001b[39mto(attention_mask\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 460\u001b[0m     final_mask \u001b[38;5;241m=\u001b[39m \u001b[43meinops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39mto(final_mask\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(final_mask, attn_scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mIGNORE)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test with ablation\n",
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_all = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_spokes = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_hub_spoke_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the effect of ablation of the hub alone both spokes alone hub and both spokes together and both spokes together I hope hub and spokes will be more effective than any of these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'total_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnormal_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ablated_results_hub[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ablated_results_some[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_accuracy'"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"total_accuracy\"])\n",
    "print(ablated_results_hub[\"total_accuracy\"])\n",
    "print(ablated_results_some[\"total_accuracy\"])\n",
    "print(ablated_results_all[\"total_accuracy\"])\n",
    "print(ablation_results_spokes[\"total_accuracy\"])\n",
    "print(ablation_results_hub_spoke_some[\"total_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.9\n",
      "0.7\n",
      "0.8\n",
      "0.7\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"numeric_accuracy\"])\n",
    "print(ablated_results_hub[\"numeric_accuracy\"])\n",
    "print(ablated_results_some[\"numeric_accuracy\"])\n",
    "print(ablated_results_all[\"numeric_accuracy\"])\n",
    "print(ablation_results_spokes[\"numeric_accuracy\"])\n",
    "print(ablation_results_hub_spoke_some[\"numeric_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"special_accuracy\"])\n",
    "print(ablated_results_hub[\"special_accuracy\"])\n",
    "print(ablated_results_some[\"special_accuracy\"])\n",
    "print(ablated_results_all[\"special_accuracy\"])\n",
    "print(ablation_results_spokes[\"special_accuracy\"])\n",
    "print(ablation_results_hub_spoke_some[\"special_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without intervention\n",
    "normal_results = evaluate_model_with_intervention(model, sae, intervention_type=\"none\")\n",
    "\n",
    "# # Test with steering\n",
    "# steered_results = evaluate_model_with_intervention(\n",
    "#     model,\n",
    "#     sae,\n",
    "#     feature_ids=12257,\n",
    "#     intervention_type=\"steering\",\n",
    "#     steering_strength=1.0,\n",
    "# )\n",
    "\n",
    "# Test with ablation\n",
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_all = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_spokes = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_hub_spoke_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-cooccurence-DZTJ6ajw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
