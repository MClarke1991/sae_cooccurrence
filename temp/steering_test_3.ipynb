{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from num2words import num2words\n",
    "\n",
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, ActivationsStore, HookedSAETransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers.utils.logging import disable_progress_bar\n",
    "\n",
    "from sae_cooccurrence.utils.set_paths import get_git_root\n",
    "\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "git_root = get_git_root()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec8e54dfe0a4319a2dd1a8ede7128d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py:245: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name\n",
    "    sae_id=\"layer_12/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=4,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\n",
      "Remember:\n",
      "- For 0, use 'zero' not '0'\n",
      "- The total number of tokens equals the sum of black and white tokens\n",
      "- Always write numbers as words (e.g., 'two' not '2')\n",
      "\n",
      "Training Questions:\n",
      "\n",
      "# Basic cases - all/none\n",
      "Q: I have ten tokens, and all of them are white. How many of my tokens are black?\n",
      "A: none of them are black\n",
      "\n",
      "\n",
      "# Basic cases - all/none\n",
      "Q: I have nine tokens, and none of them are white. How many of my tokens are black?\n",
      "A: all of them are black\n",
      "\n",
      "\n",
      "# Basic cases - all/none\n",
      "Q: I have seven tokens, and none of them are black. How many of my tokens are white?\n",
      "A: all of them are white\n",
      "\n",
      "\n",
      "# Basic cases - all/none\n",
      "Q: I have three tokens, and none of them are black. How many of my tokens are white?\n",
      "A: all of them are white\n",
      "\n",
      "\n",
      "# Basic cases - all/none\n",
      "Q: I have seven tokens, and some of them are black. How many of my tokens are white?\n",
      "A: some of them are white\n",
      "\n",
      "\n",
      "# Cases with specific numbers\n",
      "Q: I have seven tokens, and three of them are black. How many of my tokens are white?\n",
      "A: four of them are white\n",
      "\n",
      "\n",
      "# Cases with specific numbers\n",
      "Q: I have three tokens, and two of them are black. How many of my tokens are white?\n",
      "A: one of them are white\n",
      "\n",
      "\n",
      "# Cases with specific numbers\n",
      "Q: I have seven tokens, and two of them are black. How many of my tokens are white?\n",
      "A: five of them are white\n",
      "\n",
      "\n",
      "# Cases with 'some'\n",
      "Q: I have three tokens, and some of them are black. How many of my tokens are white?\n",
      "A: some of them are white\n",
      "\n",
      "\n",
      "# Cases with 'some'\n",
      "Q: I have six tokens, and some of them are white. How many of my tokens are black?\n",
      "A: some of them are black\n",
      "\n",
      "\n",
      "# Zero cases\n",
      "Q: I have ten tokens, and all of them are white. How many of my tokens are black?\n",
      "A: zero of them are black\n",
      "\n",
      "\n",
      "# Zero cases\n",
      "Q: I have ten tokens, and all of them are white. How many of my tokens are black?\n",
      "A: zero of them are black\n",
      "\n",
      "\n",
      "Test Question:\n",
      "Q: I have seven tokens, and one of them are white. How many of my tokens are black?\n",
      "\n",
      "Expected Answer:\n",
      "A: six of them are black\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from num2words import num2words\n",
    "\n",
    "\n",
    "class TokenQuestionGenerator:\n",
    "    def __init__(self):\n",
    "        self.colors = [\"black\", \"white\"]\n",
    "        self.special_cases = {\"some\": \"some\", \"all\": \"none\", \"none\": \"all\"}\n",
    "        self.sections = [\"basic\", \"numeric\", \"special\", \"zero\"]\n",
    "\n",
    "    def _number_to_words(self, n: int) -> str:\n",
    "        \"\"\"Convert a number to words.\"\"\"\n",
    "        return num2words(n)\n",
    "\n",
    "    def _questions_are_equivalent(self, q1: str, q2: str) -> bool:\n",
    "        \"\"\"Compare two questions to check if they are functionally equivalent.\"\"\"\n",
    "        # Extract key parts for comparison\n",
    "        q1_parts = [p.lower() for p in q1.split() if p.lower() not in [\"q:\", \"a:\", \"and\", \"are\", \"of\", \"them\", \"my\", \"tokens\", \"have\", \"how\", \"many\"]]\n",
    "        q2_parts = [p.lower() for p in q2.split() if p.lower() not in [\"q:\", \"a:\", \"and\", \"are\", \"of\", \"them\", \"my\", \"tokens\", \"have\", \"how\", \"many\"]]\n",
    "        return q1_parts == q2_parts\n",
    "\n",
    "    def generate_zero_case_question(self) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question where one color has zero tokens.\"\"\"\n",
    "        n_total = random.randint(2, 10)\n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"all of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "        answer = f\"A: zero of them are {test_color}\"\n",
    "        return question, answer\n",
    "\n",
    "    def generate_numeric_question(self, force_complementary: bool = False) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question with numeric values.\"\"\"\n",
    "        n_total = random.randint(3, 10)  # Minimum 3 tokens for more interesting cases\n",
    "        if force_complementary:\n",
    "            # Generate numbers that sum to interesting complements\n",
    "            n_color_tokens = random.randint(1, n_total - 1)  # Ensure at least 1 token of each color\n",
    "        else:\n",
    "            n_color_tokens = random.randint(0, n_total)\n",
    "        \n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"{self._number_to_words(n_color_tokens)} of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "        answer = f\"A: {self._number_to_words(n_total - n_color_tokens)} of them are {test_color}\"\n",
    "        return question, answer\n",
    "\n",
    "    def generate_special_case_question(self) -> tuple[str, str]:\n",
    "        \"\"\"Generate a question with special quantifiers (some, all, none).\"\"\"\n",
    "        special_type = random.choice(list(self.special_cases.keys()))\n",
    "        colors = random.sample(self.colors, 2)\n",
    "        held_color, test_color = colors\n",
    "        n_total = random.randint(3, 10)\n",
    "\n",
    "        question = (\n",
    "            f\"Q: I have {self._number_to_words(n_total)} tokens, and \"\n",
    "            f\"{special_type} of them are {held_color}. \"\n",
    "            f\"How many of my tokens are {test_color}?\"\n",
    "        )\n",
    "        answer = f\"A: {self.special_cases[special_type]} of them are {test_color}\"\n",
    "        return question, answer\n",
    "\n",
    "    def generate_test_question(self, force_numeric: bool | None = None, force_special: bool | None = None) -> tuple[str, str]:\n",
    "        \"\"\"Generate a test question.\"\"\"\n",
    "        if force_numeric and force_special:\n",
    "            raise ValueError(\"Cannot force both numeric and special case\")\n",
    "\n",
    "        if force_numeric:\n",
    "            return self.generate_numeric_question(force_complementary=True)\n",
    "        elif force_special:\n",
    "            return self.generate_special_case_question()\n",
    "        else:\n",
    "            generators = [\n",
    "                self.generate_numeric_question,\n",
    "                self.generate_special_case_question,\n",
    "                self.generate_zero_case_question\n",
    "            ]\n",
    "            return random.choice(generators)()\n",
    "\n",
    "    def generate_training_set(self, section_counts: dict) -> list[tuple[str, str, str]]:\n",
    "        \"\"\"Generate a structured training set with sections.\"\"\"\n",
    "        training_set = []\n",
    "        \n",
    "        # Basic cases (all/none)\n",
    "        for _ in range(section_counts.get(\"basic\", 0)):\n",
    "            q, a = self.generate_special_case_question()\n",
    "            training_set.append((\"# Basic cases - all/none\", q, a))\n",
    "            \n",
    "        # Numeric cases with specific complementary numbers\n",
    "        for _ in range(section_counts.get(\"numeric\", 0)):\n",
    "            q, a = self.generate_numeric_question(force_complementary=True)\n",
    "            training_set.append((\"# Cases with specific numbers\", q, a))\n",
    "            \n",
    "        # Special cases (some)\n",
    "        for _ in range(section_counts.get(\"special\", 0)):\n",
    "            q, a = self.generate_special_case_question()\n",
    "            training_set.append((\"# Cases with 'some'\", q, a))\n",
    "            \n",
    "        # Zero cases\n",
    "        for _ in range(section_counts.get(\"zero\", 0)):\n",
    "            q, a = self.generate_zero_case_question()\n",
    "            training_set.append((\"# Zero cases\", q, a))\n",
    "            \n",
    "        return training_set\n",
    "\n",
    "def generate_training_and_test(\n",
    "    section_counts: dict = {\n",
    "        \"basic\": 2,    # all/none cases\n",
    "        \"numeric\": 3,  # specific number cases\n",
    "        \"special\": 2,  # 'some' cases\n",
    "        \"zero\": 2      # explicit zero cases\n",
    "    },\n",
    "    force_test_type: str = None,\n",
    "    max_attempts: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"Generate a structured training set and test question.\"\"\"\n",
    "    generator = TokenQuestionGenerator()\n",
    "    \n",
    "    # Generate test question\n",
    "    force_numeric = True if force_test_type == \"numeric\" else None\n",
    "    force_special = True if force_test_type == \"special\" else None\n",
    "    test_question, test_answer = generator.generate_test_question(\n",
    "        force_numeric=force_numeric, force_special=force_special\n",
    "    )\n",
    "    \n",
    "    # Generate training set with sections\n",
    "    training_set = []\n",
    "    attempts = 0\n",
    "    \n",
    "    while len(training_set) < sum(section_counts.values()) and attempts < max_attempts:\n",
    "        current_set = generator.generate_training_set(section_counts)\n",
    "        \n",
    "        # Filter out any questions that match the test question\n",
    "        filtered_set = [\n",
    "            (section, q, a)\n",
    "            for section, q, a in current_set\n",
    "            if not generator._questions_are_equivalent(q, test_question)\n",
    "        ]\n",
    "        \n",
    "        if len(filtered_set) == sum(section_counts.values()):\n",
    "            training_set = filtered_set\n",
    "            break\n",
    "            \n",
    "        attempts += 1\n",
    "    \n",
    "    if attempts >= max_attempts:\n",
    "        raise RuntimeError(\"Failed to generate unique training set after maximum attempts\")\n",
    "    \n",
    "    # Format the introduction with explicit instructions\n",
    "    introduction = \"\"\"\n",
    "Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\n",
    "Remember:\n",
    "- For 0, use 'zero' not '0'\n",
    "- The total number of tokens equals the sum of black and white tokens\n",
    "- Always write numbers as words (e.g., 'two' not '2')\n",
    "\n",
    "For example, this is correct: \n",
    "Q: I have 10 tokens, and 5 of them are black. How many of my tokens are white?\n",
    "A: five of them are white\n",
    "\n",
    "Whereas this is incorrect:\n",
    "Q: I have 10 tokens, and 5 of them are black. How many of my tokens are white?\n",
    "A: 5 of them are white\n",
    "\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"introduction\": introduction.strip(),\n",
    "        \"training_questions\": training_set,\n",
    "        \"test_question\": test_question,\n",
    "        \"test_answer\": test_answer\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Custom section counts\n",
    "    section_counts = {\n",
    "        \"basic\": 5,    # all/none cases\n",
    "        \"numeric\": 3,  # specific number cases\n",
    "        \"special\": 2,  # 'some' cases\n",
    "        \"zero\": 2      # explicit zero cases\n",
    "    }\n",
    "    \n",
    "    result = generate_training_and_test(\n",
    "        section_counts=section_counts,\n",
    "        force_test_type=\"numeric\"  # Can be 'numeric', 'special', or None\n",
    "    )\n",
    "    \n",
    "    print(result[\"introduction\"])\n",
    "    print(\"\\nTraining Questions:\")\n",
    "    for section, q, a in result[\"training_questions\"]:\n",
    "        if section:\n",
    "            print(f\"\\n{section}\")\n",
    "        print(f\"{q}\\n{a}\\n\")\n",
    "    print(\"\\nTest Question:\")\n",
    "    print(result[\"test_question\"])\n",
    "    print(\"\\nExpected Answer:\")\n",
    "    print(result[\"test_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index. This is useful for\n",
    "    calibrating the right amount of the feature to add.\n",
    "    \"\"\"\n",
    "    max_activation = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae.cfg.hook_name],\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0, 1)\n",
    "        batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "        max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "\n",
    "def steering(activations, steering_strength=1.0, steering_vector=None, max_act=1.0):\n",
    "    # Note if the feature fires anyway, we'd be adding to that here.\n",
    "    return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=False if device == \"mps\" else True,\n",
    "            prepend_bos=sae.cfg.prepend_bos,\n",
    "        )\n",
    "\n",
    "    return model.tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_steering(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    feature_to_steer: int | None = None,\n",
    "    steering_strength: float = 1.0,\n",
    "    max_act: float = 60.0,\n",
    "    n_numeric: int = 10,\n",
    "    n_special: int = 10,\n",
    "    max_new_tokens: int = 5,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test model accuracy on token counting tasks with optional feature steering.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: The sparse autoencoder\n",
    "        feature_to_steer: Feature index to steer, or None for no steering\n",
    "        steering_strength: Strength of steering (default 1.0)\n",
    "        max_act: Maximum activation for the steered feature\n",
    "        n_numeric: Number of numeric test questions\n",
    "        n_special: Number of special test questions\n",
    "        max_new_tokens: Maximum tokens to generate for each answer\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics and test results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"numeric_correct\": 0,\n",
    "        \"special_correct\": 0,\n",
    "        \"numeric_tests\": [],\n",
    "        \"special_tests\": [],\n",
    "    }\n",
    "\n",
    "    for test_type in [\"numeric\", \"special\"]:\n",
    "        n_tests = n_numeric if test_type == \"numeric\" else n_special\n",
    "        for _ in range(n_tests):\n",
    "            test_data = generate_training_and_test(\n",
    "                num_training_numeric=40,\n",
    "                num_training_special=20,\n",
    "                force_test_type=test_type,\n",
    "            )\n",
    "\n",
    "            prompt = (\n",
    "                test_data[\"introduction\"]\n",
    "                + \"\\n\\n\"\n",
    "                + \"\\n\\n\".join(\n",
    "                    f\"{q}\\n{a}\\n{label}\"\n",
    "                    for q, a, label in test_data[\"training_questions\"]\n",
    "                )\n",
    "                + f\"\\n\\n{test_data['test_question']}\\nA: \"\n",
    "            )\n",
    "\n",
    "            if feature_to_steer is not None:\n",
    "                generated = generate_with_steering(\n",
    "                    model,\n",
    "                    sae,\n",
    "                    prompt,\n",
    "                    feature_to_steer,\n",
    "                    max_act,\n",
    "                    steering_strength=steering_strength,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                )\n",
    "            else:\n",
    "                # Convert prompt to tokens first\n",
    "                input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    # temperature=0.7,\n",
    "                    # top_p=0.9,\n",
    "                    stop_at_eos=False if device == \"mps\" else True,\n",
    "                    prepend_bos=sae.cfg.prepend_bos,\n",
    "                )\n",
    "                generated = model.tokenizer.decode(output[0])\n",
    "\n",
    "            test_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"expected\": test_data[\"test_answer\"],\n",
    "                \"generated\": generated,\n",
    "                \"correct\": test_data[\"test_answer\"] in generated,\n",
    "            }\n",
    "\n",
    "            if test_type == \"numeric\":\n",
    "                results[\"numeric_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"numeric_correct\"] += 1\n",
    "            else:\n",
    "                results[\"special_tests\"].append(test_result)\n",
    "                if test_result[\"correct\"]:\n",
    "                    results[\"special_correct\"] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    results[\"numeric_accuracy\"] = results[\"numeric_correct\"] / n_numeric\n",
    "    results[\"special_accuracy\"] = results[\"special_correct\"] / n_special\n",
    "    results[\"total_accuracy\"] = (\n",
    "        results[\"numeric_correct\"] + results[\"special_correct\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Test without steering\n",
    "#     normal_results = evaluate_model_with_steering(model, sae)\n",
    "#     print(\"\\nResults without steering:\")\n",
    "#     print(f\"Numeric accuracy: {normal_results['numeric_accuracy']:.2%}\")\n",
    "#     print(f\"Special accuracy: {normal_results['special_accuracy']:.2%}\")\n",
    "#     print(f\"Total accuracy: {normal_results['total_accuracy']:.2%}\")\n",
    "\n",
    "#     print(\"\\nNumeric test results:\")\n",
    "#     for i, test in enumerate(normal_results[\"numeric_tests\"], 1):\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Expected: {test['expected']}\")\n",
    "#         print(f\"Generated: {test['generated']}\")\n",
    "#         print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "#     print(\"\\nSpecial test results:\")\n",
    "#     for i, test in enumerate(normal_results[\"special_tests\"], 1):\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Expected: {test['expected']}\")\n",
    "#         print(f\"Generated: {test['generated']}\")\n",
    "#         print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "#     # Test with feature steering\n",
    "#     feature_to_steer = 12257  # Replace with your feature of interest\n",
    "#     steering_strength = 0.0\n",
    "#     steered_results = evaluate_model_with_steering(\n",
    "#         model,\n",
    "#         sae,\n",
    "#         feature_to_steer=feature_to_steer,\n",
    "#         steering_strength=steering_strength,\n",
    "#     )\n",
    "#     print(\n",
    "#         f\"\\nResults with feature {feature_to_steer} steered (strength {steering_strength}):\"\n",
    "#     )\n",
    "#     print(f\"Numeric accuracy: {steered_results['numeric_accuracy']:.2%}\")\n",
    "#     print(f\"Special accuracy: {steered_results['special_accuracy']:.2%}\")\n",
    "#     print(f\"Total accuracy: {steered_results['total_accuracy']:.2%}\")\n",
    "\n",
    "#     print(\"\\nNumeric test results (with steering):\")\n",
    "#     for i, test in enumerate(steered_results[\"numeric_tests\"], 1):\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Expected: {test['expected']}\")\n",
    "#         print(f\"Generated: {test['generated']}\")\n",
    "#         print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "#     print(\"\\nSpecial test results (with steering):\")\n",
    "#     for i, test in enumerate(steered_results[\"special_tests\"], 1):\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Expected: {test['expected']}\")\n",
    "#         print(f\"Generated: {test['generated']}\")\n",
    "#         print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "#     # Test with feature steering\n",
    "#     feature_to_steer = 15441  # Replace with your feature of interest\n",
    "#     steering_strength = 0.0\n",
    "#     steered_results = evaluate_model_with_steering(\n",
    "#         model,\n",
    "#         sae,\n",
    "#         feature_to_steer=feature_to_steer,\n",
    "#         steering_strength=steering_strength,\n",
    "#     )\n",
    "#     print(\n",
    "#         f\"\\nResults with feature {feature_to_steer} steered (strength {steering_strength}):\"\n",
    "#     )\n",
    "#     print(f\"Numeric accuracy: {steered_results['numeric_accuracy']:.2%}\")\n",
    "#     print(f\"Special accuracy: {steered_results['special_accuracy']:.2%}\")\n",
    "#     print(f\"Total accuracy: {steered_results['total_accuracy']:.2%}\")\n",
    "\n",
    "#     print(\"\\nNumeric test results (with steering):\")\n",
    "#     for i, test in enumerate(steered_results[\"numeric_tests\"], 1):\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Expected: {test['expected']}\")\n",
    "#         print(f\"Generated: {test['generated']}\")\n",
    "#         print(f\"Correct: {test['correct']}\")\n",
    "\n",
    "#     print(\"\\nSpecial test results (with steering):\")\n",
    "#     for i, test in enumerate(steered_results[\"special_tests\"], 1):\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         question = test[\"prompt\"].split(\"A: \")[0].splitlines()[-1]\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Expected: {test['expected']}\")\n",
    "#         print(f\"Generated: {test['generated']}\")\n",
    "#         print(f\"Correct: {test['correct']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_intervention(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    feature_ids: int | list[int] | None = None,\n",
    "    intervention_type: str = \"none\",  # \"none\", \"steering\", or \"ablation\"\n",
    "    steering_strength: float = 1.0,\n",
    "    max_act: float = 60.0,\n",
    "    n_numeric: int = 10,\n",
    "    n_special: int = 10,\n",
    "    max_new_tokens: int = 6,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 0.9,\n",
    "    section_counts: dict = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test model accuracy on token counting tasks with optional feature steering or ablation.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        sae: The sparse autoencoder\n",
    "        feature_ids: Feature index(es) to intervene on, or None for no intervention\n",
    "        intervention_type: Type of intervention (\"none\", \"steering\", or \"ablation\")\n",
    "        steering_strength: Strength of steering (default 1.0)\n",
    "        max_act: Maximum activation for the steered feature\n",
    "        n_numeric: Number of numeric test questions\n",
    "        n_special: Number of special test questions\n",
    "        max_new_tokens: Maximum tokens to generate for each answer\n",
    "        section_counts: Dictionary specifying counts for different question types\n",
    "            e.g. {\"basic\": 2, \"numeric\": 3, \"special\": 2, \"zero\": 2}\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing accuracy metrics and test results\n",
    "    \"\"\"\n",
    "    # Default section counts if none provided\n",
    "    if section_counts is None:\n",
    "        section_counts = {\n",
    "            \"basic\": 3,     # all/none cases\n",
    "            \"numeric\": 3,   # specific number cases\n",
    "            \"special\": 3,   # 'some' cases\n",
    "            \"zero\": 0       # explicit zero cases\n",
    "        }\n",
    "\n",
    "    results = {\n",
    "        \"numeric_correct_strict\": 0,\n",
    "        \"numeric_correct_lenient\": 0,\n",
    "        \"special_correct_strict\": 0,\n",
    "        \"special_correct_lenient\": 0,\n",
    "        \"numeric_tests\": [],\n",
    "        \"special_tests\": [],\n",
    "    }\n",
    "\n",
    "    def convert_words_to_digits(text: str) -> str:\n",
    "        \"\"\"Convert number words to digits in the text.\"\"\"\n",
    "        word_to_digit = {\n",
    "            \"zero\": \"0\",\n",
    "            \"none\": \"0\",\n",
    "            \"one\": \"1\",\n",
    "            \"two\": \"2\",\n",
    "            \"three\": \"3\",\n",
    "            \"four\": \"4\",\n",
    "            \"five\": \"5\",\n",
    "            \"six\": \"6\",\n",
    "            \"seven\": \"7\",\n",
    "            \"eight\": \"8\",\n",
    "            \"nine\": \"9\",\n",
    "            \"ten\": \"10\",\n",
    "        }\n",
    "        for word, digit in word_to_digit.items():\n",
    "            text = text.replace(word, digit)\n",
    "        return text\n",
    "\n",
    "    # Convert single feature_id to list for consistency\n",
    "    if isinstance(feature_ids, int):\n",
    "        feature_ids = [feature_ids]\n",
    "\n",
    "    def ablate_feature_hook(feature_activations, hook=None, feature_ids=None):  # noqa: ARG001\n",
    "        feature_activations[:, :, feature_ids] = 0\n",
    "        return feature_activations\n",
    "\n",
    "    for test_type in tqdm([\"numeric\", \"special\"], desc=\"Testing types\"):\n",
    "        n_tests = n_numeric if test_type == \"numeric\" else n_special\n",
    "        for _ in tqdm(range(n_tests), desc=f\"Testing {test_type}\"):\n",
    "            # Use new structured prompt generator\n",
    "            test_data = generate_training_and_test(\n",
    "                section_counts=section_counts,\n",
    "                force_test_type=test_type,\n",
    "            )\n",
    "\n",
    "            # Construct prompt with sections\n",
    "            prompt_parts = [test_data[\"introduction\"]]\n",
    "            \n",
    "            # Group questions by section\n",
    "            sections = {}\n",
    "            for section, q, a in test_data[\"training_questions\"]:\n",
    "                if section not in sections:\n",
    "                    sections[section] = []\n",
    "                sections[section].append((q, a))\n",
    "            \n",
    "            # Add each section with its header\n",
    "            for section, questions in sections.items():\n",
    "                prompt_parts.append(f\"\\n\\n{section}\")\n",
    "                for q, a in questions:\n",
    "                    prompt_parts.append(f\"{q}\\n{a}\")\n",
    "            \n",
    "            # Add test question\n",
    "            prompt_parts.append(f\"\\n\\n{test_data['test_question']}\\nA: \")\n",
    "            \n",
    "            prompt = \"\\n\\n\".join(prompt_parts)\n",
    "\n",
    "            # Convert prompt to tokens\n",
    "            input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "            if intervention_type == \"none\" or feature_ids is None:\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    stop_at_eos=False if device == \"mps\" else True,\n",
    "                    prepend_bos=sae.cfg.prepend_bos,\n",
    "                )\n",
    "\n",
    "            elif intervention_type == \"steering\":\n",
    "                steering_vector = sae.W_dec[feature_ids[0]].to(model.cfg.device)\n",
    "                steering_hook = partial(\n",
    "                    steering,\n",
    "                    steering_vector=steering_vector,\n",
    "                    steering_strength=steering_strength,\n",
    "                    max_act=max_act,\n",
    "                )\n",
    "\n",
    "                with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "                    output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=temperature,\n",
    "                        top_p=top_p,\n",
    "                        stop_at_eos=False if device == \"mps\" else True,\n",
    "                        prepend_bos=sae.cfg.prepend_bos,\n",
    "                    )\n",
    "\n",
    "            elif intervention_type == \"ablation\":\n",
    "                ablation_hook = partial(ablate_feature_hook, feature_ids=feature_ids)\n",
    "                model.add_sae(sae)\n",
    "                hook_point = sae.cfg.hook_name + \".hook_sae_acts_post\"\n",
    "\n",
    "                with model.hooks(fwd_hooks=[(hook_point, ablation_hook)]):\n",
    "                    output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        top_p=top_p,\n",
    "                        temperature=temperature,\n",
    "                        stop_at_eos=False if device == \"mps\" else True,\n",
    "                        prepend_bos=sae.cfg.prepend_bos,\n",
    "                    )\n",
    "\n",
    "                model.reset_hooks()\n",
    "                model.reset_saes()\n",
    "\n",
    "            generated = model.tokenizer.decode(output[0])  # type: ignore\n",
    "            generated_answer = generated.split(\"A: \")[-1].strip()\n",
    "            expected_answer = test_data[\"test_answer\"].replace(\"A: \", \"\").strip()\n",
    "\n",
    "            # Create lenient versions of answers\n",
    "            generated_lenient = convert_words_to_digits(generated_answer.lower())\n",
    "            expected_lenient = convert_words_to_digits(expected_answer.lower())\n",
    "\n",
    "            test_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"expected\": expected_answer,\n",
    "                \"generated\": generated_answer,\n",
    "                \"correct_strict\": expected_answer in generated_answer,\n",
    "                \"correct_lenient\": expected_lenient in generated_lenient,\n",
    "                \"section_counts\": section_counts,  # Add this for analysis\n",
    "            }\n",
    "\n",
    "            if test_type == \"numeric\":\n",
    "                results[\"numeric_tests\"].append(test_result)\n",
    "                if test_result[\"correct_strict\"]:\n",
    "                    results[\"numeric_correct_strict\"] += 1\n",
    "                if test_result[\"correct_lenient\"]:\n",
    "                    results[\"numeric_correct_lenient\"] += 1\n",
    "            else:\n",
    "                results[\"special_tests\"].append(test_result)\n",
    "                if test_result[\"correct_strict\"]:\n",
    "                    results[\"special_correct_strict\"] += 1\n",
    "                if test_result[\"correct_lenient\"]:\n",
    "                    results[\"special_correct_lenient\"] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    results[\"numeric_accuracy_strict\"] = results[\"numeric_correct_strict\"] / n_numeric\n",
    "    results[\"numeric_accuracy_lenient\"] = results[\"numeric_correct_lenient\"] / n_numeric\n",
    "    results[\"special_accuracy_strict\"] = results[\"special_correct_strict\"] / n_special\n",
    "    results[\"special_accuracy_lenient\"] = results[\"special_correct_lenient\"] / n_special\n",
    "    results[\"total_accuracy_strict\"] = (\n",
    "        results[\"numeric_correct_strict\"] + results[\"special_correct_strict\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "    results[\"total_accuracy_lenient\"] = (\n",
    "        results[\"numeric_correct_lenient\"] + results[\"special_correct_lenient\"]\n",
    "    ) / (n_numeric + n_special)\n",
    "\n",
    "    # Add section configuration to results\n",
    "    results[\"section_counts\"] = section_counts\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b244b5ff953a4faeba2093fa397e2df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa6a1b3c01345da92ee9659a809a7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379e7febfd58422682601d43138995c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e1b941caa44846a25249323beefd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422c262425c64217b8fa1df8bb92b16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_results = evaluate_model_with_intervention(\n",
    "    model, sae, intervention_type=\"none\", n_numeric=1, n_special=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have four tokens, and all of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nQ: I have five tokens, and all of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have ten tokens, and five of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have three tokens, and two of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have eight tokens, and three of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have six tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have five tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\n# Zero cases\\n\\nQ: I have eight tokens, and all of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nQ: I have ten tokens, and all of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\n\\n\\nQ: I have nine tokens, and six of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'three of them are black',\n",
       "  'generated': '3 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 2, 'numeric': 3, 'special': 2, 'zero': 2}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_results[\"numeric_tests\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88884208866b4096b354a4f5a48f781e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752520e4d5dd4333819c2407169160fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4503abb5a8d421d9fb2d2d5e8cc1b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29999f2e4cf4dab8e2358c35da67db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d5e216bb8146b0981e4119aae5dd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    "    n_numeric=1,\n",
    "    n_special=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have nine tokens, and all of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\nQ: I have six tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have eight tokens, and one of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\nQ: I have three tokens, and one of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have nine tokens, and two of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have nine tokens, and all of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: zero of them are white\\n\\n\\n\\n# Zero cases\\n\\nQ: I have eight tokens, and all of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\nQ: I have seven tokens, and all of them are white. How many of my tokens are black?\\nA: zero of them are black\\n\\n\\n\\nQ: I have eight tokens, and all of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'zero of them are white',\n",
       "  'generated': '0 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 2, 'numeric': 3, 'special': 2, 'zero': 2}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablated_results_hub['special_tests']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b2eec7ac6b4a949cc1c61259bf5936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44223315f714701834140ec352a1d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e27141b45d49b3bbdfffce92c2d865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167d14ad9ab3438caccdcd1ed8a59725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b866c81e74f4521843758fef62ae423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbd64b3675a42da930028352a2be56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fedec83a83b47fc964e5f639dfc128b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0265a68335114e4da9e4886519037964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d826aa5e334dfd94ea8f7236809cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ad83a430cd49b3ac847115d30f3e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068f72812bda4dd089b441d0b396016d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8281ba0c234bfbad096d80b3ef8d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaf211bc5954b659714f2c2120355bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4798b379236d457bb089ec34c8bc2fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87139262187a488fa1362282166d607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeb4d40d5a249b89b1ed82e5836c924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0064dbef3384f1e97749c42dd0c1a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67f1bca7e0041db8ac3e10d51f76dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e06df39258418bae562deb38cb260a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13a1151e0544194ac1b0c7fa7cf961b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893828e26f13411cab827d73bb5c4f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4cb546b4cf48a4b43bcf2ade5281ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10724e8c37f4e5794d6711e8cde6455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test without intervention\n",
    "normal_results = evaluate_model_with_intervention(model, sae, intervention_type=\"none\")\n",
    "\n",
    "# # Test with steering\n",
    "# steered_results = evaluate_model_with_intervention(\n",
    "#     model,\n",
    "#     sae,\n",
    "#     feature_ids=12257,\n",
    "#     intervention_type=\"steering\",\n",
    "#     steering_strength=1.0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8\n",
      "0.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(normal_results[\"numeric_accuracy_strict\"])\n",
    "print(normal_results[\"numeric_accuracy_lenient\"])\n",
    "print(normal_results[\"special_accuracy_strict\"])\n",
    "print(normal_results[\"special_accuracy_lenient\"])\n",
    "# Test with ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have eight tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have three tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have ten tokens, and four of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nQ: I have seven tokens, and four of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have six tokens, and two of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have five tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have five tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have seven tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\n\\n\\nQ: I have seven tokens, and three of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'four of them are white',\n",
       "  'generated': '4 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have three tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have eight tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have ten tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have ten tokens, and nine of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have nine tokens, and four of them are white. How many of my tokens are black?\\nA: five of them are black\\n\\nQ: I have six tokens, and five of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have nine tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have seven tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have seven tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\nQ: I have eight tokens, and seven of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'one of them are black',\n",
       "  'generated': '1 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have eight tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have seven tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have six tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have nine tokens, and five of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nQ: I have six tokens, and one of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have six tokens, and three of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have four tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have eight tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have five tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have five tokens, and three of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'two of them are black',\n",
       "  'generated': '2 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have five tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have eight tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have five tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have ten tokens, and seven of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have eight tokens, and five of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nQ: I have nine tokens, and two of them are white. How many of my tokens are black?\\nA: seven of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have five tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have eight tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have seven tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\nQ: I have six tokens, and one of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'five of them are black',\n",
       "  'generated': '5 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have three tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have nine tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have five tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have four tokens, and two of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have seven tokens, and one of them are white. How many of my tokens are black?\\nA: six of them are black\\n\\nQ: I have three tokens, and one of them are white. How many of my tokens are black?\\nA: two of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have four tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have six tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\n\\n\\nQ: I have five tokens, and two of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'three of them are white',\n",
       "  'generated': '3 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have ten tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have five tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have six tokens, and five of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have eight tokens, and seven of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have seven tokens, and one of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have three tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have three tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have three tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have four tokens, and two of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'two of them are black',\n",
       "  'generated': '2 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have ten tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have ten tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have eight tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have three tokens, and one of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have six tokens, and one of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have nine tokens, and two of them are white. How many of my tokens are black?\\nA: seven of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have three tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have three tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have seven tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\n\\n\\nQ: I have nine tokens, and three of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'six of them are black',\n",
       "  'generated': '6 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have seven tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have five tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have nine tokens, and six of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have eight tokens, and two of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nQ: I have seven tokens, and four of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have four tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have three tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have three tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have six tokens, and five of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'one of them are white',\n",
       "  'generated': '1 of them is white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have five tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have five tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have nine tokens, and eight of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have six tokens, and five of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have seven tokens, and one of them are white. How many of my tokens are black?\\nA: six of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have three tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have eight tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\n\\n\\nQ: I have six tokens, and one of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'five of them are black',\n",
       "  'generated': '5 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have eight tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have three tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have four tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have ten tokens, and two of them are black. How many of my tokens are white?\\nA: eight of them are white\\n\\nQ: I have seven tokens, and four of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nQ: I have three tokens, and one of them are white. How many of my tokens are black?\\nA: two of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have six tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have ten tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'one of them are white',\n",
       "  'generated': '1 of them is white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_results['numeric_tests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have four tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have nine tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have seven tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have ten tokens, and two of them are white. How many of my tokens are black?\\nA: eight of them are black\\n\\nQ: I have ten tokens, and five of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have eight tokens, and six of them are white. How many of my tokens are black?\\nA: two of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have six tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have ten tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have ten tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\nQ: I have eight tokens, and some of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'some of them are black',\n",
       "  'generated': '8 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have eight tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have ten tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have ten tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have nine tokens, and two of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\nQ: I have seven tokens, and one of them are black. How many of my tokens are white?\\nA: six of them are white\\n\\nQ: I have eight tokens, and five of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have five tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have five tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\nQ: I have ten tokens, and some of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'some of them are white',\n",
       "  'generated': '10 - 2 =',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have nine tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have ten tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have four tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have six tokens, and five of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have nine tokens, and one of them are black. How many of my tokens are white?\\nA: eight of them are white\\n\\nQ: I have four tokens, and one of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have three tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have nine tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have seven tokens, and none of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'all of them are black',\n",
       "  'generated': '6 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have three tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have six tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have nine tokens, and seven of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have seven tokens, and six of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have six tokens, and five of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have eight tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have seven tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'none of them are white',\n",
       "  'generated': '0 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have eight tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have three tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have seven tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have four tokens, and three of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\nQ: I have eight tokens, and five of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have nine tokens, and eight of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have nine tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have five tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have three tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have eight tokens, and none of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'all of them are white',\n",
       "  'generated': '8 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have four tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have eight tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have six tokens, and one of them are black. How many of my tokens are white?\\nA: five of them are white\\n\\nQ: I have ten tokens, and six of them are black. How many of my tokens are white?\\nA: four of them are white\\n\\nQ: I have five tokens, and one of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have nine tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have seven tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have five tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have nine tokens, and all of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'none of them are white',\n",
       "  'generated': '0 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have five tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have seven tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have ten tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have six tokens, and four of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have three tokens, and one of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\nQ: I have three tokens, and two of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have six tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have three tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\nQ: I have four tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\n\\n\\nQ: I have six tokens, and all of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'none of them are white',\n",
       "  'generated': '0 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have seven tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have four tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have ten tokens, and some of them are black. How many of my tokens are white?\\nA: some of them are white\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have eight tokens, and one of them are black. How many of my tokens are white?\\nA: seven of them are white\\n\\nQ: I have seven tokens, and four of them are black. How many of my tokens are white?\\nA: three of them are white\\n\\nQ: I have ten tokens, and nine of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have five tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\nQ: I have four tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have nine tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\n\\n\\nQ: I have six tokens, and some of them are white. How many of my tokens are black?\\nA: \",\n",
       "  'expected': 'some of them are black',\n",
       "  'generated': '4 of them are black',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': False,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have ten tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have nine tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have six tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have four tokens, and one of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have five tokens, and one of them are white. How many of my tokens are black?\\nA: four of them are black\\n\\nQ: I have eight tokens, and seven of them are black. How many of my tokens are white?\\nA: one of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have nine tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have five tokens, and none of them are black. How many of my tokens are white?\\nA: all of them are white\\n\\nQ: I have seven tokens, and all of them are black. How many of my tokens are white?\\nA: none of them are white\\n\\n\\n\\nQ: I have four tokens, and all of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'none of them are white',\n",
       "  'generated': '0 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}},\n",
       " {'prompt': \"Tokens can be either black or white. Complete the following sentences using number words (one, two, three, etc.) never digits (1, 2, 3, etc.).\\nRemember:\\n- For 0, use 'zero' not '0'\\n- The total number of tokens equals the sum of black and white tokens\\n- Always write numbers as words (e.g., 'two' not '2')\\n\\n\\n\\n# Basic cases - all/none\\n\\nQ: I have six tokens, and none of them are white. How many of my tokens are black?\\nA: all of them are black\\n\\nQ: I have ten tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\nQ: I have eight tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\n\\n\\n# Cases with specific numbers\\n\\nQ: I have eight tokens, and seven of them are white. How many of my tokens are black?\\nA: one of them are black\\n\\nQ: I have eight tokens, and five of them are white. How many of my tokens are black?\\nA: three of them are black\\n\\nQ: I have eight tokens, and six of them are black. How many of my tokens are white?\\nA: two of them are white\\n\\n\\n\\n# Cases with 'some'\\n\\nQ: I have eight tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have eight tokens, and some of them are white. How many of my tokens are black?\\nA: some of them are black\\n\\nQ: I have three tokens, and all of them are white. How many of my tokens are black?\\nA: none of them are black\\n\\n\\n\\nQ: I have ten tokens, and all of them are black. How many of my tokens are white?\\nA: \",\n",
       "  'expected': 'none of them are white',\n",
       "  'generated': '0 of them are white',\n",
       "  'correct_strict': False,\n",
       "  'correct_lenient': True,\n",
       "  'section_counts': {'basic': 3, 'numeric': 3, 'special': 3, 'zero': 0}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_results['special_tests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2cbfc1e3648d9ac8733eec55211d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b0a4ebf40f49d0983fe2641afd841d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cfcb06487c4e6aa294066f0fdd1d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6208e945c1f840d1b440ab5ca3c4d7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a638a6eaafe4c0da0acb6e422e49d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b411ed8b355454c967729ef12fde4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8ea2fa888d4bc2964f3c235b6027d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682b199e6a374f23bd655e73f0c20a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a7fd5a29c2476cb7db252d56e5c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ef741bd9dd4be79cb8270a9a8adee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c665e0c7a49248dd92d224e001005236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1642a7ddab5242dfa81f363fb497864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5edd8bef29453bbb802f9559c44d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d789d636bb704f2b9131872699e66ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620cba154d8f471a9f77634d15caf735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b2f8355b54ee780eae6ed34162544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fa70c3763e4d2c8b234c50563e9615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae65355e5cc4a5d8eb946f2296b8c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91097ef007bd440e8515866cf4942e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9520eb0b6264cc4b2056fcd24518a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9042772f4614bf6a4021af3ba69a042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f8f38ece304d25bb08e28c169f91e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1422faa955c8472ba8a5eab00e51b344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50be551fd5841eda5a619a9b59bd57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efab38bce744483bb81c2209454edb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d07e62f9fa444c841b89af7d1d4c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02e293f4c7b4957b8925e0fca1d0cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45686ab6befc4f15aa2a5cf92aba53cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d97900ee634720ae7be70f809ff111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9000e7c8704448697bfba67904e1866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c046c1ff1c74c159b24d6d84483c5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b0845ac02b463bb6a889ce78837d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cba8ec0d20427789b85219eed2a54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cbf1a30b744dc9a4da99132fcaf2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bdd638043f4e2c8c38da1d3448a255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed608b33e44c4d8d953c1874977d1786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8ac0f179c04d0e89bbd3cc9c2f2d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffeeed72025145d2a42b6ae12fb17d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13438612edc6425ea63440dd895c5fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235872b72f1f46eb9cd19d5b049644ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c7bee7059741bfb3a7fb78a684f0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26140f004f834abb8a9e93dc21970a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d931a46e6f1445d388ca464116658300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdf1f534f4d45ecb7d4fd24530b7d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32fa88f21be4f0ebce53d8f783fe54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fccb49f870c42ef948e2387a665aa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca75349cb39c41338b199b9eeb966e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf793ef21d12491db516c99cabcf9ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6901ec43534cc1a2ace94bf88c7d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df740567abb14af08823629d137e5989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107888b5d2c84a47ba2353d559f7eea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b6e13005a9433fa88757cfb3128ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ab1fe5397040fcafb6370437a76276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d86cc0b9ecd44bfb4c2e55b26caf4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cea90286a5427ebc529708a2be81ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eb875056a84eb1acb603201647dbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542e29a8a3094734a04150e682ba0de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c9fa161963475db5f080f110732053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b614e85bb6945c8841bd30b9c5554a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effea78e62034d8783f7bc531b43a034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc81686ca4b4e60a36da978f08aad71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae7677643284bf590cb4c5ca3cae593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848a043737274181bd6d59d0ae4cc320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05034e50d3f04b56bc30b87e9eadd0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baf7811fd904f6ebf24d11fe5bb0ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d03e6e5c6743e89862255968129fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf77a660e20432f8f6506112d0be9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c4209e065549d8981a11484d026961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cfc7bf09ec4adb980e9eefdc955056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d999b0ffa8436ebf8e8cf7efa21624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8947a24e30f143a180b74fa346aa1906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98f1bffb8c0401ab4db4842b3e390ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bf6c243eca489399392ec4ba1ad4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f245bbd6ee614d7fa62279927a00dab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbfadf477964957884f02d9c2a20e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb1eb1c4a8e4d57a61a660fab62ca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5f7e65a4ca4131a5721be6e0fdcad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9b9c0d0221449dbdbcf54e8840cd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618aab4df06d44d699796022fd0dc6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bdfc8aeb6545c380c25b01f6a8b2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd28ebccdca8407687c107a8a75a3652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5691a87d0f466fbb897a4570b047fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb617638e854827945656f3eda3c681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a084573a6c4bef965f83d89ccd1e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757192bff8c74307b073f1aaa5d06a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dbf30d57ba494088bf4b5f36b1057a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0d33436a234328ac9b8208655c9c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c134735654a4245a173494f95a78163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2791abd1badf4c668c809b3bbbe84938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616244cf900541e4a85c532a4cb0b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f02a49de524e9298812ee8e9a5f77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e816104a68564b7ba85ee8e927e1c1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e74e15f5aa4962a044165201098f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530b8af9993c4b779c9c1c2915280d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639f1941426c41bd84269e11b1855799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fdc47dfee24cb691fc0d45e2d4ff56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f3d169f99a43c2bf531d99b009e045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ea8ad30a3942519efaab4a9ead7358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aedac9389b49558e8c9562f3d3d497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da1d3b88b22453c8348e529181c6108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8259c9594c814f7998d0b563c570d1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b4fe36c0054663a0fea7a77b39fadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694d525dfc7c409daf15ad197b4049be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dabbcee65f46ec9414ce94af5c5e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9e7740534842438f5653e5159d4650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce0bac68f0d4c34b579606bdb4a67c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724b331168bd48bf978aa66736c53691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f616516a16184ddfaaaf0e3b7b42db96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b57236ba5c41c29e365e84d8b3f920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b116f649da4e4cbbb2eeb7238d475e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66c7ec7f5dd47cab76b506446ee8e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afb2cc297264ed9ae95a1b51c87feb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d826f39445489087f8a2205e583566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f7229f947f4288a9557dfd64f15f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a743692b1a194a03981b31e5110d06f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with ablation\n",
    "ablated_results_hub = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablated_results_all = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_spokes = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12649, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n",
    "\n",
    "ablation_results_hub_spoke_some = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257, 15441],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2882d4e5b494cccae858b3cb187c58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing types:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db815ae79f64045a7f965b1f5be1132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing numeric:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c216a4426d4e7e9ae142808b833963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb40aff0546444bc8231a59c9532cb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450028fef5264a93adaa5fca8077a7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a6fcfe0c0d43a9b45f3ee6b8fba5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b7a355509748acb1207ef92d2e0405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce549b4d10b44adb573b9e2d8acf74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313fc60fea254f02ab5f9b1020261523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751d47e575704eca9707ee0dff53ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82fa665cc2246349ea093a98e57e77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabc8bf58b304993b5491394c4fc4557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd544b1cbed46699cc39fb8f9857c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing special:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b02b424e4a40fabb92d3eedba68e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ec104d61894117bddd40f067aa52e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c59d2c9079f4667bfa0684ea6a3eb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1ed932e5545278f19c9c74cf50ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064fe40b414a4ea99777428548130282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b04f637392043c4b5d1eb6ea95a2d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdb6cefd77448fbb9f9f0a1db7060e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a9cf450d4640ad986251a888bf3436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15771d141ded48b29db2ab5401120c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916c9c1acda94608b100943d2f2defb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ablated_results_hub_spoke_all = evaluate_model_with_intervention(\n",
    "    model,\n",
    "    sae,\n",
    "    feature_ids=[12257, 12649],  # Can ablate multiple features\n",
    "    intervention_type=\"ablation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the effect of ablation of the hub alone both spokes alone hub and both spokes together and both spokes together I hope hub and spokes will be more effective than any of these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(ablated_results_hub[\"numeric_accuracy_strict\"])\n",
    "print(ablated_results_hub[\"numeric_accuracy_lenient\"])\n",
    "print(ablated_results_hub[\"special_accuracy_strict\"])\n",
    "print(ablated_results_hub[\"special_accuracy_lenient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9\n",
      "0.0\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "print(ablated_results_some[\"numeric_accuracy_strict\"])\n",
    "print(ablated_results_some[\"numeric_accuracy_lenient\"])\n",
    "print(ablated_results_some[\"special_accuracy_strict\"])\n",
    "print(ablated_results_some[\"special_accuracy_lenient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(ablated_results_all[\"numeric_accuracy_strict\"])\n",
    "print(ablated_results_all[\"numeric_accuracy_lenient\"])\n",
    "print(ablated_results_all[\"special_accuracy_strict\"])\n",
    "print(ablated_results_all[\"special_accuracy_lenient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.7\n",
      "0.0\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(ablation_results_spokes[\"numeric_accuracy_strict\"])\n",
    "print(ablation_results_spokes[\"numeric_accuracy_lenient\"])\n",
    "print(ablation_results_spokes[\"special_accuracy_strict\"])\n",
    "print(ablation_results_spokes[\"special_accuracy_lenient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8\n",
      "0.0\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(ablation_results_hub_spoke_some[\"numeric_accuracy_strict\"])\n",
    "print(ablation_results_hub_spoke_some[\"numeric_accuracy_lenient\"])\n",
    "print(ablation_results_hub_spoke_some[\"special_accuracy_strict\"])\n",
    "print(ablation_results_hub_spoke_some[\"special_accuracy_lenient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9\n",
      "0.0\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(ablated_results_hub_spoke_all[\"numeric_accuracy_strict\"])\n",
    "print(ablated_results_hub_spoke_all[\"numeric_accuracy_lenient\"])\n",
    "print(ablated_results_hub_spoke_all[\"special_accuracy_strict\"])\n",
    "print(ablated_results_hub_spoke_all[\"special_accuracy_lenient\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-cooccurence-DZTJ6ajw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
