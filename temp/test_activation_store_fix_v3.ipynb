{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE, ActivationsStore\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_sae(\n",
    "    model_name: str, sae_release: str, sae_id: str, device: str\n",
    ") -> tuple:\n",
    "    model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "    sae, _, _ = SAE.from_pretrained(release=sae_release, sae_id=sae_id, device=device)\n",
    "    sae.W_dec.norm(dim=-1).mean()\n",
    "    sae.fold_W_dec_norm()\n",
    "    return model, sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10230440998495f813b06be1028d14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_gemma, sae_gemma = load_model_and_sae(\n",
    "    \"gemma-2-2b\",\n",
    "    \"gemma-scope-2b-pt-res-canonical\",\n",
    "    \"layer_18/width_16k/canonical\",\n",
    "    \"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_gpt2, sae_gpt2 = load_model_and_sae(\n",
    "    \"gpt2-small\",\n",
    "    \"gpt2-small-res-jb-feature-splitting\",\n",
    "    \"blocks.8.hook_resid_pre_768\",\n",
    "    \"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/training/activations_store.py:245: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "activation_store_gpt2 = ActivationsStore.from_sae(\n",
    "    model=model_gpt2,\n",
    "    sae=sae_gpt2,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=512,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d183baeae7ac4fc09c7433a070a896cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# activation_store_gemma = ActivationsStore.from_sae(\n",
    "#     model=model_gemma,\n",
    "#     sae=sae_gemma,\n",
    "#     streaming=True,\n",
    "#     store_batch_size_prompts=8,\n",
    "#     train_batch_size_tokens=512,\n",
    "#     n_batches_in_buffer=16,\n",
    "#     device=\"mps\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_batch_old = activation_store_gpt2.next_batch()\n",
    "activations_batch_old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gpt2.encode(activations_batch_old).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = activation_store_gpt2.train_batch_size_tokens\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,   257,  1256,  ...,   465,    11,   326],\n",
       "        [50256,   447,   247,  ...,  1406,  5543,   356],\n",
       "        [50256,  1410,   284,  ...,    72,  4893,   262],\n",
       "        ...,\n",
       "        [50256,   262,  8425,  ...,   850,    12, 35448],\n",
       "        [50256,  4249,  7310,  ..., 16686,   284,   257],\n",
       "        [50256,  2060, 15815,  ...,   517,  3665,  4899]], device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens = activation_store_gpt2.get_batch_tokens(batch_size)\n",
    "# batch_tokens = batch_tokens[:, 1:, ...]\n",
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128, 1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = activation_store_gpt2.get_activations(batch_tokens)\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 127, 1, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_wout_bos = activations[:, 1:, ...]\n",
    "activations_wout_bos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65024, 1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattened_activations = activations_wout_bos.view(-1, activation_store_gpt2.d_in)\n",
    "flattened_activations = activations_wout_bos.reshape(-1, 1, activation_store_gpt2.d_in)\n",
    "flattened_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_without_first_token(activations_store):\n",
    "    \"\"\"\n",
    "    Get a batch of activations from the ActivationsStore, removing the first token of every prompt.\n",
    "\n",
    "    Args:\n",
    "    activations_store (ActivationsStore): An instance of the ActivationsStore class.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor of shape [train_batch_size, 1, d_in] containing activations,\n",
    "                  with the first token of each prompt removed.\n",
    "    \"\"\"\n",
    "    # Get a batch of tokens\n",
    "    batch_tokens = activations_store.get_batch_tokens()\n",
    "\n",
    "    # Get activations for these tokens\n",
    "    with torch.no_grad():\n",
    "        activations = activations_store.get_activations(batch_tokens)\n",
    "\n",
    "    # Remove the first token's activation from each prompt\n",
    "    activations = activations[:, 1:, ...]\n",
    "\n",
    "    # Reshape to match the output of next_batch()\n",
    "    activations = activations.reshape(-1, 1, activations.shape[-1])\n",
    "\n",
    "    # If there's any normalization applied in the original next_batch(), apply it here\n",
    "    if activations_store.normalize_activations == \"expected_average_only_in\":\n",
    "        activations = activations_store.apply_norm_scaling_factor(activations)\n",
    "\n",
    "    # Shuffle the activations\n",
    "    activations = activations[torch.randperm(activations.shape[0])]\n",
    "\n",
    "    # Get the correct batch size\n",
    "    train_batch_size = activations_store.train_batch_size_tokens\n",
    "\n",
    "    # Return only the required number of activations\n",
    "    return activations[:train_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_batch_without_first_token(activation_store_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1806,  3.0300, -2.3312,  ..., -4.5175,  0.5414, -1.1878]],\n",
       "\n",
       "        [[ 1.1776, -3.1908,  3.4033,  ..., -0.0204, -0.2108,  0.5850]],\n",
       "\n",
       "        [[ 0.2742, -0.9426, -3.4186,  ..., -1.2453, -1.3458, -0.5370]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.8214,  0.6326,  1.9179,  ..., -1.0603, -4.0802,  0.6450]],\n",
       "\n",
       "        [[-0.0046, -2.4361, -2.7972,  ...,  0.3780,  3.9732,  2.3055]],\n",
       "\n",
       "        [[-3.3601, -1.0612,  3.0431,  ...,  1.8759,  1.5579, -0.6897]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_batch_old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.1935, -1.2809, -0.4450,  ...,  0.0861, -3.5058,  2.0328]],\n",
       "\n",
       "        [[ 2.4250,  2.5613,  1.7368,  ..., -0.4346, -0.3918,  0.6911]],\n",
       "\n",
       "        [[ 1.5355,  0.1123, -1.2726,  ..., -0.2158, -2.3195,  1.5503]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.9747, -2.4543, -0.0878,  ..., -0.1840, -2.4515,  1.1209]],\n",
       "\n",
       "        [[ 1.0434,  2.8203, -0.3051,  ...,  5.3944, -2.9895, -1.9296]],\n",
       "\n",
       "        [[ 3.3147, -0.0511,  0.8030,  ...,  0.8374, -2.1553,  1.0311]]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_batch_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_gemma.cfg.normalize_activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-cooccurence-DZTJ6ajw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
