{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(93804) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(93805) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sae_lens import SAE, ActivationsStore\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_sae(\n",
    "    model_name: str, sae_release: str, sae_id: str, device: str\n",
    ") -> tuple:\n",
    "    model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "    sae, _, _ = SAE.from_pretrained(release=sae_release, sae_id=sae_id, device=device)\n",
    "    sae.W_dec.norm(dim=-1).mean()\n",
    "    sae.fold_W_dec_norm()\n",
    "    return model, sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Python(93806) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1845bdb95de9403c8e06f98bbecb15c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_gemma, sae_gemma = load_model_and_sae(\n",
    "    \"gemma-2-2b\",\n",
    "    \"gemma-scope-2b-pt-res-canonical\",\n",
    "    \"layer_18/width_16k/canonical\",\n",
    "    \"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_gemma.cfg.normalize_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_gpt2, sae_gpt2 = load_model_and_sae(\n",
    "    \"gpt2-small\",\n",
    "    \"gpt2-small-res-jb-feature-splitting\",\n",
    "    \"blocks.8.hook_resid_pre_768\",\n",
    "    \"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/sae_lens/training/activations_store.py:245: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "activation_store_gpt2 = ActivationsStore.from_sae(\n",
    "    model=model_gpt2,\n",
    "    sae=sae_gpt2,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=512,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6b50f8d7e14ad7b753a63879a0b7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_store_gemma = ActivationsStore.from_sae(\n",
    "    model=model_gemma,\n",
    "    sae=sae_gemma,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=512,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_activations_for_batch(\n",
    "#     activation_store: ActivationsStore, sae: SAE\n",
    "# ) -> torch.Tensor:\n",
    "#     activations_batch = activation_store.next_batch()\n",
    "#     feature_acts = sae.encode(activations_batch).squeeze()\n",
    "#     return feature_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_activations_for_batch(\n",
    "    activation_store: ActivationsStore,\n",
    "    sae: SAE,\n",
    "    remove_first_token: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get feature activations for a batch of tokens from an ActivationsStore.\n",
    "\n",
    "    This function retrieves a batch of activations from the ActivationsStore,\n",
    "    optionally removes the first token (typically the beginning-of-sequence token),\n",
    "    and encodes the activations using the provided SAE (Sparse Autoencoder).\n",
    "\n",
    "    Args:\n",
    "        activation_store (ActivationsStore): The ActivationsStore object to get activations from.\n",
    "        sae (SAE): The Sparse Autoencoder used to encode the activations.\n",
    "        remove_first_token (bool, optional): Whether to remove the first token from each sequence.\n",
    "                                             Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Encoded feature activations, shape (batch_size * context_size, d_sae).\n",
    "\n",
    "    Note:\n",
    "        - If remove_first_token is True, the function uses get_flattened_activations_wout_first\n",
    "          to retrieve activations without the first token.\n",
    "        - The returned tensor is squeezed to remove any singleton dimensions.\n",
    "    \"\"\"\n",
    "    if not remove_first_token:\n",
    "        activations_batch = activation_store.next_batch()\n",
    "    else:\n",
    "        activations_batch = get_flattened_activations_wout_first(activation_store)\n",
    "    feature_acts = sae.encode(activations_batch).squeeze()\n",
    "    return feature_acts\n",
    "\n",
    "\n",
    "def get_flattened_activations_wout_first(\n",
    "    activation_store: ActivationsStore,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    NOTE: this will be a different size from normal activation store batch as we do not get additional tokens to replace the BOS\n",
    "    Get flattened activations without the first token (BOS) from an ActivationsStore.\n",
    "\n",
    "    This function retrieves a batch of tokens, gets their activations, removes the first token\n",
    "    (typically the beginning-of-sequence token), and flattens the resulting activations.\n",
    "\n",
    "    Args:\n",
    "        activation_store (ActivationsStore): The ActivationsStore object to get activations from.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Flattened activations without the first token, shape (batch_size * (context_size - 1), d_in).\n",
    "\n",
    "    Note:\n",
    "        - The function assumes that the first token in each sequence is a BOS token.\n",
    "        - The returned tensor is reshaped to match the format of activation_store.next_batch().\n",
    "    \"\"\"\n",
    "    batch_size = activation_store.train_batch_size_tokens\n",
    "    batch_tokens = activation_store.get_batch_tokens(batch_size)\n",
    "    activations = activation_store.get_activations(batch_tokens)\n",
    "    activations_wout_bos = activations[:, 1:, ...]\n",
    "    flattened_activations = activations_wout_bos.view(-1, activation_store.d_in)\n",
    "    return flattened_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_activations_for_batch_V2(\n",
    "#     activation_store: ActivationsStore, sae: SAE, remove_bos: bool = False,\n",
    "# ) -> torch.Tensor:\n",
    "# if remove_bos:\n",
    "\n",
    "# else:\n",
    "#     activations_batch = activation_store.next_batch()\n",
    "#     feature_acts = sae.encode(activations_batch).squeeze()\n",
    "#     return feature_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "feature_acts_gpt2 = get_feature_activations_for_batch(activation_store_gpt2, sae_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_gemma = get_feature_activations_for_batch(\n",
    "    activation_store_gemma, sae_gemma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store_next_tokens_gpt2 = activation_store_gpt2.get_batch_tokens(\n",
    "    512\n",
    ")  # TODO what does the batch size here correspond to compared to activation_store.next_batch()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAEConfig(architecture='standard', d_in=768, d_sae=768, activation_fn_str='relu', apply_b_dec_to_input=True, finetuning_scaling_factor=False, context_size=128, model_name='gpt2-small', hook_name='blocks.8.hook_resid_pre', hook_layer=8, hook_head_index=None, prepend_bos=True, dataset_path='Skylion007/openwebtext', dataset_trust_remote_code=True, normalize_activations='none', dtype='torch.float32', device='mps', sae_lens_training_version=None, activation_fn_kwargs={}, neuronpedia_id='gpt2-small/8-res_fs768-jb', model_from_pretrained_kwargs={'center_writing_weights': True})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gpt2.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_gpt2.next_batch().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gpt2.encode(activation_store_gpt2.next_batch()).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gpt2.encode(activation_store_gpt2.next_batch()).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_next_tokens_gpt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store_next_activations_gpt2 = activation_store_gpt2.get_activations(\n",
    "    activation_store_next_tokens_gpt2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128, 1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_next_activations_gpt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "         [[ 1.7617,  0.8806,  0.0538,  ..., -1.5403,  2.9543,  0.5674]],\n",
      "\n",
      "         [[-0.3194, -1.2106, -3.8291,  ..., -1.4626,  3.1476,  2.6306]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7187,  0.1953, -1.9804,  ..., -0.7839,  0.2512,  3.1737]],\n",
      "\n",
      "         [[ 4.4271, -1.2376,  2.8379,  ..., -5.1405,  1.5336, -0.6301]],\n",
      "\n",
      "         [[ 4.4993,  3.0335, -3.4330,  ..., -2.7207, -2.7646, -0.8193]]],\n",
      "\n",
      "\n",
      "        [[[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "         [[ 3.3672,  2.4428,  2.3995,  ...,  2.0427,  1.3496, -1.3920]],\n",
      "\n",
      "         [[ 1.9654, -2.2770, -1.2881,  ...,  1.6756,  1.4694,  0.8900]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7806,  0.9456, -1.4776,  ..., -3.8351, -1.0554,  3.2915]],\n",
      "\n",
      "         [[-5.2381, -4.4683,  1.4433,  ..., -1.8673,  3.7738, -1.2534]],\n",
      "\n",
      "         [[ 3.8893, -1.1445, -0.1531,  ...,  0.0826, -2.6133, -1.2079]]],\n",
      "\n",
      "\n",
      "        [[[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "         [[-0.3884, -1.1591, -4.9589,  ..., -1.7024,  2.4346, -4.2702]],\n",
      "\n",
      "         [[-1.0197, -0.2406, -1.0453,  ..., -0.8680, -2.1442, -1.2344]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0974,  2.1541, -1.6004,  ..., -2.7303,  1.5804,  3.1907]],\n",
      "\n",
      "         [[ 4.0064,  1.8763, -2.9437,  ..., -2.2403,  1.6900,  0.2838]],\n",
      "\n",
      "         [[-0.9858,  2.9562, -1.7064,  ...,  1.5734, -0.7267,  2.2492]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "         [[-5.3899, -0.1990, -0.4023,  ...,  0.5644,  0.6115, -1.6808]],\n",
      "\n",
      "         [[ 1.5830, -2.7692, -1.4388,  ..., -4.6888,  2.1773,  0.5598]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0156,  1.7121, -1.9624,  ..., -1.8131, -3.2546,  5.0079]],\n",
      "\n",
      "         [[-0.7993, -0.5675, -4.2966,  ...,  0.9451,  2.9111,  1.5660]],\n",
      "\n",
      "         [[-1.8063,  0.5402, -3.8495,  ...,  0.2482,  4.5868,  3.2817]]],\n",
      "\n",
      "\n",
      "        [[[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "         [[ 2.1969,  1.1092,  3.3271,  ..., -1.6219,  2.7024,  4.3633]],\n",
      "\n",
      "         [[ 0.3941,  3.1786, -3.2508,  ..., -1.6293,  3.0548,  2.1689]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0223, -2.6557, -2.9594,  ..., -0.3505, -1.4209,  2.3587]],\n",
      "\n",
      "         [[-3.2361, -4.3337,  0.2061,  ..., -2.2092, -1.0470, -1.2533]],\n",
      "\n",
      "         [[-5.6314,  0.6848,  1.4926,  ..., -2.4040, -2.6433,  0.8411]]],\n",
      "\n",
      "\n",
      "        [[[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "         [[ 2.8759, -0.8300,  0.3157,  ..., -1.5836, -3.0659, -0.0629]],\n",
      "\n",
      "         [[ 0.1364,  1.3991, -1.4885,  ..., -2.3514, -1.7963, -0.9105]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3117, -1.0911,  1.3821,  ..., -0.5711, -3.9211,  1.7687]],\n",
      "\n",
      "         [[ 0.8259, -1.3356,  0.6333,  ..., -3.0254, -1.3165, -3.9896]],\n",
      "\n",
      "         [[ 0.7529, -2.8001, -1.9985,  ..., -1.0792,  0.8054, -2.0423]]]])\n",
      "tensor([[[ 1.8916, -6.2451,  4.8756,  ..., -2.2415, -3.0685, -3.1433]],\n",
      "\n",
      "        [[ 4.7301,  1.6743, -1.0801,  ..., -4.1048,  5.6552,  0.6657]],\n",
      "\n",
      "        [[ 2.0979, -6.3212,  3.9944,  ..., -4.1749, -4.2775,  2.6101]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.0307, -5.0059, -4.4554,  ..., -3.5955, -3.8039, -4.3690]],\n",
      "\n",
      "        [[-0.1716, -1.7709,  3.0865,  ...,  0.2050,  3.8144,  0.3635]],\n",
      "\n",
      "        [[-2.2035,  0.6249, -1.6267,  ..., -3.3697, -1.8376,  1.1067]]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(activation_store_next_activations_gpt2)\n",
    "print(activation_store_gpt2.next_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4198e+00, -3.6200e+00, -4.8885e-01,  1.4290e+00, -6.1072e-01,\n",
       "          2.1432e+00, -3.0247e+00, -5.1112e+00, -1.4319e+00,  1.5245e+00,\n",
       "          2.6599e-01, -4.3088e+00,  1.0644e+00,  4.3840e+00, -4.4498e+00,\n",
       "          8.2160e-01,  3.2493e+00,  3.5883e+00,  2.5692e+00,  7.2998e-01,\n",
       "          4.8065e+00, -9.3076e-01, -5.0682e+00, -1.2309e+00,  1.0484e+00,\n",
       "          3.9354e+00,  3.2414e-01,  1.0189e+00, -1.0666e+00,  8.7230e-01,\n",
       "         -1.9818e+00, -5.8232e-01, -2.0265e+00, -2.7809e+00, -4.4952e-01,\n",
       "          3.8016e+00,  4.4219e+00, -2.7464e+00, -1.6595e+00,  1.7461e+00,\n",
       "          5.1277e+00, -1.8172e+00,  6.1772e-01,  1.6547e-01, -5.3222e-02,\n",
       "          3.0436e+00,  1.2391e+00,  3.2485e+00, -3.3789e+00,  1.1650e+00,\n",
       "         -1.9357e+00,  6.3813e-01, -7.4953e+00,  3.3693e+00, -1.5321e+00,\n",
       "          2.9719e+00,  2.7411e+00, -2.8643e+00, -2.8488e+00, -4.1920e+00,\n",
       "          2.8716e+00, -4.1937e-01, -4.4081e+00, -1.6992e-01, -2.7602e+01,\n",
       "          3.2994e+00, -1.8970e+00,  9.5023e-01, -3.2348e+00,  2.2535e+00,\n",
       "          5.2443e+00, -3.7001e+00, -6.2766e+00, -3.3705e+00, -3.6381e+00,\n",
       "         -2.2953e+00, -1.1296e+00,  2.6972e+00, -4.7247e+00,  6.2717e+00,\n",
       "         -5.0266e+00, -2.5130e+00,  1.7302e+00, -4.0658e+00, -1.0940e-01,\n",
       "          1.4026e+00, -1.5071e+00, -9.5650e+00,  3.0866e+00, -4.1855e+00,\n",
       "          1.5462e-01,  5.2863e+00, -9.2927e-01, -2.3301e+00,  1.6495e+00,\n",
       "         -2.5028e-01,  4.6643e+00, -2.6487e+00,  1.7863e+00,  8.1231e-01,\n",
       "          1.3287e-01,  1.1581e+00,  3.0174e-01,  3.6329e-01, -4.6915e+00,\n",
       "          1.0623e-01,  3.2853e+00,  3.2783e+00, -1.0417e+00,  3.2851e-02,\n",
       "         -3.6884e+00, -4.8206e-01,  1.4865e+00,  4.0899e+00,  4.1843e+00,\n",
       "          7.7521e+00, -6.1232e-01, -8.8441e-01, -1.1300e+00,  3.6716e+00,\n",
       "         -3.5268e+00, -1.8873e+00,  2.0482e+00, -1.9139e+00, -6.2639e-01,\n",
       "          2.4096e+00, -3.8598e+00, -7.5799e-01, -2.0551e+00,  2.0066e+00,\n",
       "         -5.3656e+00, -3.2007e+00,  5.5102e+00, -2.3024e+00, -2.2297e+00,\n",
       "          3.2934e+00,  4.5206e+00, -5.0838e+00,  5.7252e-01,  3.0874e+00,\n",
       "          3.4903e+00, -1.8385e+00, -1.4330e+00, -2.0360e+00,  5.7219e+00,\n",
       "         -2.2366e+00,  5.4960e-01,  3.0918e-02,  9.8753e-02, -6.3723e+00,\n",
       "          1.0154e+00,  5.8141e+00,  2.3997e+00,  2.0880e+00,  4.2587e-01,\n",
       "         -3.0084e-01,  4.1063e-01, -7.0004e+00,  1.3097e+00, -1.9081e+00,\n",
       "          3.5424e+00, -2.2249e-02, -2.2328e-01, -5.4571e-01, -1.8475e+00,\n",
       "          1.2083e+00, -2.3554e+00, -5.4527e+00, -2.1796e+00,  7.9449e-01,\n",
       "         -3.0697e+00,  1.7741e+00,  1.9836e+00,  2.9804e+00,  2.6438e+00,\n",
       "         -8.6446e-01,  2.8696e-01,  3.9016e+00,  1.7161e+00, -2.9076e+00,\n",
       "         -1.3010e-01, -8.1212e-01,  9.8033e-01,  5.8223e+00, -2.1547e+00,\n",
       "          2.5515e+00, -7.7354e-01, -4.5232e+00,  2.2185e+00, -4.0942e+00,\n",
       "         -5.0257e+00, -2.0333e-02,  3.0276e+00,  3.4619e+00, -3.2064e+00,\n",
       "         -3.0915e+00,  1.1166e+00,  3.4796e-02,  7.5414e-01,  1.0240e+00,\n",
       "         -1.4654e+00,  1.3581e+00,  1.5052e+00,  1.6784e+00, -8.8681e-01,\n",
       "          2.9548e+00,  1.9918e+00,  2.7286e+00, -1.1375e+00,  1.9469e+00,\n",
       "         -1.0583e+00, -4.7958e+00, -3.8969e+00, -8.7614e-01, -4.6235e+00,\n",
       "         -4.6537e+00,  6.5956e-01,  5.0612e-01, -6.4951e+00, -7.2026e+00,\n",
       "          3.9545e+00, -8.5772e-03, -4.2466e-01, -8.4441e+00, -7.0491e-02,\n",
       "          3.4600e+00,  3.1144e+00,  1.4464e+00,  2.7492e-02, -4.9779e-01,\n",
       "          3.6469e+00,  3.0532e+00,  2.4446e+00,  1.1031e+00, -2.5408e-02,\n",
       "         -1.4810e+00, -2.0925e-01, -2.5008e+00, -5.5656e-01, -1.4019e+00,\n",
       "          1.1709e+00, -2.6711e+00, -1.6626e+00, -8.8054e-01, -3.5996e+00,\n",
       "          4.1978e+00, -2.0808e+00, -2.0086e+00, -2.9840e+00, -3.4302e+00,\n",
       "         -1.9370e+00, -3.5217e-01, -3.2487e-01,  2.6941e+00,  5.3987e+00,\n",
       "         -4.5610e+00, -4.5191e+00, -7.1180e-02,  4.1376e+00, -2.1831e+00,\n",
       "         -1.5856e-01,  2.6363e+00,  3.3041e+00,  3.3376e+00,  8.8254e-01,\n",
       "         -5.2932e+00,  1.4571e+01,  4.9130e+00,  8.3285e-01, -1.3117e+00,\n",
       "          8.4589e-01,  4.8482e+00,  5.7401e+00, -1.6343e+00,  1.5615e+00,\n",
       "         -1.8586e+00,  2.4975e+00,  7.5476e-01, -1.9703e+00,  5.1680e+00,\n",
       "         -2.4750e+00, -3.1757e+00,  1.5353e-01, -7.2034e-01, -7.5039e-01,\n",
       "         -3.5274e+00, -6.1034e-01, -2.4469e+00,  1.1906e+01, -1.1803e+00,\n",
       "          4.0623e+00,  3.9170e+00, -7.8502e-01, -6.0870e-02,  1.2828e+00,\n",
       "          1.3684e+00, -1.4851e+00, -2.9731e-01,  3.1441e+00, -2.2001e+00,\n",
       "          1.4568e+00,  3.0174e+00,  8.5997e-01, -2.9118e+00, -2.7573e+00,\n",
       "         -3.4831e+00,  4.2602e+00,  1.2517e+00, -1.4881e+00, -4.5729e+00,\n",
       "          9.7560e-01, -1.7848e+00, -8.9536e-01,  4.7935e-01, -9.8297e+00,\n",
       "          5.9490e-01, -4.2678e+00,  4.6212e+00, -2.4991e+00,  1.5287e+00,\n",
       "         -3.0190e+00,  3.5860e-01, -4.3650e-01,  5.8291e+00, -2.4015e+00,\n",
       "         -1.0232e+00,  1.2447e+01, -4.8789e+00, -1.2729e+00,  1.8105e-01,\n",
       "          1.4427e+00,  1.1268e+00, -2.2708e+00,  1.1969e+00, -1.0567e+00,\n",
       "          8.9914e-01,  8.7600e-01, -4.6179e-01, -3.7366e+00, -7.4971e-02,\n",
       "          4.1515e+00, -2.3499e+00,  2.3301e+00, -2.0967e+00,  8.1957e-01,\n",
       "          7.4039e-01,  6.6863e+00,  7.3828e-02,  1.7512e+00, -2.4186e+00,\n",
       "         -1.9342e+00, -8.2717e-01,  4.7555e+00, -1.3576e+00, -5.9557e+00,\n",
       "          1.9545e+00,  1.1415e+00, -3.9752e+00,  1.7436e+00, -3.9320e+00,\n",
       "          2.4983e+00, -2.9599e+00,  6.5117e-01,  2.8158e+00, -3.5381e-01,\n",
       "         -9.8167e-01, -2.2177e+00,  1.0627e+00, -2.0539e+00, -1.6871e+00,\n",
       "         -1.4363e+00, -4.8799e+00, -2.0957e+00, -3.1724e+01, -4.3179e+00,\n",
       "         -1.6666e+00, -6.3726e+00,  4.8672e+00, -1.8056e+00, -1.0419e+00,\n",
       "          2.8105e+00,  2.6477e+00, -6.9135e-01,  1.6462e+00,  2.1884e+00,\n",
       "          2.6475e+00,  1.6670e+00, -4.3915e+00,  2.7162e+00, -4.5659e+00,\n",
       "          2.3372e+00, -1.8189e+00,  1.0414e-01,  6.0683e+00, -6.4817e+00,\n",
       "         -5.5705e-01,  2.4292e+00,  7.0743e-01,  2.1715e+00,  5.2564e-01,\n",
       "         -1.3257e+00,  2.6818e+00, -1.3150e-01,  1.1770e+00, -1.6003e+00,\n",
       "         -1.8653e+00, -2.5828e+00, -2.4426e+00,  7.5002e+00,  2.4630e+00,\n",
       "         -4.0822e+00, -3.8980e+00, -2.2483e+00, -6.1959e-01,  2.9067e+00,\n",
       "         -2.7413e+00, -3.5287e+00,  2.2852e-01,  1.8096e+00,  9.8777e-02,\n",
       "          2.4153e+00,  1.8826e+00,  1.7034e+00,  2.1382e-02,  1.5799e+00,\n",
       "         -1.9539e+00, -5.8910e-01,  2.1704e-01,  2.3326e+00, -6.6392e-02,\n",
       "         -3.8313e+00, -3.1897e+00,  1.4433e+00, -1.6776e+00,  5.2581e-01,\n",
       "          1.6867e+00,  1.6695e+00, -1.5518e+00,  6.3024e-01,  1.7537e+00,\n",
       "         -7.3631e-01, -2.8183e+00,  4.4131e+00, -3.4602e+00, -2.3103e+00,\n",
       "          2.3461e+00, -2.1357e+00,  4.4829e+01,  3.5178e-01,  3.1591e+00,\n",
       "          1.2819e+00,  4.7071e-01,  6.8303e+00,  2.8919e+00, -1.0301e+00,\n",
       "          5.2325e+00, -2.4535e+00, -4.3145e+00, -1.3795e+00, -1.0223e+01,\n",
       "          4.5033e+00, -9.3589e-01, -7.8828e+00,  1.2485e+00,  1.4173e+00,\n",
       "         -4.4987e+00, -3.0333e+00, -1.6946e+00,  1.7137e+00,  2.9399e+00,\n",
       "          1.4499e+00, -1.4895e+00, -4.0760e+00,  1.0876e+00, -7.5259e-01,\n",
       "          1.1548e+00, -4.2299e+00, -1.5079e+00, -1.3339e+00,  9.2060e-01,\n",
       "          2.3715e+00,  3.2616e+01,  1.2246e+00,  3.6599e+00,  2.8460e+00,\n",
       "         -1.1516e+00, -4.6478e+00,  1.2063e+00,  3.9633e+00,  9.5741e-01,\n",
       "         -1.5170e+00, -1.9003e+00, -1.2081e+00, -1.0172e+00,  6.5264e-01,\n",
       "          3.3261e+00,  2.5903e+00, -9.9414e-01,  1.1080e+00, -2.0573e+00,\n",
       "          1.2340e+00,  4.3348e+00,  1.8559e+00,  4.0030e-02,  5.9408e+00,\n",
       "          1.1016e+00,  2.6541e+00, -3.5596e+00,  1.8496e+00,  8.3942e-01,\n",
       "         -5.1029e+00,  8.0088e-01,  4.0892e+00, -2.6188e+00, -1.4624e+00,\n",
       "          4.5607e-01, -2.3557e-01, -7.1760e-01,  6.8588e-01, -2.2244e+00,\n",
       "          5.0459e+00, -7.7733e+00,  3.4603e+00,  9.1857e-01, -1.6779e+00,\n",
       "          5.4597e+00, -4.3604e+00, -6.2230e-02, -9.7635e-02,  5.8830e+00,\n",
       "         -3.2988e+00,  1.8337e-01,  1.8541e+00, -7.4965e-01, -1.2446e+00,\n",
       "         -3.3745e+00,  5.4422e+00,  2.9738e+00, -7.1896e+00, -2.8552e+00,\n",
       "         -8.8668e-01,  2.5689e+00,  3.8254e+00, -4.4758e+00, -1.2191e+00,\n",
       "          1.7815e+00, -1.3353e+00,  3.1987e+00, -2.2322e+00, -7.0012e+00,\n",
       "          1.6721e+00,  2.5853e+00,  2.5939e-01,  4.4543e+00,  1.8990e+00,\n",
       "          3.8356e+00, -3.0572e+00, -4.7702e+00,  6.5599e-01,  7.0690e-02,\n",
       "         -6.3131e+00,  2.8605e+00, -2.1745e+00, -2.3284e+00,  2.9491e+00,\n",
       "         -5.7014e+00,  9.3265e-02,  4.1144e+00,  5.6682e-02,  1.7784e+00,\n",
       "          2.3885e-01,  4.4584e+00,  7.8445e+00,  7.3988e-01, -3.9928e-01,\n",
       "          7.2195e+00,  4.8387e+00, -4.3075e+00,  4.0020e+00, -9.1271e-01,\n",
       "          4.2395e+00,  6.0218e+00, -2.3639e+00,  9.4550e-01, -1.6740e+00,\n",
       "          8.7009e-01, -5.6186e+00,  5.7293e+00, -4.7065e+00,  1.5105e+00,\n",
       "         -2.0922e+00, -8.0510e-01,  9.6123e-01,  3.5820e+00, -2.9857e+00,\n",
       "          2.6317e+00, -3.2909e+00, -2.9725e+00, -2.7024e-01,  1.4717e+00,\n",
       "         -2.0520e-01,  4.4639e-01,  1.3725e+00,  3.4630e+00, -2.6991e+00,\n",
       "          1.1468e-01, -3.6347e-01, -1.0465e+00, -6.8104e-01,  6.9604e+00,\n",
       "         -2.2625e+00, -4.8465e-01,  2.0690e+00, -1.4890e+00, -1.8899e+00,\n",
       "          7.8691e-01, -1.7632e+00, -1.4345e+00,  3.2998e+00,  8.0601e-01,\n",
       "          4.1286e-01, -1.2389e+00, -1.5961e-01,  1.4836e+00, -3.2166e+00,\n",
       "          1.3035e+00, -8.2137e+00, -3.0273e+00,  4.8567e+00, -5.2084e+00,\n",
       "          1.4172e+00,  2.2182e+00, -2.5882e+00,  2.8828e+00,  1.8095e+00,\n",
       "         -2.8567e+00, -1.1750e+00, -5.0866e-01, -2.1127e-02,  9.2592e-01,\n",
       "          2.0027e+00, -1.2226e+00, -9.5067e-01, -6.6010e+00,  1.7036e+00,\n",
       "          1.7484e+00, -1.8652e+00,  2.0175e+00, -1.5143e-01,  1.0093e+00,\n",
       "          9.2398e-01,  2.9276e+00, -6.6641e+00,  3.6869e-01,  7.5834e-01,\n",
       "          3.5598e+00,  1.4816e+00, -1.9879e+00,  5.1616e+00, -1.2688e+00,\n",
       "          5.4552e+00, -2.6136e+00,  3.5047e-01, -7.9804e-01, -4.6942e+00,\n",
       "          3.3420e+00,  1.3453e+00, -2.6695e+00, -9.5941e-01, -2.0186e+00,\n",
       "         -3.7833e+00, -3.4915e+00, -3.9734e-01, -2.7445e-01, -5.2418e+00,\n",
       "         -1.1993e+00,  9.8027e-01, -5.9480e-01,  1.9773e-01,  3.0436e+00,\n",
       "          5.2266e+00, -3.8407e-01,  5.1504e+00,  3.1289e+00, -7.8060e-01,\n",
       "         -3.1154e+00, -2.0033e-01, -3.8861e+00,  2.9614e-01,  1.6973e+00,\n",
       "          3.4897e+00, -4.8586e+00, -3.2556e+00,  6.3288e+00,  5.1890e+00,\n",
       "          1.1636e+00, -2.1363e+00, -2.8226e-01,  3.5224e+00,  3.4031e+00,\n",
       "          2.1580e+00,  9.4075e-01, -2.4604e+00,  1.1747e+00,  2.6613e+00,\n",
       "          1.7297e+00,  3.7641e+00,  2.2953e+00, -4.4812e+00,  1.0096e+00,\n",
       "          7.3247e+00, -2.6034e+00, -1.7802e+00,  2.3347e+00,  1.1657e+00,\n",
       "         -1.5611e+00, -2.6456e-01, -1.7936e+00, -5.0851e+00, -2.1818e+00,\n",
       "          3.3530e+00, -1.8800e+00, -2.7923e+00,  2.3786e+00,  7.3239e-01,\n",
       "          5.1997e+00,  3.2503e+00,  1.6940e+00, -4.8957e+00,  2.0609e+00,\n",
       "         -2.8343e+00, -1.7213e+00, -5.5069e+00, -7.4544e-01,  1.2006e-01,\n",
       "         -2.3097e+00,  1.6213e+00, -3.2864e+00, -2.6232e+00, -2.1752e+00,\n",
       "          2.1734e-01,  1.5692e+00, -5.1166e+00,  3.0114e+00,  6.2428e-02,\n",
       "         -8.0924e+00,  1.1257e+00,  3.2406e+00, -2.4249e+00, -1.9346e+00,\n",
       "         -2.5853e+00,  4.6754e-01,  3.5937e+00,  2.7296e+00,  1.6017e-01,\n",
       "          2.6889e+00, -3.8949e-01, -1.6683e+00, -1.0535e+00,  7.1830e-01,\n",
       "         -4.6686e+00, -2.7063e+00,  1.0393e+00,  1.2627e+00, -1.1842e+00,\n",
       "          2.3147e+00, -5.6997e-02, -2.0753e+00]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_gpt2.next_batch()[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gpt2.encode(\n",
    "    activation_store_next_activations_gpt2.to(sae_gpt2.device)\n",
    ").squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_gpt2.encode(activation_store_gpt2.next_batch()).squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume 128 is the context size, but why is that not in activation_store_next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get this to conform to old shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ActivationCache' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acts_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_gpt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_store_next_tokens_gpt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43msae_gemma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ActivationCache' object is not callable"
     ]
    }
   ],
   "source": [
    "acts_2 = model_gpt2.run_with_cache(activation_store_next_tokens_gpt2, stop_at_layer=1)[\n",
    "    1\n",
    "](sae_gemma.cfg.hook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'blocks.8.hook_resid_pre'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acts_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_gpt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_store_next_tokens_gpt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msae_gpt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_name\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/transformer_lens/ActivationCache.py:168\u001b[0m, in \u001b[0;36mActivationCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dict[key]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_act_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blocks.8.hook_resid_pre'"
     ]
    }
   ],
   "source": [
    "acts_2 = model_gpt2.run_with_cache(activation_store_next_tokens_gpt2, stop_at_layer=1)[\n",
    "    1\n",
    "][sae_gpt2.cfg.hook_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_batches, n_context = batch_tokens.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,   286,  8761,  ...,  7691,   284,  1592],\n",
       "        [50256,  2872,   711,  ..., 44088,   532,  2619],\n",
       "        [50256, 19650,  2310,  ..., 44654,  1028, 13094],\n",
       "        ...,\n",
       "        [50256,   683,    13,  ...,    82,  4497,   329],\n",
       "        [50256,   428,  2168,  ...,   351,  1399,  7091],\n",
       "        [50256,   447,   247,  ...,    83,  3221,   423]], device='mps:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_next_tokens_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128, 1, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_next_activations_gpt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_store_next_tokens_gpt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gpt2_batch = activation_store_gpt2.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_gpt2_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_activations_for_batch(\n",
    "#     activation_store: ActivationsStore, sae: SAE, remove_first = False,\n",
    "# ) -> torch.Tensor:\n",
    "\n",
    "#     if not remove_first:\n",
    "#         activations_batch = activation_store.next_batch()\n",
    "#         feature_acts = sae.encode(activations_batch).squeeze()\n",
    "#         return feature_acts\n",
    "#     else:\n",
    "#         activations_batch = get_flattened_activations(activation_store, batch_size = )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rough notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_like_next_batch(self, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = self.train_batch_size_tokens\n",
    "\n",
    "    # Get batch tokens\n",
    "    batch_tokens = self.get_batch_tokens()\n",
    "\n",
    "    # Get activations\n",
    "    activations = self.get_activations(batch_tokens)\n",
    "\n",
    "    # Reshape activations to match next_batch() output\n",
    "    # next_batch() returns shape (batch_size, d_in)\n",
    "    activations = activations.view(-1, self.d_in)\n",
    "\n",
    "    # Apply normalization if needed\n",
    "    if self.normalize_activations == \"expected_average_only_in\":\n",
    "        activations = self.apply_norm_scaling_factor(activations)\n",
    "\n",
    "    # Ensure we return the correct batch size\n",
    "    if activations.shape[0] > batch_size:\n",
    "        activations = activations[:batch_size]\n",
    "    elif activations.shape[0] < batch_size:\n",
    "        # If we don't have enough activations, we'll need to get more\n",
    "        additional_activations = self.get_activations_like_next_batch(\n",
    "            batch_size - activations.shape[0]\n",
    "        )\n",
    "        activations = torch.cat([activations, additional_activations], dim=0)\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_activations(self, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = self.train_batch_size_tokens\n",
    "\n",
    "    # Get batch tokens\n",
    "    batch_tokens = self.get_batch_tokens()\n",
    "\n",
    "    # Get activations\n",
    "    activations = self.get_activations(batch_tokens)\n",
    "\n",
    "    # Reshape activations to flatten out the sequence structure\n",
    "    # Original shape: (batch_size, context_size, num_layers, d_in)\n",
    "    # New shape: (batch_size * context_size, d_in)\n",
    "    flattened_activations = activations.view(-1, self.d_in)\n",
    "\n",
    "    # Apply normalization if needed\n",
    "    if self.normalize_activations == \"expected_average_only_in\":\n",
    "        flattened_activations = self.apply_norm_scaling_factor(flattened_activations)\n",
    "\n",
    "    # Ensure we return the correct batch size\n",
    "    if flattened_activations.shape[0] > batch_size:\n",
    "        flattened_activations = flattened_activations[:batch_size]\n",
    "    elif flattened_activations.shape[0] < batch_size:\n",
    "        # If we don't have enough activations, we'll need to get more\n",
    "        additional_activations = self.get_flattened_activations(\n",
    "            batch_size - flattened_activations.shape[0]\n",
    "        )\n",
    "        flattened_activations = torch.cat(\n",
    "            [flattened_activations, additional_activations], dim=0\n",
    "        )\n",
    "\n",
    "    return flattened_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_activations_without_bos(self, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = self.train_batch_size_tokens\n",
    "\n",
    "    # Get batch tokens\n",
    "    batch_tokens = self.get_batch_tokens()\n",
    "\n",
    "    # Get activations\n",
    "    activations = self.get_activations(batch_tokens)\n",
    "\n",
    "    # Remove the first token (BOS) from each sequence\n",
    "    # Original shape: (batch_size, context_size, num_layers, d_in)\n",
    "    # New shape: (batch_size, context_size - 1, num_layers, d_in)\n",
    "    activations_without_bos = activations[:, 1:, ...]\n",
    "\n",
    "    # Reshape activations to flatten out the sequence structure\n",
    "    # New shape: (batch_size * (context_size - 1), d_in)\n",
    "    flattened_activations = activations_without_bos.view(-1, self.d_in)\n",
    "\n",
    "    # Apply normalization if needed\n",
    "    if self.normalize_activations == \"expected_average_only_in\":\n",
    "        flattened_activations = self.apply_norm_scaling_factor(flattened_activations)\n",
    "\n",
    "    # Ensure we return the correct batch size\n",
    "    if flattened_activations.shape[0] > batch_size:\n",
    "        flattened_activations = flattened_activations[:batch_size]\n",
    "    elif flattened_activations.shape[0] < batch_size:\n",
    "        # If we don't have enough activations, we'll need to get more\n",
    "        additional_activations = self.get_flattened_activations_without_bos(\n",
    "            batch_size - flattened_activations.shape[0]\n",
    "        )\n",
    "        flattened_activations = torch.cat(\n",
    "            [flattened_activations, additional_activations], dim=0\n",
    "        )\n",
    "\n",
    "    return flattened_activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-cooccurence-DZTJ6ajw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
