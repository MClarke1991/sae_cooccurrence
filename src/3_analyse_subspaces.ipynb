{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from os.path import join as pj\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sae_lens import SAE, ActivationsStore\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from PIBBSS.graph_generation import (\n",
    "    load_subgraph,\n",
    "    plot_subgraph_static,\n",
    ")\n",
    "from PIBBSS.pca import (\n",
    "    calculate_pca_decoder,\n",
    "    create_pca_plots_decoder,\n",
    "    perform_pca_on_results,\n",
    "    plot_pca_explanation_and_save,\n",
    "    plot_pca_feature_strength,\n",
    "    plot_pca_with_active_features,\n",
    "    plot_pca_with_top_feature,\n",
    "    plot_simple_scatter,\n",
    "    plot_token_pca_and_save,\n",
    "    plot_doubly_clustered_activation_heatmap, \n",
    "    plot_feature_activations_combined, \n",
    "    get_point_result,\n",
    "    plot_feature_activations, \n",
    "    analyze_representative_points, \n",
    "    analyze_representative_points_comp, \n",
    "    analyze_user_specified_points_comp, \n",
    "    analyze_user_specified_points_comp_subgraph, \n",
    "    analyze_specific_points,\n",
    "    load_data_from_pickle, \n",
    "    save_data_to_pickle,\n",
    "    generate_data\n",
    ")\n",
    "from PIBBSS.utils.saving_loading import load_npz_files, set_device\n",
    "from PIBBSS.utils.set_paths import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_path):\n",
    "    logging.basicConfig(\n",
    "        filename=log_path,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "# Config -------------\n",
    "torch.set_grad_enabled(False)\n",
    "device = set_device()\n",
    "git_root = get_git_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings to perform PCA on a particular subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = True\n",
    "\n",
    "model_name = \"gpt2-small\"\n",
    "sae_release_short = \"res-jb-feature-splitting\"\n",
    "sae_id = \"blocks.8.hook_resid_pre_24576\"\n",
    "n_batches_reconstruction = 100\n",
    "\n",
    "# model_name = \"gemma-2-2b\"\n",
    "# sae_release_short = \"gemma-scope-2b-pt-res-canonical\"\n",
    "# sae_id = \"layer_0/width_16k/canonical\"\n",
    "# n_batches_reconstruction = 10\n",
    "\n",
    "activation_threshold = 1.5\n",
    "subgraph_id =  2332\n",
    "\n",
    "fs_splitting_cluster = subgraph_id\n",
    "pca_prefix = \"pca\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Documents/Github/PIBBSS/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning:\n",
      "\n",
      "`clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "# Process the specific subgraph\n",
    "sae_id_neat = sae_id.replace(\".\", \"_\").replace(\"/\", \"_\")\n",
    "results_dir = f\"results/cooc/{model_name}/{sae_release_short}/{sae_id_neat}\"\n",
    "results_path = pj(git_root, results_dir)\n",
    "activation_threshold_safe = str(activation_threshold).replace(\".\", \"_\")\n",
    "\n",
    "figures_path = pj(git_root, f\"figures/{model_name}/{sae_release_short}/{sae_id_neat}\")\n",
    "pca_dir = f\"{pca_prefix}_{activation_threshold_safe}_subgraph_{subgraph_id}\"\n",
    "pca_path = pj(figures_path, pca_dir)\n",
    "if not os.path.exists(pca_path):\n",
    "    os.makedirs(pca_path)\n",
    "pickle_file = pj(pca_path, f'pca_data_subgraph_{subgraph_id}.pkl')\n",
    "\n",
    "# Set up logging\n",
    "log_path = pj(pca_path, 'pca_analysis.log')\n",
    "setup_logging(log_path)\n",
    "\n",
    "# Log all settings\n",
    "logging.info(f\"Script started\")\n",
    "logging.info(f\"Settings:\")\n",
    "logging.info(f\"  save_figs: {save_figs}\")\n",
    "logging.info(f\"  git_root: {git_root}\")\n",
    "logging.info(f\"  sae_id: {sae_id}\")\n",
    "logging.info(f\"  activation_threshold: {activation_threshold}\")\n",
    "logging.info(f\"  subgraph_id: {subgraph_id}\")\n",
    "logging.info(f\"  fs_splitting_cluster: {fs_splitting_cluster}\")\n",
    "logging.info(f\"  pca_prefix: {pca_prefix}\")\n",
    "logging.info(f\"  model_name: {model_name}\")\n",
    "logging.info(f\"  sae_release_short: {sae_release_short}\")\n",
    "logging.info(f\"  n_batches_reconstruction: {n_batches_reconstruction}\")\n",
    "logging.info(f\"  device: {device}\")\n",
    "logging.info(f\"  results_path: {results_path}\")\n",
    "logging.info(f\"  pca_path: {pca_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading npz files: 100%|██████████| 4/4 [00:00<00:00, 985.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "node_df = pd.read_csv(pj(results_path, f\"dataframes/node_info_df_{activation_threshold_safe}.csv\"))\n",
    "logging.info(f\"Loaded node_df from {pj(results_path, f'dataframes/node_info_df_{activation_threshold_safe}.csv')}\")\n",
    "\n",
    "overall_feature_activations = load_npz_files(results_path, f'feature_acts_cooc_activations').get(activation_threshold)\n",
    "\n",
    "# with open(pj(results_path, f\"subgraph_objects/activation_{activation_threshold_safe}/subgraph_{subgraph_id}.pkl\"), 'rb') as f:\n",
    "#     subgraph = pickle.load(f)\n",
    "\n",
    "\n",
    "# Filter for the specific subgraph\n",
    "fs_splitting_nodes = node_df.query('subgraph_id == @subgraph_id')['node_id'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "regen_data = True\n",
    "if not regen_data:\n",
    "    raise ValueError(\"Are you sure you don't want to use existing data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287c059ed65482986ca197cce947e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"PCA analysis script\")\n",
    "# parser.add_argument('--save_pickle', action='store_true', help='Save generated data to pickle')\n",
    "# parser.add_argument('--load_pickle', action='store_true', help='Load data from pickle instead of regenerating')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "if not regen_data and os.path.exists(pickle_file):\n",
    "    data = load_data_from_pickle(pickle_file)\n",
    "    results = data['results']\n",
    "    pca_df = data['pca_df']\n",
    "    pca = data['pca']\n",
    "    pca_decoder = data['pca_decoder']\n",
    "    pca_decoder_df = data['pca_decoder_df']\n",
    "else:\n",
    "    if model_name == \"gemma-2-2b\":\n",
    "        sae_release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    else:\n",
    "        sae_release = f\"{model_name}-{sae_release_short}\"\n",
    "\n",
    "    # Load SAE and set up activation store\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=sae_release,\n",
    "        sae_id=sae_id,\n",
    "        device=device\n",
    "    )\n",
    "    sae.fold_W_dec_norm()\n",
    "    \n",
    "    activation_store = ActivationsStore.from_sae(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        streaming=True,\n",
    "        store_batch_size_prompts=8,\n",
    "        train_batch_size_tokens=4096,\n",
    "        n_batches_in_buffer=32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    data = generate_data(model, sae, activation_store, fs_splitting_nodes, n_batches_reconstruction, decoder=False)\n",
    "    \n",
    "    if regen_data:\n",
    "        save_data_to_pickle(data, pickle_file)\n",
    "\n",
    "    results = data['results']\n",
    "    pca_df = data['pca_df']\n",
    "    pca = data['pca']\n",
    "    pca_decoder = data['pca_decoder']\n",
    "    pca_decoder_df = data['pca_decoder_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Save pca_decoder_df as CSV\u001b[39;00m\n\u001b[1;32m     15\u001b[0m pca_decoder_df_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_decoder_df_subgraph_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgraph_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpca_decoder_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(pj(pca_path, pca_decoder_df_filename), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m create_pca_plots_decoder(pca_decoder_df, subgraph_id, pca_path, save\u001b[38;5;241m=\u001b[39msave_figs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing completed for subgraph ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgraph_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Save pca_df as CSV\n",
    "# pca_df_filename = f\"pca_df_subgraph_{subgraph_id}.csv\"\n",
    "# pca_df.to_csv(pj(pca_path, pca_df_filename), index=False)\n",
    "\n",
    "plot_token_pca_and_save(pca_df, pca_path, subgraph_id, color_by='token', save=save_figs)\n",
    "\n",
    "plot_pca_explanation_and_save(pca, pca_path, subgraph_id, save=save_figs)\n",
    "\n",
    "plot_simple_scatter(results, pca_path, subgraph_id, fs_splitting_nodes, save=save_figs)\n",
    "\n",
    "if pca_decoder is not None:\n",
    "    pca_decoder, pca_decoder_df = calculate_pca_decoder(sae, fs_splitting_nodes)\n",
    "\n",
    "# Save pca_decoder_df as CSV\n",
    "pca_decoder_df_filename = f\"pca_decoder_df_subgraph_{subgraph_id}.csv\"\n",
    "pca_decoder_df.to_csv(pj(pca_path, pca_decoder_df_filename), index=False)\n",
    "\n",
    "create_pca_plots_decoder(pca_decoder_df, subgraph_id, pca_path, save=save_figs)\n",
    "\n",
    "print(f\"Processing completed for subgraph ID {subgraph_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_pca_with_top_feature(pca_df, results, fs_splitting_nodes, fs_splitting_cluster, pca_path, save=save_figs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_feature_strength(pca_df, results, fs_splitting_nodes, fs_splitting_cluster, pca_path, pc_x='PC1', pc_y='PC2', save=save_figs)\n",
    "plot_pca_feature_strength(pca_df, results, fs_splitting_nodes, fs_splitting_cluster, pca_path, pc_x='PC1', pc_y='PC3', save=save_figs)\n",
    "plot_pca_feature_strength(pca_df, results, fs_splitting_nodes, fs_splitting_cluster, pca_path, pc_x='PC2', pc_y='PC3', save=save_figs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_with_active_features(pca_df, results, fs_splitting_nodes, fs_splitting_cluster, pca_path, activation_threshold=activation_threshold, save=save_figs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doubly_clustered_activation_heatmap(results, fs_splitting_nodes, pca_df, pca_path, fs_splitting_cluster, max_examples=1000, save=save_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_activations_combined(get_point_result(results, 2), fs_splitting_nodes, fs_splitting_cluster, activation_threshold, node_df, results_path, pca_path, save_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_feature_activations(get_point_result(results, 2), fs_splitting_nodes, fs_splitting_cluster, activation_threshold, node_df, results_path, save_figs=False, pca_path=pca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "pca_df, _ = perform_pca_on_results(results)\n",
    "analyze_representative_points(results=results, \n",
    "                              fs_splitting_nodes=fs_splitting_nodes, \n",
    "                              fs_splitting_cluster=fs_splitting_cluster, \n",
    "                              activation_threshold=activation_threshold, \n",
    "                              node_df=node_df, \n",
    "                              results_path=results_path, \n",
    "                              pca_df=pca_df, save_figs=True, pca_path=pca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_representative_points_comp(results, fs_splitting_nodes, activation_threshold, node_df, pca_df, save_figs=True, pca_path=pca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating the PCA plot and identifying interesting points\n",
    "interesting_point_ids = [0] # Replace with actual IDs of interest\n",
    "analyze_specific_points(results, fs_splitting_nodes, fs_splitting_cluster, activation_threshold, node_df, results_path, pca_df, interesting_point_ids, save_figs=True, pca_path=pca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_user_specified_points_comp(results, fs_splitting_nodes, activation_threshold, node_df, pca_df, interesting_point_ids, save_figs=True, pca_path=pca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_user_specified_points_comp_subgraph(results, fs_splitting_nodes, fs_splitting_cluster, activation_threshold, node_df, pca_df, interesting_point_ids, results_path, save_figs=True, pca_path=pca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subgraph_static(load_subgraph(results_path, activation_threshold, subgraph_id), node_df, 0.0, os.path.join(pca_path, 'overall_subgraph'), overall_feature_activations, normalize_globally=False, save_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "\n",
    "def plot_pca_weekdays(pca_df, pca_path, fs_splitting_cluster, plot_inner=False, save_figs=False):\n",
    "    # Define colors for each day and gray for others\n",
    "    if not plot_inner: \n",
    "        color_map = {\n",
    "            'Monday': '#FF9999',\n",
    "            'Tuesday': '#66B2FF',\n",
    "            'Wednesday': '#99FF99',\n",
    "            'Thursday': '#FFCC99',\n",
    "            'Friday': '#FF99FF',\n",
    "            'Saturday': '#99FFFF',\n",
    "            'Sunday': '#FFFF99',\n",
    "            'Other': '#CCCCCC'\n",
    "        }\n",
    "    else:\n",
    "        color_map = {\n",
    "            'Mon': '#FF9999',\n",
    "            'Tues': '#66B2FF',\n",
    "            'Wed': '#99FF99',\n",
    "            'Thurs': '#FFCC99',\n",
    "            'Fri': '#FF99FF',\n",
    "            'Sat': '#99FFFF',\n",
    "            'Sun': '#FFFF99',\n",
    "            'Other': '#CCCCCC'\n",
    "        }\n",
    "\n",
    "    # Function to determine color\n",
    "    def get_color(token):\n",
    "        token_lower = token.lower()\n",
    "        for day in color_map.keys():\n",
    "            if day.lower() in token_lower:\n",
    "                return color_map[day]\n",
    "        return color_map['Other']\n",
    "\n",
    "    # Apply the function to get colors\n",
    "    pca_df['color'] = pca_df['tokens'].apply(get_color)\n",
    "\n",
    "    # Create three figures for different PC combinations\n",
    "    figs = []\n",
    "    pc_combinations = [('PC1', 'PC2'), ('PC1', 'PC3'), ('PC2', 'PC3')]\n",
    "\n",
    "    for pc_x, pc_y in pc_combinations:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add traces for colors (days)\n",
    "        for day in list(color_map.keys()):\n",
    "            df_day = pca_df[pca_df['color'] == color_map[day]]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_day[pc_x],\n",
    "                    y=df_day[pc_y],\n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_map[day], size=12, line=dict(width=0)),\n",
    "                    name=day,\n",
    "                    text=[f\"Token: {t}<br>Context: {c}\" for t, c in zip(df_day['tokens'], df_day['context'])],\n",
    "                    hoverinfo='text'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=800,\n",
    "            title_text=f\"PCA Analysis - Cluster {fs_splitting_cluster} ({pc_x} vs {pc_y})\",\n",
    "            xaxis_title=pc_x,\n",
    "            yaxis_title=pc_y,\n",
    "            legend=dict(\n",
    "                groupclick=\"toggleitem\",\n",
    "                tracegroupgap=20\n",
    "            )\n",
    "        )\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "    outer_suffix = \"\" if not plot_inner else \"_inner\"\n",
    "\n",
    "    if save_figs:\n",
    "        for i, (pc_x, pc_y) in enumerate(pc_combinations):\n",
    "            # Save as PNG\n",
    "            png_path = os.path.join(pca_path, f\"pca_plot_weekdays_{fs_splitting_cluster}_{pc_x}_{pc_y}{outer_suffix}.png\")\n",
    "            figs[i].write_image(png_path, scale=3.0)\n",
    "\n",
    "            # Save as HTML\n",
    "            html_path = os.path.join(pca_path, f\"pca_plot_weekdays_{fs_splitting_cluster}_{pc_x}_{pc_y}{outer_suffix}.html\")\n",
    "            figs[i].write_html(html_path)\n",
    "    else:\n",
    "        for fig in figs:\n",
    "            fig.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_weekdays(pca_df, pca_path, fs_splitting_cluster, save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "def plot_pca_weekdays_3d(pca_df, pca_path, fs_splitting_cluster, plot_inner=False, save_figs=False):\n",
    "    # Define colors for each day and gray for others\n",
    "    if not plot_inner: \n",
    "        color_map = {\n",
    "            'Monday': '#FF9999',\n",
    "            'Tuesday': '#66B2FF',\n",
    "            'Wednesday': '#99FF99',\n",
    "            'Thursday': '#FFCC99',\n",
    "            'Friday': '#FF99FF',\n",
    "            'Saturday': '#99FFFF',\n",
    "            'Sunday': '#FFFF99',\n",
    "            'Other': '#CCCCCC'\n",
    "        }\n",
    "    else:\n",
    "        color_map = {\n",
    "            'Mon': '#FF9999',\n",
    "            'Tues': '#66B2FF',\n",
    "            'Wed': '#99FF99',\n",
    "            'Thurs': '#FFCC99',\n",
    "            'Fri': '#FF99FF',\n",
    "            'Sat': '#99FFFF',\n",
    "            'Sun': '#FFFF99',\n",
    "            'Other': '#CCCCCC'\n",
    "        }\n",
    "\n",
    "    # Function to determine color\n",
    "    def get_color(token):\n",
    "        token_lower = token.lower()\n",
    "        for day in color_map.keys():\n",
    "            if day.lower() in token_lower:\n",
    "                return color_map[day]\n",
    "        return color_map['Other']\n",
    "\n",
    "    # Apply the function to get colors\n",
    "    pca_df['color'] = pca_df['tokens'].apply(get_color)\n",
    "\n",
    "    # Create a 3D figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for colors (days)\n",
    "    for day in list(color_map.keys()):\n",
    "        df_day = pca_df[pca_df['color'] == color_map[day]]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=df_day['PC1'],\n",
    "                y=df_day['PC2'],\n",
    "                z=df_day['PC3'],\n",
    "                mode='markers',\n",
    "                marker=dict(color=color_map[day], size=3, line=dict(width=0)),\n",
    "                name=day,\n",
    "                text=[f\"Token: {t}<br>Context: {c}\" for t, c in zip(df_day['tokens'], df_day['context'])],\n",
    "                hoverinfo='text'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_text=f\"3D PCA Analysis - Cluster {fs_splitting_cluster}\",\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        ),\n",
    "        legend=dict(\n",
    "            groupclick=\"toggleitem\",\n",
    "            tracegroupgap=20\n",
    "        )\n",
    "    )\n",
    "\n",
    "    outer_suffix = \"\" if not plot_inner else \"_inner\"\n",
    "\n",
    "    if save_figs:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(pca_path, f\"pca_plot_weekdays_3d_{fs_splitting_cluster}{outer_suffix}.png\")\n",
    "        fig.write_image(png_path, scale=3.0)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(pca_path, f\"pca_plot_weekdays_3d_{fs_splitting_cluster}{outer_suffix}.html\")\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_weekdays_3d(pca_df, pca_path, fs_splitting_cluster, save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import os\n",
    "import plotly.express as px\n",
    "import re\n",
    "\n",
    "def plot_pca_filtered_context(pca_df, pca_path, fs_splitting_cluster, save_figs=False):\n",
    "    def process_and_count_chars(context):\n",
    "        # Remove '<|endoftext|>' from the context\n",
    "        cleaned_context = context.replace('<|endoftext|>', '')\n",
    "        \n",
    "        # Split the cleaned context by '|'\n",
    "        parts = cleaned_context.split('|')\n",
    "        \n",
    "        # Check if there's exactly one character between '|' symbols\n",
    "        if len(parts) == 3 and len(parts[1]) == 1:\n",
    "            single_char = parts[1]\n",
    "            before_part = parts[0]\n",
    "            \n",
    "            # Check for '/watch?' string\n",
    "            watch_index = before_part.rfind('/watch?')\n",
    "            if watch_index != -1:\n",
    "                # Count characters from end of '/watch?' to the single character\n",
    "                return len(before_part) - (watch_index + 7)  # 7 is the length of '/watch?'\n",
    "            else:\n",
    "                # Check if there's a '/' before the single character without spaces\n",
    "                match = re.search(r'/([^/\\s]+)$', before_part)\n",
    "                if match:\n",
    "                    # Count characters between the last '/' and the single character\n",
    "                    return len(match.group(1))\n",
    "        \n",
    "        # Return None for cases that don't meet the criteria\n",
    "        return None\n",
    "\n",
    "    # Apply the processing and counting function\n",
    "    pca_df['char_count'] = pca_df['context'].apply(process_and_count_chars)\n",
    "\n",
    "    # Filter out None values\n",
    "    pca_df_filtered = pca_df.dropna(subset=['char_count'])\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add trace for all points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pca_df_filtered['PC2'],\n",
    "            y=pca_df_filtered['PC3'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=pca_df_filtered['char_count'],\n",
    "                colorscale='turbo',\n",
    "                size=12,\n",
    "                colorbar=dict(title=\"Character Count\"),\n",
    "                line=dict(width=1, color='DarkSlateGrey')\n",
    "            ),\n",
    "            text=[f\"Token: {t}<br>Context: {c}<br>Char Count: {count}\" \n",
    "                  for t, c, count in zip(pca_df_filtered['tokens'], pca_df_filtered['context'], pca_df_filtered['char_count'])],\n",
    "            hoverinfo='text'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_text=f\"PCA Analysis - Cluster {fs_splitting_cluster} (Filtered Context Character Count)\",\n",
    "        xaxis_title=\"PC2\",\n",
    "        yaxis_title=\"PC3\",\n",
    "    )\n",
    "\n",
    "    if save_figs:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(pca_path, f\"pca_plot_filtered_context_char_count_{fs_splitting_cluster}.png\")\n",
    "        fig.write_image(png_path, scale=3.0)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(pca_path, f\"pca_plot_filtered_context_char_count_{fs_splitting_cluster}.html\")\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_filtered_context(pca_df, pca_path, fs_splitting_cluster, save_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def plot_feature_activation_normalized_area_chart(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    pca_df,\n",
    "    pca_path,\n",
    "    fs_splitting_cluster,\n",
    "    max_examples=1000,\n",
    "    save=False,\n",
    "):\n",
    "    def process_context(context):\n",
    "        parts = context.split('|')\n",
    "        if len(parts) == 3 and len(parts[1]) == 1:\n",
    "            before_part = parts[0]\n",
    "            watch_index = before_part.rfind('/watch?')\n",
    "            if watch_index != -1:\n",
    "                return len(before_part) - (watch_index + 7)\n",
    "            else:\n",
    "                match = re.search(r'/([^/\\s]+)$', before_part)\n",
    "                if match:\n",
    "                    return len(match.group(1))\n",
    "        return None\n",
    "\n",
    "    # Extract feature activations\n",
    "    feature_activations = results.all_graph_feature_acts.cpu().numpy()\n",
    "\n",
    "    # Limit the number of examples if there are too many\n",
    "    n_examples = min(feature_activations.shape[0], max_examples)\n",
    "    feature_activations = feature_activations[:n_examples]\n",
    "\n",
    "    # Calculate char_count for each example\n",
    "    char_counts = pca_df['context'].iloc[:n_examples].apply(process_context)\n",
    "\n",
    "    # Remove examples with None char_count\n",
    "    valid_indices = char_counts.notna()\n",
    "    feature_activations = feature_activations[valid_indices]\n",
    "    char_counts = char_counts[valid_indices]\n",
    "\n",
    "    # Create a DataFrame with char_counts and feature activations\n",
    "    df = pd.DataFrame(feature_activations, columns=fs_splitting_nodes)\n",
    "    df['char_count'] = char_counts.values\n",
    "\n",
    "    # Group by char_count and calculate mean activations\n",
    "    grouped = df.groupby('char_count').mean().reset_index()\n",
    "    grouped = grouped.sort_values('char_count')\n",
    "\n",
    "    # Normalize activations to sum to 1 for each char_count\n",
    "    activation_columns = grouped.columns.drop('char_count')\n",
    "    grouped[activation_columns] = grouped[activation_columns].div(grouped[activation_columns].sum(axis=1), axis=0)\n",
    "\n",
    "    # Create area chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for feature in fs_splitting_nodes:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=grouped['char_count'],\n",
    "            y=grouped[feature],\n",
    "            mode='lines',\n",
    "            line=dict(width=0.5),\n",
    "            stackgroup='one',\n",
    "            groupnorm='fraction',\n",
    "            name=f'Feature {feature}',\n",
    "            hoverinfo='text',\n",
    "            text=[f\"Feature: {feature}<br>Char Count: {count}<br>Normalized Activation: {act:.4f}\" \n",
    "                  for count, act in zip(grouped['char_count'], grouped[feature])],\n",
    "        ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Normalized Feature Activation by Character Count - Cluster {fs_splitting_cluster}\",\n",
    "        xaxis_title=\"Character Count\",\n",
    "        yaxis_title=\"Proportion of Feature Activation\",\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        legend_title=\"Features\",\n",
    "        hovermode='closest',\n",
    "        showlegend=True,\n",
    "        yaxis=dict(tickformat='.0%')  # Format y-axis as percentages\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    if save:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(pca_path, f\"feature_activation_normalized_area_chart_{fs_splitting_cluster}.png\")\n",
    "        fig.write_image(png_path, scale=4.0)\n",
    "\n",
    "        svg_path = os.path.join(pca_path, f\"feature_activation_normalized_area_chart_{fs_splitting_cluster}.svg\")\n",
    "        fig.write_image(svg_path)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(pca_path, f\"feature_activation_normalized_area_chart_{fs_splitting_cluster}.html\")\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_activation_normalized_area_chart(results, fs_splitting_nodes, pca_df, pca_path, fs_splitting_cluster,  save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_pca_domain(pca_df, pca_path, fs_splitting_cluster, save_figs=False):\n",
    "    # Define colors for each category\n",
    "    color_map = {\n",
    "        'twitter': '#1DA1F2',  # Twitter blue\n",
    "        'usat': '#FF0000',     # Red for USA Today\n",
    "        'youtube': '#00FF00',  # YouTube red\n",
    "        'other': '#CCCCCC'     # Gray for others\n",
    "    }\n",
    "\n",
    "    # Function to determine color\n",
    "    def get_color(row):\n",
    "        context = row['context'].lower()\n",
    "        if 'twitter' in context or 't.co' in context:\n",
    "            return color_map['twitter']\n",
    "        elif 'usat' in context:\n",
    "            return color_map['usat']\n",
    "        elif 'watch?v=' in context:\n",
    "            return color_map['youtube']\n",
    "        else:\n",
    "            return color_map['other']\n",
    "\n",
    "    # Apply the function to get colors\n",
    "    pca_df['color'] = pca_df.apply(get_color, axis=1)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for colors (categories)\n",
    "    for category, color in color_map.items():\n",
    "        df_category = pca_df[pca_df['color'] == color]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_category['PC2'],\n",
    "                y=df_category['PC3'],\n",
    "                mode='markers',\n",
    "                marker=dict(color=color, size=8),\n",
    "                name=category.capitalize(),\n",
    "                text=[f\"Token: {t}<br>Context: {c}\" for t, c in zip(df_category['tokens'], df_category['context'])],\n",
    "                hoverinfo='text'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_text=f\"PCA Analysis - Cluster {fs_splitting_cluster} (Context Categories)\",\n",
    "        xaxis_title=\"PC2\",\n",
    "        yaxis_title=\"PC3\",\n",
    "        legend_title_text=\"Context Category\"\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "    if save_figs:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(pca_path, f\"pca_plot_context_{fs_splitting_cluster}.png\")\n",
    "        fig.write_image(png_path, scale=3.0)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(pca_path, f\"pca_plot_context_{fs_splitting_cluster}.html\")\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_domain(pca_df, pca_path, fs_splitting_cluster, save_figs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
