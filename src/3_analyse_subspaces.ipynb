{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Library/Caches/pypoetry/virtualenvs/sae-cooccurence-DZTJ6ajw-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from os.path import join as pj\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sae_lens import SAE, ActivationsStore\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from sae_cooccurrence.graph_generation import (\n",
    "    load_subgraph,\n",
    "    plot_subgraph_static,\n",
    ")\n",
    "from sae_cooccurrence.normalised_cooc_functions import (\n",
    "    neat_sae_id,\n",
    ")\n",
    "from sae_cooccurrence.pca import (\n",
    "    analyze_representative_points,\n",
    "    analyze_representative_points_comp,\n",
    "    analyze_specific_points,\n",
    "    analyze_user_specified_points_comp,\n",
    "    analyze_user_specified_points_comp_subgraph,\n",
    "    calculate_pca_decoder,\n",
    "    create_pca_plots_decoder,\n",
    "    generate_data,\n",
    "    get_point_result,\n",
    "    load_data_from_pickle,\n",
    "    perform_pca_on_results,\n",
    "    plot_doubly_clustered_activation_heatmap,\n",
    "    plot_feature_activations,\n",
    "    plot_feature_activations_combined,\n",
    "    plot_pca_explanation_and_save,\n",
    "    plot_pca_feature_strength,\n",
    "    plot_pca_with_active_features,\n",
    "    plot_pca_with_top_feature,\n",
    "    plot_simple_scatter,\n",
    "    plot_token_pca_and_save,\n",
    "    save_data_to_pickle,\n",
    ")\n",
    "from sae_cooccurrence.utils.saving_loading import load_npz_files, set_device\n",
    "from sae_cooccurrence.utils.set_paths import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_path):\n",
    "    logging.basicConfig(\n",
    "        filename=log_path,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Config -------------\n",
    "torch.set_grad_enabled(False)\n",
    "device = set_device()\n",
    "git_root = get_git_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings to perform PCA on a particular subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = True\n",
    "\n",
    "model_name = \"gpt2-small\"\n",
    "sae_release_short = \"res-jb-feature-splitting\"\n",
    "sae_id = \"blocks.8.hook_resid_pre_24576\"\n",
    "n_batches_reconstruction = 100\n",
    "\n",
    "# model_name = \"gemma-2-2b\"\n",
    "# sae_release_short = \"gemma-scope-2b-pt-res-canonical\"\n",
    "# sae_id = \"layer_0/width_16k/canonical\"\n",
    "# n_batches_reconstruction = 10\n",
    "\n",
    "activation_threshold = 1.5\n",
    "subgraph_id = 2332\n",
    "\n",
    "fs_splitting_cluster = subgraph_id\n",
    "pca_prefix = \"pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Documents/Github/PIBBSS/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning:\n",
      "\n",
      "`clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "# Process the specific subgraph\n",
    "sae_id_neat = neat_sae_id(sae_id)\n",
    "results_dir = f\"results/{model_name}/{sae_release_short}/{sae_id_neat}\"\n",
    "results_path = pj(git_root, results_dir)\n",
    "activation_threshold_safe = str(activation_threshold).replace(\".\", \"_\")\n",
    "\n",
    "figures_path = pj(git_root, f\"figures/{model_name}/{sae_release_short}/{sae_id_neat}\")\n",
    "pca_dir = f\"{pca_prefix}_{activation_threshold_safe}_subgraph_{subgraph_id}\"\n",
    "pca_path = pj(figures_path, pca_dir)\n",
    "if not os.path.exists(pca_path):\n",
    "    os.makedirs(pca_path)\n",
    "pickle_file = pj(pca_path, f\"pca_data_subgraph_{subgraph_id}.pkl\")\n",
    "\n",
    "# Set up logging\n",
    "log_path = pj(pca_path, \"pca_analysis.log\")\n",
    "setup_logging(log_path)\n",
    "\n",
    "# Log all settings\n",
    "logging.info(\"Script started\")\n",
    "logging.info(\"Settings:\")\n",
    "logging.info(f\"  save_figs: {save_figs}\")\n",
    "logging.info(f\"  git_root: {git_root}\")\n",
    "logging.info(f\"  sae_id: {sae_id}\")\n",
    "logging.info(f\"  activation_threshold: {activation_threshold}\")\n",
    "logging.info(f\"  subgraph_id: {subgraph_id}\")\n",
    "logging.info(f\"  fs_splitting_cluster: {fs_splitting_cluster}\")\n",
    "logging.info(f\"  pca_prefix: {pca_prefix}\")\n",
    "logging.info(f\"  model_name: {model_name}\")\n",
    "logging.info(f\"  sae_release_short: {sae_release_short}\")\n",
    "logging.info(f\"  n_batches_reconstruction: {n_batches_reconstruction}\")\n",
    "logging.info(f\"  device: {device}\")\n",
    "logging.info(f\"  results_path: {results_path}\")\n",
    "logging.info(f\"  pca_path: {pca_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading npz files: 100%|██████████| 4/4 [00:00<00:00, 985.68it/s]\n"
     ]
    }
   ],
   "source": [
    "node_df = pd.read_csv(\n",
    "    pj(results_path, f\"dataframes/node_info_df_{activation_threshold_safe}.csv\")\n",
    ")\n",
    "logging.info(\n",
    "    f\"Loaded node_df from {pj(results_path, f'dataframes/node_info_df_{activation_threshold_safe}.csv')}\"\n",
    ")\n",
    "\n",
    "overall_feature_activations = load_npz_files(\n",
    "    results_path, \"feature_acts_cooc_activations\"\n",
    ").get(activation_threshold)\n",
    "\n",
    "# with open(pj(results_path, f\"subgraph_objects/activation_{activation_threshold_safe}/subgraph_{subgraph_id}.pkl\"), 'rb') as f:\n",
    "#     subgraph = pickle.load(f)\n",
    "\n",
    "\n",
    "# Filter for the specific subgraph\n",
    "fs_splitting_nodes = node_df.query(\"subgraph_id == @subgraph_id\")[\"node_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "regen_data = True\n",
    "if not regen_data:\n",
    "    raise ValueError(\"Are you sure you don't want to use existing data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287c059ed65482986ca197cce947e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"PCA analysis script\")\n",
    "# parser.add_argument('--save_pickle', action='store_true', help='Save generated data to pickle')\n",
    "# parser.add_argument('--load_pickle', action='store_true', help='Load data from pickle instead of regenerating')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "if not regen_data and os.path.exists(pickle_file):\n",
    "    data = load_data_from_pickle(pickle_file)\n",
    "    results = data[\"results\"]\n",
    "    pca_df = data[\"pca_df\"]\n",
    "    pca = data[\"pca\"]\n",
    "    pca_decoder = data[\"pca_decoder\"]\n",
    "    pca_decoder_df = data[\"pca_decoder_df\"]\n",
    "else:\n",
    "    if model_name == \"gemma-2-2b\":\n",
    "        sae_release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    else:\n",
    "        sae_release = f\"{model_name}-{sae_release_short}\"\n",
    "\n",
    "    # Load SAE and set up activation store\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=sae_release, sae_id=sae_id, device=device\n",
    "    )\n",
    "    sae.fold_W_dec_norm()\n",
    "\n",
    "    activation_store = ActivationsStore.from_sae(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        streaming=True,\n",
    "        store_batch_size_prompts=8,\n",
    "        train_batch_size_tokens=4096,\n",
    "        n_batches_in_buffer=32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    data = generate_data(\n",
    "        model,\n",
    "        sae,\n",
    "        activation_store,\n",
    "        fs_splitting_nodes,\n",
    "        n_batches_reconstruction,\n",
    "        decoder=False,\n",
    "    )\n",
    "\n",
    "    if regen_data:\n",
    "        save_data_to_pickle(data, pickle_file)\n",
    "\n",
    "    results = data[\"results\"]\n",
    "    pca_df = data[\"pca_df\"]\n",
    "    pca = data[\"pca\"]\n",
    "    pca_decoder = data[\"pca_decoder\"]\n",
    "    pca_decoder_df = data[\"pca_decoder_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Save pca_decoder_df as CSV\u001b[39;00m\n\u001b[1;32m     15\u001b[0m pca_decoder_df_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_decoder_df_subgraph_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgraph_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpca_decoder_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(pj(pca_path, pca_decoder_df_filename), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m create_pca_plots_decoder(pca_decoder_df, subgraph_id, pca_path, save\u001b[38;5;241m=\u001b[39msave_figs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing completed for subgraph ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgraph_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# # Save pca_df as CSV\n",
    "# pca_df_filename = f\"pca_df_subgraph_{subgraph_id}.csv\"\n",
    "# pca_df.to_csv(pj(pca_path, pca_df_filename), index=False)\n",
    "\n",
    "plot_token_pca_and_save(pca_df, pca_path, subgraph_id, color_by=\"token\", save=save_figs)\n",
    "\n",
    "plot_pca_explanation_and_save(pca, pca_path, subgraph_id, save=save_figs)\n",
    "\n",
    "plot_simple_scatter(results, pca_path, subgraph_id, fs_splitting_nodes, save=save_figs)\n",
    "\n",
    "if pca_decoder is not None:\n",
    "    pca_decoder, pca_decoder_df = calculate_pca_decoder(sae, fs_splitting_nodes)\n",
    "\n",
    "# Save pca_decoder_df as CSV\n",
    "pca_decoder_df_filename = f\"pca_decoder_df_subgraph_{subgraph_id}.csv\"\n",
    "pca_decoder_df.to_csv(pj(pca_path, pca_decoder_df_filename), index=False)\n",
    "\n",
    "create_pca_plots_decoder(pca_decoder_df, subgraph_id, pca_path, save=save_figs)\n",
    "\n",
    "print(f\"Processing completed for subgraph ID {subgraph_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_with_top_feature(\n",
    "    pca_df, results, fs_splitting_nodes, fs_splitting_cluster, pca_path, save=save_figs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_feature_strength(\n",
    "    pca_df,\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    pca_path,\n",
    "    pc_x=\"PC1\",\n",
    "    pc_y=\"PC2\",\n",
    "    save=save_figs,\n",
    ")\n",
    "plot_pca_feature_strength(\n",
    "    pca_df,\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    pca_path,\n",
    "    pc_x=\"PC1\",\n",
    "    pc_y=\"PC3\",\n",
    "    save=save_figs,\n",
    ")\n",
    "plot_pca_feature_strength(\n",
    "    pca_df,\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    pca_path,\n",
    "    pc_x=\"PC2\",\n",
    "    pc_y=\"PC3\",\n",
    "    save=save_figs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_with_active_features(\n",
    "    pca_df,\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    pca_path,\n",
    "    activation_threshold=activation_threshold,\n",
    "    save=save_figs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doubly_clustered_activation_heatmap(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    pca_df,\n",
    "    pca_path,\n",
    "    fs_splitting_cluster,\n",
    "    max_examples=1000,\n",
    "    save=save_figs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_activations_combined(\n",
    "    get_point_result(results, 2),\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    activation_threshold,\n",
    "    node_df,\n",
    "    results_path,\n",
    "    pca_path,\n",
    "    save_figs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_activations(\n",
    "    get_point_result(results, 2),\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    activation_threshold,\n",
    "    node_df,\n",
    "    results_path,\n",
    "    save_figs=False,\n",
    "    pca_path=pca_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "pca_df, _ = perform_pca_on_results(results)\n",
    "analyze_representative_points(\n",
    "    results=results,\n",
    "    fs_splitting_nodes=fs_splitting_nodes,\n",
    "    fs_splitting_cluster=fs_splitting_cluster,\n",
    "    activation_threshold=activation_threshold,\n",
    "    node_df=node_df,\n",
    "    results_path=results_path,\n",
    "    pca_df=pca_df,\n",
    "    save_figs=True,\n",
    "    pca_path=pca_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_representative_points_comp(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    activation_threshold,\n",
    "    node_df,\n",
    "    pca_df,\n",
    "    save_figs=True,\n",
    "    pca_path=pca_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating the PCA plot and identifying interesting points\n",
    "interesting_point_ids = [0]  # Replace with actual IDs of interest\n",
    "analyze_specific_points(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    activation_threshold,\n",
    "    node_df,\n",
    "    results_path,\n",
    "    pca_df,\n",
    "    interesting_point_ids,\n",
    "    save_figs=True,\n",
    "    pca_path=pca_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_user_specified_points_comp(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    activation_threshold,\n",
    "    node_df,\n",
    "    pca_df,\n",
    "    interesting_point_ids,\n",
    "    save_figs=True,\n",
    "    pca_path=pca_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_user_specified_points_comp_subgraph(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    fs_splitting_cluster,\n",
    "    activation_threshold,\n",
    "    node_df,\n",
    "    pca_df,\n",
    "    interesting_point_ids,\n",
    "    results_path,\n",
    "    save_figs=True,\n",
    "    pca_path=pca_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subgraph_static(\n",
    "    subgraph=load_subgraph(results_path, activation_threshold, subgraph_id),\n",
    "    node_info_df=node_df,\n",
    "    output_path=os.path.join(pca_path, \"overall_subgraph\"),\n",
    "    activation_array=overall_feature_activations,\n",
    "    normalize_globally=False,\n",
    "    save_figs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_pca_weekdays(\n",
    "    pca_df, pca_path, fs_splitting_cluster, plot_inner=False, save_figs=False\n",
    "):\n",
    "    # Define colors for each day and gray for others\n",
    "    if not plot_inner:\n",
    "        color_map = {\n",
    "            \"Monday\": \"#FF9999\",\n",
    "            \"Tuesday\": \"#66B2FF\",\n",
    "            \"Wednesday\": \"#99FF99\",\n",
    "            \"Thursday\": \"#FFCC99\",\n",
    "            \"Friday\": \"#FF99FF\",\n",
    "            \"Saturday\": \"#99FFFF\",\n",
    "            \"Sunday\": \"#FFFF99\",\n",
    "            \"Other\": \"#CCCCCC\",\n",
    "        }\n",
    "    else:\n",
    "        color_map = {\n",
    "            \"Mon\": \"#FF9999\",\n",
    "            \"Tues\": \"#66B2FF\",\n",
    "            \"Wed\": \"#99FF99\",\n",
    "            \"Thurs\": \"#FFCC99\",\n",
    "            \"Fri\": \"#FF99FF\",\n",
    "            \"Sat\": \"#99FFFF\",\n",
    "            \"Sun\": \"#FFFF99\",\n",
    "            \"Other\": \"#CCCCCC\",\n",
    "        }\n",
    "\n",
    "    # Function to determine color\n",
    "    def get_color(token):\n",
    "        token_lower = token.lower()\n",
    "        for day in color_map.keys():\n",
    "            if day.lower() in token_lower:\n",
    "                return color_map[day]\n",
    "        return color_map[\"Other\"]\n",
    "\n",
    "    # Apply the function to get colors\n",
    "    pca_df[\"color\"] = pca_df[\"tokens\"].apply(get_color)\n",
    "\n",
    "    # Create three figures for different PC combinations\n",
    "    figs = []\n",
    "    pc_combinations = [(\"PC1\", \"PC2\"), (\"PC1\", \"PC3\"), (\"PC2\", \"PC3\")]\n",
    "\n",
    "    for pc_x, pc_y in pc_combinations:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add traces for colors (days)\n",
    "        for day in list(color_map.keys()):\n",
    "            df_day = pca_df[pca_df[\"color\"] == color_map[day]]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_day[pc_x],\n",
    "                    y=df_day[pc_y],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=color_map[day], size=12, line=dict(width=0)),\n",
    "                    name=day,\n",
    "                    text=[\n",
    "                        f\"Token: {t}<br>Context: {c}\"\n",
    "                        for t, c in zip(df_day[\"tokens\"], df_day[\"context\"])\n",
    "                    ],\n",
    "                    hoverinfo=\"text\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=800,\n",
    "            title_text=f\"PCA Analysis - Cluster {fs_splitting_cluster} ({pc_x} vs {pc_y})\",\n",
    "            xaxis_title=pc_x,\n",
    "            yaxis_title=pc_y,\n",
    "            legend=dict(groupclick=\"toggleitem\", tracegroupgap=20),\n",
    "        )\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "    outer_suffix = \"\" if not plot_inner else \"_inner\"\n",
    "\n",
    "    if save_figs:\n",
    "        for i, (pc_x, pc_y) in enumerate(pc_combinations):\n",
    "            # Save as PNG\n",
    "            png_path = os.path.join(\n",
    "                pca_path,\n",
    "                f\"pca_plot_weekdays_{fs_splitting_cluster}_{pc_x}_{pc_y}{outer_suffix}.png\",\n",
    "            )\n",
    "            figs[i].write_image(png_path, scale=3.0)\n",
    "\n",
    "            # Save as HTML\n",
    "            html_path = os.path.join(\n",
    "                pca_path,\n",
    "                f\"pca_plot_weekdays_{fs_splitting_cluster}_{pc_x}_{pc_y}{outer_suffix}.html\",\n",
    "            )\n",
    "            figs[i].write_html(html_path)\n",
    "    else:\n",
    "        for fig in figs:\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_weekdays(pca_df, pca_path, fs_splitting_cluster, save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def plot_pca_weekdays_3d(\n",
    "    pca_df, pca_path, fs_splitting_cluster, plot_inner=False, save_figs=False\n",
    "):\n",
    "    # Define colors for each day and gray for others\n",
    "    if not plot_inner:\n",
    "        color_map = {\n",
    "            \"Monday\": \"#FF9999\",\n",
    "            \"Tuesday\": \"#66B2FF\",\n",
    "            \"Wednesday\": \"#99FF99\",\n",
    "            \"Thursday\": \"#FFCC99\",\n",
    "            \"Friday\": \"#FF99FF\",\n",
    "            \"Saturday\": \"#99FFFF\",\n",
    "            \"Sunday\": \"#FFFF99\",\n",
    "            \"Other\": \"#CCCCCC\",\n",
    "        }\n",
    "    else:\n",
    "        color_map = {\n",
    "            \"Mon\": \"#FF9999\",\n",
    "            \"Tues\": \"#66B2FF\",\n",
    "            \"Wed\": \"#99FF99\",\n",
    "            \"Thurs\": \"#FFCC99\",\n",
    "            \"Fri\": \"#FF99FF\",\n",
    "            \"Sat\": \"#99FFFF\",\n",
    "            \"Sun\": \"#FFFF99\",\n",
    "            \"Other\": \"#CCCCCC\",\n",
    "        }\n",
    "\n",
    "    # Function to determine color\n",
    "    def get_color(token):\n",
    "        token_lower = token.lower()\n",
    "        for day in color_map.keys():\n",
    "            if day.lower() in token_lower:\n",
    "                return color_map[day]\n",
    "        return color_map[\"Other\"]\n",
    "\n",
    "    # Apply the function to get colors\n",
    "    pca_df[\"color\"] = pca_df[\"tokens\"].apply(get_color)\n",
    "\n",
    "    # Create a 3D figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for colors (days)\n",
    "    for day in list(color_map.keys()):\n",
    "        df_day = pca_df[pca_df[\"color\"] == color_map[day]]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=df_day[\"PC1\"],\n",
    "                y=df_day[\"PC2\"],\n",
    "                z=df_day[\"PC3\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=color_map[day], size=3, line=dict(width=0)),\n",
    "                name=day,\n",
    "                text=[\n",
    "                    f\"Token: {t}<br>Context: {c}\"\n",
    "                    for t, c in zip(df_day[\"tokens\"], df_day[\"context\"])\n",
    "                ],\n",
    "                hoverinfo=\"text\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_text=f\"3D PCA Analysis - Cluster {fs_splitting_cluster}\",\n",
    "        scene=dict(xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\"),\n",
    "        legend=dict(groupclick=\"toggleitem\", tracegroupgap=20),\n",
    "    )\n",
    "\n",
    "    outer_suffix = \"\" if not plot_inner else \"_inner\"\n",
    "\n",
    "    if save_figs:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(\n",
    "            pca_path, f\"pca_plot_weekdays_3d_{fs_splitting_cluster}{outer_suffix}.png\"\n",
    "        )\n",
    "        fig.write_image(png_path, scale=3.0)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(\n",
    "            pca_path, f\"pca_plot_weekdays_3d_{fs_splitting_cluster}{outer_suffix}.html\"\n",
    "        )\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_weekdays_3d(pca_df, pca_path, fs_splitting_cluster, save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_filtered_context(pca_df, pca_path, fs_splitting_cluster, save_figs=False):\n",
    "    def process_and_count_chars(context):\n",
    "        # Remove '<|endoftext|>' from the context\n",
    "        cleaned_context = context.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "        # Split the cleaned context by '|'\n",
    "        parts = cleaned_context.split(\"|\")\n",
    "\n",
    "        # Check if there's exactly one character between '|' symbols\n",
    "        if len(parts) == 3 and len(parts[1]) == 1:\n",
    "            # single_char = parts[1]\n",
    "            before_part = parts[0]\n",
    "\n",
    "            # Check for '/watch?' string\n",
    "            watch_index = before_part.rfind(\"/watch?\")\n",
    "            if watch_index != -1:\n",
    "                # Count characters from end of '/watch?' to the single character\n",
    "                return len(before_part) - (\n",
    "                    watch_index + 7\n",
    "                )  # 7 is the length of '/watch?'\n",
    "            else:\n",
    "                # Check if there's a '/' before the single character without spaces\n",
    "                match = re.search(r\"/([^/\\s]+)$\", before_part)\n",
    "                if match:\n",
    "                    # Count characters between the last '/' and the single character\n",
    "                    return len(match.group(1))\n",
    "\n",
    "        # Return None for cases that don't meet the criteria\n",
    "        return None\n",
    "\n",
    "    # Apply the processing and counting function\n",
    "    pca_df[\"char_count\"] = pca_df[\"context\"].apply(process_and_count_chars)\n",
    "\n",
    "    # Filter out None values\n",
    "    pca_df_filtered = pca_df.dropna(subset=[\"char_count\"])\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add trace for all points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pca_df_filtered[\"PC2\"],\n",
    "            y=pca_df_filtered[\"PC3\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=pca_df_filtered[\"char_count\"],\n",
    "                colorscale=\"turbo\",\n",
    "                size=12,\n",
    "                colorbar=dict(title=\"Character Count\"),\n",
    "                line=dict(width=1, color=\"DarkSlateGrey\"),\n",
    "            ),\n",
    "            text=[\n",
    "                f\"Token: {t}<br>Context: {c}<br>Char Count: {count}\"\n",
    "                for t, c, count in zip(\n",
    "                    pca_df_filtered[\"tokens\"],\n",
    "                    pca_df_filtered[\"context\"],\n",
    "                    pca_df_filtered[\"char_count\"],\n",
    "                )\n",
    "            ],\n",
    "            hoverinfo=\"text\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_text=f\"PCA Analysis - Cluster {fs_splitting_cluster} (Filtered Context Character Count)\",\n",
    "        xaxis_title=\"PC2\",\n",
    "        yaxis_title=\"PC3\",\n",
    "    )\n",
    "\n",
    "    if save_figs:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(\n",
    "            pca_path, f\"pca_plot_filtered_context_char_count_{fs_splitting_cluster}.png\"\n",
    "        )\n",
    "        fig.write_image(png_path, scale=3.0)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(\n",
    "            pca_path,\n",
    "            f\"pca_plot_filtered_context_char_count_{fs_splitting_cluster}.html\",\n",
    "        )\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_filtered_context(pca_df, pca_path, fs_splitting_cluster, save_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_feature_activation_normalized_area_chart(\n",
    "    results,\n",
    "    fs_splitting_nodes,\n",
    "    pca_df,\n",
    "    pca_path,\n",
    "    fs_splitting_cluster,\n",
    "    max_examples=1000,\n",
    "    save=False,\n",
    "):\n",
    "    def process_context(context):\n",
    "        parts = context.split(\"|\")\n",
    "        if len(parts) == 3 and len(parts[1]) == 1:\n",
    "            before_part = parts[0]\n",
    "            watch_index = before_part.rfind(\"/watch?\")\n",
    "            if watch_index != -1:\n",
    "                return len(before_part) - (watch_index + 7)\n",
    "            else:\n",
    "                match = re.search(r\"/([^/\\s]+)$\", before_part)\n",
    "                if match:\n",
    "                    return len(match.group(1))\n",
    "        return None\n",
    "\n",
    "    # Extract feature activations\n",
    "    feature_activations = results.all_graph_feature_acts.cpu().numpy()\n",
    "\n",
    "    # Limit the number of examples if there are too many\n",
    "    n_examples = min(feature_activations.shape[0], max_examples)\n",
    "    feature_activations = feature_activations[:n_examples]\n",
    "\n",
    "    # Calculate char_count for each example\n",
    "    char_counts = pca_df[\"context\"].iloc[:n_examples].apply(process_context)\n",
    "\n",
    "    # Remove examples with None char_count\n",
    "    valid_indices = char_counts.notna()\n",
    "    feature_activations = feature_activations[valid_indices]\n",
    "    char_counts = char_counts[valid_indices]\n",
    "\n",
    "    # Create a DataFrame with char_counts and feature activations\n",
    "    df = pd.DataFrame(feature_activations, columns=fs_splitting_nodes)\n",
    "    df[\"char_count\"] = char_counts.values\n",
    "\n",
    "    # Group by char_count and calculate mean activations\n",
    "    grouped = df.groupby(\"char_count\").mean().reset_index()\n",
    "    grouped = grouped.sort_values(\"char_count\")\n",
    "\n",
    "    # Normalize activations to sum to 1 for each char_count\n",
    "    activation_columns = grouped.columns.drop(\"char_count\")\n",
    "    grouped[activation_columns] = grouped[activation_columns].div(\n",
    "        grouped[activation_columns].sum(axis=1), axis=0\n",
    "    )\n",
    "\n",
    "    # Create area chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for feature in fs_splitting_nodes:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=grouped[\"char_count\"],\n",
    "                y=grouped[feature],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5),\n",
    "                stackgroup=\"one\",\n",
    "                groupnorm=\"fraction\",\n",
    "                name=f\"Feature {feature}\",\n",
    "                hoverinfo=\"text\",\n",
    "                text=[\n",
    "                    f\"Feature: {feature}<br>Char Count: {count}<br>Normalized Activation: {act:.4f}\"\n",
    "                    for count, act in zip(grouped[\"char_count\"], grouped[feature])\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Normalized Feature Activation by Character Count - Cluster {fs_splitting_cluster}\",\n",
    "        xaxis_title=\"Character Count\",\n",
    "        yaxis_title=\"Proportion of Feature Activation\",\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        legend_title=\"Features\",\n",
    "        hovermode=\"closest\",\n",
    "        showlegend=True,\n",
    "        yaxis=dict(tickformat=\".0%\"),  # Format y-axis as percentages\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    if save:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(\n",
    "            pca_path,\n",
    "            f\"feature_activation_normalized_area_chart_{fs_splitting_cluster}.png\",\n",
    "        )\n",
    "        fig.write_image(png_path, scale=4.0)\n",
    "\n",
    "        svg_path = os.path.join(\n",
    "            pca_path,\n",
    "            f\"feature_activation_normalized_area_chart_{fs_splitting_cluster}.svg\",\n",
    "        )\n",
    "        fig.write_image(svg_path)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(\n",
    "            pca_path,\n",
    "            f\"feature_activation_normalized_area_chart_{fs_splitting_cluster}.html\",\n",
    "        )\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_activation_normalized_area_chart(\n",
    "    results, fs_splitting_nodes, pca_df, pca_path, fs_splitting_cluster, save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_domain(pca_df, pca_path, fs_splitting_cluster, save_figs=False):\n",
    "    # Define colors for each category\n",
    "    color_map = {\n",
    "        \"twitter\": \"#1DA1F2\",  # Twitter blue\n",
    "        \"usat\": \"#FF0000\",  # Red for USA Today\n",
    "        \"youtube\": \"#00FF00\",  # YouTube red\n",
    "        \"other\": \"#CCCCCC\",  # Gray for others\n",
    "    }\n",
    "\n",
    "    # Function to determine color\n",
    "    def get_color(row):\n",
    "        context = row[\"context\"].lower()\n",
    "        if \"twitter\" in context or \"t.co\" in context:\n",
    "            return color_map[\"twitter\"]\n",
    "        elif \"usat\" in context:\n",
    "            return color_map[\"usat\"]\n",
    "        elif \"watch?v=\" in context:\n",
    "            return color_map[\"youtube\"]\n",
    "        else:\n",
    "            return color_map[\"other\"]\n",
    "\n",
    "    # Apply the function to get colors\n",
    "    pca_df[\"color\"] = pca_df.apply(get_color, axis=1)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for colors (categories)\n",
    "    for category, color in color_map.items():\n",
    "        df_category = pca_df[pca_df[\"color\"] == color]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_category[\"PC2\"],\n",
    "                y=df_category[\"PC3\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=color, size=8),\n",
    "                name=category.capitalize(),\n",
    "                text=[\n",
    "                    f\"Token: {t}<br>Context: {c}\"\n",
    "                    for t, c in zip(df_category[\"tokens\"], df_category[\"context\"])\n",
    "                ],\n",
    "                hoverinfo=\"text\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=800,\n",
    "        title_text=f\"PCA Analysis - Cluster {fs_splitting_cluster} (Context Categories)\",\n",
    "        xaxis_title=\"PC2\",\n",
    "        yaxis_title=\"PC3\",\n",
    "        legend_title_text=\"Context Category\",\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=12, line=dict(width=2, color=\"DarkSlateGrey\")),\n",
    "        selector=dict(mode=\"markers\"),\n",
    "    )\n",
    "\n",
    "    if save_figs:\n",
    "        # Save as PNG\n",
    "        png_path = os.path.join(\n",
    "            pca_path, f\"pca_plot_context_{fs_splitting_cluster}.png\"\n",
    "        )\n",
    "        fig.write_image(png_path, scale=3.0)\n",
    "\n",
    "        # Save as HTML\n",
    "        html_path = os.path.join(\n",
    "            pca_path, f\"pca_plot_context_{fs_splitting_cluster}.html\"\n",
    "        )\n",
    "        fig.write_html(html_path)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_domain(pca_df, pca_path, fs_splitting_cluster, save_figs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
